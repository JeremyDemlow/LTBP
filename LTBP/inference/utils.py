# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/02_Inference_Utilities.ipynb.

# %% auto 0
__all__ = ['pull_sklearn_object_from_adls', 'prediction_to_adls_and_sf']

# %% ../../nbs/02_Inference_Utilities.ipynb 3
from ..data.utils import snowflake_query

from data_system_utilities.snowflake.copyinto import adls_url_to_sf_query_generator
from data_system_utilities.snowflake.utils import create_table_query_from_df
from data_system_utilities.azure.storage import FileHandling

import pickle
import os
import logging
import datetime
import shutil

# %% ../../nbs/02_Inference_Utilities.ipynb 4
def pull_sklearn_object_from_adls(adls_path: str,
                                  file_name: str,
                                  container_name: str,
                                  connection_str: str,
                                  drop_local_path: str = '.',
                                  clean_up: bool = True):
    """pulls a pickeld sklearn object from azure data lake to memory

    Args:
        file_name (str): name of file
        path (str): data lake path
        container (str): data lake container
        connection_str (str): azure connection string for the account

    Returns:
        (sklearn object): sklearn object loaded from azure
    """
    logging.info(f'Loading Sklearn Object to: {os.path.join(drop_local_path, file_name)}')

    if not os.path.exists(drop_local_path):
        os.makedirs(drop_local_path)

    fh = FileHandling(connection_str)
    fh.download_file(
        azure_file_path=adls_path+file_name,
        container_name=container_name,
        local_file_path=drop_local_path,
        overwrite=True
    )

    with open(os.path.join(drop_local_path, file_name), 'rb') as f:
        pipeline = pickle.load(f)
        logging.info('Sklearn Object Loaded')

    if clean_up:
        shutil.rmtree(drop_local_path)

    return pipeline

# %% ../../nbs/02_Inference_Utilities.ipynb 6
def prediction_to_adls_and_sf(
    df,  # pandas dataframe to infer on
    sk_model_pipe,  # Sklearn Pipeline that brings preprocessing and modeling to data
    adls_path: str,  # adls root path
    models_dict: dict,  # model dict used through out the project classes would avoid this
    etl_dict: dict,  # etl dict used through the project
    experiment_name: str,  # name of experiment being ran
    sfSchema=os.getenv("sfSchema", "DEV"),  # defaults to enviornment variable or default
):
    """custom to this project small changes to make it more flexible"""
    sf_df = df[models_dict['identification']].copy()
    # Change Here Name change for a regression and to predict or multi-labled needs some work
    sf_df['PROBABILITY'] = sk_model_pipe.predict_proba(df)[:, 1]
    del df
    date_created = datetime.datetime.today().strftime('%Y-%m-%d %H:%M:%S')
    sf_df['CI_COMMIT_SHA'] = os.environ.get('CI_COMMIT_SHA', 'LocalRunNBS')
    sf_df['DATE_CREATED'] = datetime.datetime.today().strftime('%Y-%m-%d %H:%M:%S')
    sf_df['EXPERIMENT'] = experiment_name
    file_name = f"predictions_{os.environ.get('CI_COMMIT_SHA','LocalRunNBS')+experiment_name}.csv"
    # Saving as a .csv for simple reading from adls download using dask would be best here
    sf_df.to_csv(file_name, index=False)
    logging.info(f'preview predictions being added:\n{sf_df.head(3)}')
    logging.info(f'preview predictions values addes:\n{sf_df.iloc[0].values}')
    logging.info(f'preview predictions being added columns:\n{sf_df.columns}')
    az = FileHandling(os.environ[models_dict['connection_str']])
    az.upload_file(
        azure_file_path=os.path.join(adls_path,
                                     models_dict['predictions_adls_path'],
                                     models_dict[experiment_name]['model_trainer']),
        local_file_path=file_name,
        container_name=etl_dict['azure_container'],
        overwrite=True,
    )
    os.unlink(file_name)
    stage_url = f"azure://{etl_dict['azure_account']}.blob.core.windows.net/{etl_dict['azure_container']}/"
    preds_file_path = os.path.join(adls_path,
                                   models_dict['predictions_adls_path'],
                                   models_dict['BASELINE']['model_trainer'],
                                   file_name)

    sf = snowflake_query(sfSchema=sfSchema)
    if models_dict['inference_sf_table_name'].upper() not in sf.run_sql_str("show tables;").name.tolist():
        sf.run_sql_str(create_table_query_from_df(sf_df, table_name_sf=models_dict['inference_sf_table_name'], varchar=False))

    logging.info("Pushing Forecasted Season from ADLS to Snowflake")
    adls_query = adls_url_to_sf_query_generator(
        azure_path=os.path.join(stage_url, preds_file_path),
        azure_sas_token=os.environ[models_dict['sas_token']],
        table_name=models_dict['inference_sf_table_name'],
        database=sf.connection_inputs['database'],
        schema=sf.connection_inputs['schema'],
        skip_header='1',
        file_type='csv',
        pattern='.*.csv')
    sf.run_sql_str(adls_query)

    exp_table = sf.run_sql_str(f"""
    SELECT *
    FROM {models_dict['inference_sf_table_name']}
    WHERE DATE_CREATED = '{date_created}'
    AND EXPERIMENT = '{experiment_name}'
    LIMIT 3
    """)
    logging.info(f'preview of queried table being added:\n{exp_table.head(3)}')
    logging.info(f'preview predictions values addes:\n{exp_table.iloc[0].values}')
