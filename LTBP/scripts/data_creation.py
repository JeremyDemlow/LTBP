# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/02a_Data_Creation.ipynb.

# %% auto 0
__all__ = ['data_creation']

# %% ../../nbs/02a_Data_Creation.ipynb 5
@call_parse
def data_creation(train_or_inference: Param(help="YAML section to read", type=str, default='train')):  # noqa:
    """

    What is happpening is this scriptOverview:
    1. Reads yaml files needed to complete this stage ``get_yaml_dicts``
    2. Generate query, feature names and feature types for DS model ``generate_feature_set_query``
    3. Generates datalake stage ``make_data_lake_stage``
    4. Executes generated query and dumps to the datalake stage that was created in step 3 ``snowflake_to_data_lake_from_query``

    Key Note: the train_or_test is the trigger for test/inference data set.
    """
    data, etl, _ = get_yaml_dicts(yaml_file_list)
    val_return = validate_sections(data, train_or_inference)
    output = generate_feature_set_query(ecid_list=val_return[0],
                                        static_features=val_return[1],
                                        temporal_feature_dict=val_return[2],
                                        resort_feature_dict=val_return[3],
                                        subseason_feature_dict=val_return[4],
                                        label_dict=val_return[5],
                                        years_of_data=val_return[6],
                                        time_dict=val_return[7],
                                        custom_feature_dict=val_return[8])
    full_query, feature_names, feature_types = output
    stage_name = etl['snowflake']['database']+'.'+etl[os.environ.get('prod_or_dev')]+'.'+etl['data_lake']['stage_name']+os.environ.get('CI_COMMIT_SHA')
    logger.info('Begining Creation of DataLake Stage')
    sf = snowflake_query(sfSchema=etl[os.environ.get('prod_or_dev')])
    make_data_lake_stage(sf_connection=sf,
                         stage_name=stage_name,
                         account=etl['data_lake']['account'],
                         container=etl['data_lake']['container'],
                         data_lake_path=etl['data_lake']['stage_path'].replace('COMMITID', os.environ['CI_COMMIT_SHA']),
                         sas_token=os.environ[etl['data_lake']['sas_token']])
    logger.info('Begining Query Dump from SF --> Azure Datalake')
    blob_path = etl['data_lake']['training_path'] if train_or_inference == 'train' else etl['data_lake']['inference_path']
    snowflake_to_data_lake_from_query(sf=sf,
                                      stage_name=stage_name,
                                      blob_path=blob_path,
                                      query=full_query)
    if train_or_inference == 'train':
        logger.info('pushing feature list to gitlab artifact to be used in the next section')
        pd.DataFrame([feature_names, feature_types]).T.rename(columns={0: 'featurenames', 1: 'dtypes'}).to_csv('features.csv', index=False)
    else:
        logger.info('pushing inference list to gitlab artifact to be used in the next section')
        pd.DataFrame([feature_names, feature_types]).T.rename(columns={0: 'featurenames', 1: 'dtypes'}).to_csv('inference_features.csv', index=False)
