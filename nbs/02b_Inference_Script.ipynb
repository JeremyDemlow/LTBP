{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Script\n",
    "\n",
    "> Inference utilities used in this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp scripts.inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore.script import Param, call_parse\n",
    "\n",
    "from LTBP.modeling.utils import create_stage_and_query_stage_sf\n",
    "from LTBP.data.utils import snowflake_query, get_yaml_dicts\n",
    "from LTBP.inference.utils import pull_sklearn_object_from_adls\n",
    "\n",
    "from data_system_utilities.snowflake.copyinto import adls_url_to_sf_query_generator\n",
    "from data_system_utilities.snowflake.utils import create_table_query_from_df\n",
    "from data_system_utilities.azure.storage import FileHandling\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the DSDE standard process for using Xboost with hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@call_parse\n",
    "def model_inference(\n",
    "    yaml_file_list: Param(help=\"YAML files to read\", type=list,  # noqa:\n",
    "                      default=['features.yaml', 'udf_inputs.yaml', 'etl.yaml', 'models.yaml']),  # noqa:\n",
    "    experiment_name: Param(help=\"tell function what experiment is being ran\", type=str, default='BASELINE'),  # noqa:\n",
    "    experiment: Param(help=\"add experiment state it is not an experiment\", type=bool, default=True),  # noqa:\n",
    "    sfSchema: Param(help=\"dev queries dev schema anything else will query project schema\", type=str, default='dev')  # noqa:\n",
    "    ):  # noqa:\n",
    "\n",
    "    features, udf_inputs, etl_dict, models_dict = get_yaml_dicts(yaml_file_list)\n",
    "    sf = snowflake_query(sfSchema=sfSchema)\n",
    "    adls_paths = []\n",
    "    model_names = []\n",
    "    experiment_names = []\n",
    "    experiments = []\n",
    "    if sfSchema.lower() != 'dev':\n",
    "        prod_model = sf.run_sql_str(f'''SELECT *\n",
    "        FROM MACHINELEARNINGOUTPUTS.{sfSchema}.{models_dict['tracking_table']}\n",
    "        WHERE PRODUCTION_MODEL\n",
    "        ''')\n",
    "        sf.run_sql_str(f\"DROP TABLE IF EXISTS MACHINELEARNINGOUTPUTS.{sfSchema}.{models_dict['inference_sf_table_name']}\")\n",
    "        for i, v in prod_model.iterrows():\n",
    "            adls_path = os.path.join(\n",
    "                (os.path.join(etl_dict['data_lake_path'], 'experiments', v['EXPERIMENT_NAME'])\n",
    "                 if v['EXPERIMENT']\n",
    "                 else os.path.join(\n",
    "                    etl_dict['data_lake_path'], v['COMMITID'], v['EXPERIMENT_NAME'])))\n",
    "            adls_paths.append(adls_path)\n",
    "            model_name = (models_dict[v['EXPERIMENT_NAME']]['model_trainer']\n",
    "                          + v['COMMITID']\n",
    "                          + v['EXPERIMENT_NAME']+'.pkl'\n",
    "                          )\n",
    "            model_names.append(model_name)\n",
    "            experiment_names.append(v['EXPERIMENT_NAME'])\n",
    "            experiments.append(v['EXPERIMENT'])\n",
    "    else:\n",
    "        adls_path = os.path.join(\n",
    "            (os.path.join(etl_dict['data_lake_path'], 'experiments', experiment_name)\n",
    "             if experiment\n",
    "             else os.path.join(\n",
    "                etl_dict['data_lake_path'], os.environ.get('CI_COMMIT_SHA', 'LocalRunNBS'))))\n",
    "        adls_paths.append(adls_path)\n",
    "        model_name = (models_dict[experiment_name]['model_trainer']\n",
    "                      + os.environ.get('CI_COMMIT_SHA', 'LocalRunNBS')\n",
    "                      + experiment_name+'.pkl'\n",
    "                      )\n",
    "        model_names.append(model_name)\n",
    "        experiment_names.append(experiment_name)\n",
    "        experiments.append(experiment)\n",
    "    \"\"\"\n",
    "    This came about while thinking about having more than one production model\n",
    "    making a functional call to this is probably better than this long code\n",
    "    \"\"\"\n",
    "    for adls_path, model_name, exp_name in zip(adls_paths, model_names, experiment_names):\n",
    "        df_infer = create_stage_and_query_stage_sf(\n",
    "            sf=sf,\n",
    "            features=features,\n",
    "            etl=etl_dict,\n",
    "            udf_inputs=udf_inputs,\n",
    "            train_or_inference='INFERENCE',\n",
    "            experiment_name=exp_name,\n",
    "            experiment=experiment,\n",
    "            indentification=models_dict['identification'],\n",
    "            extra_statement='LIMIT 1000'  # Can add limit when experimenting 'LIMIT 1000'\n",
    "        )\n",
    "        model = pull_sklearn_object_from_adls(\n",
    "            adls_path=os.path.join(adls_path,\n",
    "                                   models_dict['modeling_adls_path'],\n",
    "                                   models_dict[exp_name]['model_trainer']\n",
    "                                   ) + '/',\n",
    "            file_name=model_name,\n",
    "            drop_local_path='./models/',\n",
    "            container_name=etl_dict['azure_container'],\n",
    "            connection_str=os.environ[models_dict['connection_str']]\n",
    "        )\n",
    "        sf_df = df_infer[models_dict['identification']].copy()\n",
    "        # Change Here Name change for a regression and to predict or multi-labled needs some work\n",
    "        sf_df['PROBABILITY'] = model.predict_proba(df_infer)[:, 1]\n",
    "        date_created = datetime.datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        sf_df['CI_COMMIT_SHA'] = os.environ.get('CI_COMMIT_SHA', 'LocalRunNBS')\n",
    "        sf_df['DATE_CREATED'] = datetime.datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        sf_df['EXPERIMENT'] = exp_name\n",
    "        file_name = f\"predictions_{os.environ.get('CI_COMMIT_SHA','LocalRunNBS')+exp_name}.csv\"\n",
    "        # Saving as a .csv for simple reading from adls download using dask would be best here\n",
    "        sf_df.to_csv(file_name, index=False)\n",
    "        logging.info(f'preview predictions being added:\\n{sf_df.head(3)}')\n",
    "        logging.info(f'preview predictions values addes:\\n{sf_df.iloc[0].values}')\n",
    "        logging.info(f'preview predictions being added columns:\\n{sf_df.columns}')\n",
    "        az = FileHandling(os.environ[models_dict['connection_str']])\n",
    "        az.upload_file(\n",
    "            azure_file_path=os.path.join(adls_path,\n",
    "                                         models_dict['predictions_adls_path'],\n",
    "                                         models_dict[exp_name]['model_trainer']),\n",
    "            local_file_path=file_name,\n",
    "            container_name=etl_dict['azure_container'],\n",
    "            overwrite=True,\n",
    "        )\n",
    "        os.unlink(file_name)\n",
    "        stage_url = f\"azure://{etl_dict['azure_account']}.blob.core.windows.net/{etl_dict['azure_container']}/\"\n",
    "        preds_file_path = os.path.join(adls_path,\n",
    "                                       models_dict['predictions_adls_path'],\n",
    "                                       models_dict[exp_name]['model_trainer'],\n",
    "                                       file_name)\n",
    "\n",
    "        sf = snowflake_query(sfSchema=sfSchema)\n",
    "        if models_dict['inference_sf_table_name'].upper() not in sf.run_sql_str(\"show tables;\").name.tolist():\n",
    "            sf.run_sql_str(create_table_query_from_df(sf_df, table_name_sf=models_dict['inference_sf_table_name'], varchar=False))\n",
    "        logging.info(\"Pushing Forecasted Season from ADLS to Snowflake\")\n",
    "        adls_query = adls_url_to_sf_query_generator(\n",
    "            azure_path=os.path.join(stage_url, preds_file_path),\n",
    "            azure_sas_token=os.environ[models_dict['sas_token']],\n",
    "            table_name=models_dict['inference_sf_table_name'],\n",
    "            database=sf.connection_inputs['database'],\n",
    "            schema=sf.connection_inputs['schema'],\n",
    "            skip_header='1',\n",
    "            file_type='csv',\n",
    "            pattern='.*.csv')\n",
    "        sf.run_sql_str(adls_query)\n",
    "\n",
    "        exp_table = sf.run_sql_str(f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {models_dict['inference_sf_table_name']}\n",
    "        WHERE DATE_CREATED = '{date_created}'\n",
    "        AND EXPERIMENT = '{exp_name}'\n",
    "        LIMIT 3\n",
    "        \"\"\")\n",
    "        logging.info(f'preview of queried table being added:\\n{exp_table.head(3)}')\n",
    "        logging.info(f'preview predictions values addes:\\n{exp_table.iloc[0].values}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:data_system_utilities.snowflake.utils:connection to snowflake established...\n",
      "INFO:data_system_utilities.snowflake.query:executing query\n",
      "INFO:data_system_utilities.snowflake.query:data loaded from snowflake\n",
      "INFO:data_system_utilities.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:data_system_utilities.snowflake.utils:stage_query: \n",
      " create or replace stage ltbpFY23LocalRunTest\n",
      "url='azure://vaildtscadls.blob.core.windows.net/vailadls/projects/LTBP/FY23/experiments/BASELINE'\n",
      "credentials=(azure_sas_token='**MASKED**')\n",
      "encryption=(type= 'NONE')\n",
      "file_format = (type = parquet        )\n",
      "INFO:data_system_utilities.snowflake.utils:connection to snowflake established...\n",
      "INFO:data_system_utilities.snowflake.query:executing query\n",
      "INFO:data_system_utilities.snowflake.query:data loaded from snowflake\n",
      "INFO:data_system_utilities.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:data_system_utilities.snowflake.query:Stage area LTBPFY23LOCALRUNTEST successfully created.\n",
      "INFO:root:adls snowflake stage query \n",
      "    select\n",
      "    $1:\"ECID\"::varchar as ECID\n",
      ", $1:\"SEASONYEAR\"::varchar as SEASONYEAR\n",
      ", $1:\"AGE\"::varchar as AGE\n",
      ", $1:\"AVGVISITPERSEASON\"::varchar as AVGVISITPERSEASON\n",
      ", $1:\"BOUGHTPASS\"::varchar as BOUGHTPASS\n",
      ", $1:\"DESTINATIONGEOAFINITYLABEL\"::varchar as DESTINATIONGEOAFINITYLABEL\n",
      ", $1:\"EVERCOREPASS\"::varchar as EVERCOREPASS\n",
      ", $1:\"EVERPASS\"::varchar as EVERPASS\n",
      ", $1:\"GENDERCODE\"::varchar as GENDERCODE\n",
      ", $1:\"GUESTBEHAVIOR\"::varchar as GUESTBEHAVIOR\n",
      ", $1:\"ISEPICMIXACTIVATED\"::varchar as ISEPICMIXACTIVATED\n",
      ", $1:\"MARKETINGZONE\"::varchar as MARKETINGZONE\n",
      ", $1:\"MOSTCOMMONTICKETCOMP\"::varchar as MOSTCOMMONTICKETCOMP\n",
      ", $1:\"MOSTSUBSEASONVISITED\"::varchar as MOSTSUBSEASONVISITED\n",
      ", $1:\"MOSTVISITEDREGION\"::varchar as MOSTVISITEDREGION\n",
      ", $1:\"MOSTVISITEDRESORT\"::varchar as MOSTVISITEDRESORT\n",
      ", $1:\"ONLYSINGLERESORTKEY\"::varchar as ONLYSINGLERESORTKEY\n",
      ", $1:\"PARTNERRESORTSCANNERFLAG\"::varchar as PARTNERRESORTSCANNERFLAG\n",
      ", $1:\"RESORTSVISITED\"::varchar as RESORTSVISITED\n",
      ", $1:\"SKIERABILITYLABEL\"::varchar as SKIERABILITYLABEL\n",
      ", $1:\"SUBSEASONSPERYEAR\"::varchar as SUBSEASONSPERYEAR\n",
      ", $1:\"TOTALSEASONSSCANNED\"::varchar as TOTALSEASONSSCANNED\n",
      ", $1:\"TOTALVISITS\"::varchar as TOTALVISITS\n",
      ", $1:\"VISITMOSTINPEAK\"::varchar as VISITMOSTINPEAK\n",
      "\n",
      "    from @ltbpFY23LocalRunTest/inference_data/\n",
      "    LIMIT 1000\n",
      "    \n",
      "INFO:data_system_utilities.snowflake.utils:connection to snowflake established...\n",
      "INFO:data_system_utilities.snowflake.query:executing query\n",
      "INFO:data_system_utilities.snowflake.query:data loaded from snowflake\n",
      "INFO:data_system_utilities.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:root:Preview dataframe queried         ECID SEASONYEAR AGE AVGVISITPERSEASON BOUGHTPASS  \\\n",
      "0  135726655    2021/22  14                 0          0   \n",
      "1   61268828    2021/22  14                 0          0   \n",
      "2   19101424    2021/22  42                 0          0   \n",
      "3   54383456    2021/22  17                 0          0   \n",
      "4     506510    2021/22  45                 0          0   \n",
      "\n",
      "  DESTINATIONGEOAFINITYLABEL EVERCOREPASS EVERPASS GENDERCODE GUESTBEHAVIOR  \\\n",
      "0                    Unknown            0        0          F          None   \n",
      "1                Destination            0        0          F   Lapsed Paid   \n",
      "2                      Local            0        1          M   Lapsed Pass   \n",
      "3                Destination            0        0          F   Lapsed Paid   \n",
      "4                Destination            0        0          M   Lapsed Paid   \n",
      "\n",
      "   ... MOSTVISITEDREGION MOSTVISITEDRESORT ONLYSINGLERESORTKEY  \\\n",
      "0  ...              None              None                None   \n",
      "1  ...              None              None                None   \n",
      "2  ...              None              None                None   \n",
      "3  ...              None              None                None   \n",
      "4  ...              None              None                None   \n",
      "\n",
      "  PARTNERRESORTSCANNERFLAG RESORTSVISITED SKIERABILITYLABEL SUBSEASONSPERYEAR  \\\n",
      "0                        0              0              None              None   \n",
      "1                        0              0              None              None   \n",
      "2                        0              0              None              None   \n",
      "3                        0              0              None              None   \n",
      "4                        0              0              None              None   \n",
      "\n",
      "  TOTALSEASONSSCANNED TOTALVISITS VISITMOSTINPEAK  \n",
      "0                   0           0               0  \n",
      "1                   0           0               0  \n",
      "2                   0           0               0  \n",
      "3                   0           0               0  \n",
      "4                   0           0               0  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "INFO:root:Loading Sklearn Object to: ./models/train_xgbLocalRunTestBASELINE.pkl\n",
      "INFO:data_system_utilities.azure.storage:Downloading projects/LTBP/FY23/experiments/BASELINE/modeling/train_xgb/train_xgbLocalRunTestBASELINE.pkl to ./models/train_xgbLocalRunTestBASELINE.pkl\n",
      "INFO:data_system_utilities.azure.storage:Download complete\n",
      "INFO:root:Sklearn Object Loaded\n",
      "INFO:root:preview predictions being added:\n",
      "        ECID SEASONYEAR  PROBABILITY CI_COMMIT_SHA         DATE_CREATED  \\\n",
      "0  135726655    2021/22     0.112221  LocalRunTest  2022-11-03 16:08:31   \n",
      "1   61268828    2021/22     0.136225  LocalRunTest  2022-11-03 16:08:31   \n",
      "2   19101424    2021/22     0.280357  LocalRunTest  2022-11-03 16:08:31   \n",
      "\n",
      "  EXPERIMENT  \n",
      "0   BASELINE  \n",
      "1   BASELINE  \n",
      "2   BASELINE  \n",
      "INFO:root:preview predictions values addes:\n",
      "['135726655' '2021/22' 0.11222139 'LocalRunTest' '2022-11-03 16:08:31'\n",
      " 'BASELINE']\n",
      "INFO:root:preview predictions being added columns:\n",
      "Index(['ECID', 'SEASONYEAR', 'PROBABILITY', 'CI_COMMIT_SHA', 'DATE_CREATED',\n",
      "       'EXPERIMENT'],\n",
      "      dtype='object')\n",
      "INFO:data_system_utilities.azure.storage:Uploading predictions_LocalRunTestBASELINE.csv, to Azure Storage projects/LTBP/FY23/experiments/BASELINE/predictions/train_xgb/predictions_LocalRunTestBASELINE.csv\n",
      "INFO:data_system_utilities.azure.storage:Azure Upload Complete\n",
      "INFO:data_system_utilities.snowflake.utils:connection to snowflake established...\n",
      "INFO:data_system_utilities.snowflake.query:executing query\n",
      "INFO:data_system_utilities.snowflake.query:data loaded from snowflake\n",
      "INFO:data_system_utilities.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:root:Pushing Forecasted Season from ADLS to Snowflake\n",
      "INFO:data_system_utilities.snowflake.copyinto:\n",
      "copy into MACHINELEARNINGOUTPUTS.LTBP.LTBP_PREDICTIONS_FY23\n",
      "from 'azure://vaildtscadls.blob.core.windows.net/vailadls/projects/LTBP/FY23/experiments/BASELINE/predictions/train_xgb/predictions_LocalRunTestBASELINE.csv'\n",
      "file_format = (type = csv     skip_header = 1)\n",
      "credentials= (azure_sas_token = '?sv=2019-12-12&ss=bfqt&srt=sco&sp=rwdlacupx&se=2031-01-22T06:17:14Z&st=2021-01-21T22:17:14Z&spr=https&sig=kIHogByJjyVWyL6XupA0CBUB1iw12%2FeXWFQiOj5fB5c%3D')\n",
      "pattern = '.*.csv';\n",
      "INFO:data_system_utilities.snowflake.utils:connection to snowflake established...\n",
      "INFO:data_system_utilities.snowflake.query:executing query\n",
      "INFO:data_system_utilities.snowflake.query:data loaded from snowflake\n",
      "INFO:data_system_utilities.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:data_system_utilities.snowflake.utils:connection to snowflake established...\n",
      "INFO:data_system_utilities.snowflake.query:executing query\n",
      "INFO:data_system_utilities.snowflake.query:data loaded from snowflake\n",
      "INFO:data_system_utilities.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:root:preview of queried table being added:\n",
      "        ECID SEASONYEAR  PROBABILITY CI_COMMIT_SHA         DATE_CREATED  \\\n",
      "0  135726655    2021/22     0.112221  LocalRunTest  2022-11-03 16:08:31   \n",
      "1   61268828    2021/22     0.136225  LocalRunTest  2022-11-03 16:08:31   \n",
      "2   19101424    2021/22     0.280357  LocalRunTest  2022-11-03 16:08:31   \n",
      "\n",
      "  EXPERIMENT  \n",
      "0   BASELINE  \n",
      "1   BASELINE  \n",
      "2   BASELINE  \n",
      "INFO:root:preview predictions values addes:\n",
      "['135726655' '2021/22' 0.11222139 'LocalRunTest' '2022-11-03 16:08:31'\n",
      " 'BASELINE']\n",
      "INFO:data_system_utilities.snowflake.utils:stage_query: \n",
      " create or replace stage ltbpFY23LocalRunTest\n",
      "url='azure://vaildtscadls.blob.core.windows.net/vailadls/projects/LTBP/FY23/experiments/NOHYPEROPT'\n",
      "credentials=(azure_sas_token='**MASKED**')\n",
      "encryption=(type= 'NONE')\n",
      "file_format = (type = parquet        )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:data_system_utilities.snowflake.utils:connection to snowflake established...\n",
      "INFO:data_system_utilities.snowflake.query:executing query\n",
      "INFO:data_system_utilities.snowflake.query:data loaded from snowflake\n",
      "INFO:data_system_utilities.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:data_system_utilities.snowflake.query:Stage area LTBPFY23LOCALRUNTEST successfully created.\n",
      "INFO:root:adls snowflake stage query \n",
      "    select\n",
      "    $1:\"ECID\"::varchar as ECID\n",
      ", $1:\"SEASONYEAR\"::varchar as SEASONYEAR\n",
      ", $1:\"AGE\"::varchar as AGE\n",
      ", $1:\"AVGVISITPERSEASON\"::varchar as AVGVISITPERSEASON\n",
      ", $1:\"BOUGHTPASS\"::varchar as BOUGHTPASS\n",
      ", $1:\"DESTINATIONGEOAFINITYLABEL\"::varchar as DESTINATIONGEOAFINITYLABEL\n",
      ", $1:\"EVERCOREPASS\"::varchar as EVERCOREPASS\n",
      ", $1:\"EVERPASS\"::varchar as EVERPASS\n",
      ", $1:\"GENDERCODE\"::varchar as GENDERCODE\n",
      ", $1:\"GUESTBEHAVIOR\"::varchar as GUESTBEHAVIOR\n",
      ", $1:\"ISEPICMIXACTIVATED\"::varchar as ISEPICMIXACTIVATED\n",
      ", $1:\"MARKETINGZONE\"::varchar as MARKETINGZONE\n",
      ", $1:\"MOSTCOMMONTICKETCOMP\"::varchar as MOSTCOMMONTICKETCOMP\n",
      ", $1:\"MOSTSUBSEASONVISITED\"::varchar as MOSTSUBSEASONVISITED\n",
      ", $1:\"MOSTVISITEDREGION\"::varchar as MOSTVISITEDREGION\n",
      ", $1:\"MOSTVISITEDRESORT\"::varchar as MOSTVISITEDRESORT\n",
      ", $1:\"ONLYSINGLERESORTKEY\"::varchar as ONLYSINGLERESORTKEY\n",
      ", $1:\"PARTNERRESORTSCANNERFLAG\"::varchar as PARTNERRESORTSCANNERFLAG\n",
      ", $1:\"RESORTSVISITED\"::varchar as RESORTSVISITED\n",
      ", $1:\"SKIERABILITYLABEL\"::varchar as SKIERABILITYLABEL\n",
      ", $1:\"SUBSEASONSPERYEAR\"::varchar as SUBSEASONSPERYEAR\n",
      ", $1:\"TOTALSEASONSSCANNED\"::varchar as TOTALSEASONSSCANNED\n",
      ", $1:\"TOTALVISITS\"::varchar as TOTALVISITS\n",
      ", $1:\"VISITMOSTINPEAK\"::varchar as VISITMOSTINPEAK\n",
      "\n",
      "    from @ltbpFY23LocalRunTest/inference_data/\n",
      "    LIMIT 1000\n",
      "    \n",
      "INFO:data_system_utilities.snowflake.utils:connection to snowflake established...\n",
      "INFO:data_system_utilities.snowflake.query:executing query\n",
      "INFO:data_system_utilities.snowflake.query:data loaded from snowflake\n",
      "INFO:data_system_utilities.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:root:Preview dataframe queried        ECID SEASONYEAR  AGE AVGVISITPERSEASON BOUGHTPASS  \\\n",
      "0   2944820    2021/22   19                 0          0   \n",
      "1  57022042    2021/22  119                 0          0   \n",
      "2  35572199    2021/22   24                 0          0   \n",
      "3  63462035    2021/22   44                 0          0   \n",
      "4  58190809    2021/22  119                 0          0   \n",
      "\n",
      "  DESTINATIONGEOAFINITYLABEL EVERCOREPASS EVERPASS GENDERCODE GUESTBEHAVIOR  \\\n",
      "0                      Local            0        0          M   Lapsed Paid   \n",
      "1                Destination            0        0          M   Lapsed Paid   \n",
      "2                    Unknown            0        0          F   Lapsed Paid   \n",
      "3                      Local            0        0          M   Lapsed Paid   \n",
      "4                    Unknown            0        0          F   Lapsed Paid   \n",
      "\n",
      "   ... MOSTVISITEDREGION MOSTVISITEDRESORT ONLYSINGLERESORTKEY  \\\n",
      "0  ...              None              None                None   \n",
      "1  ...              None              None                None   \n",
      "2  ...              None              None                None   \n",
      "3  ...              None              None                None   \n",
      "4  ...              None              None                None   \n",
      "\n",
      "  PARTNERRESORTSCANNERFLAG RESORTSVISITED SKIERABILITYLABEL SUBSEASONSPERYEAR  \\\n",
      "0                        0              0              None              None   \n",
      "1                        0              0              None              None   \n",
      "2                        0              0              None              None   \n",
      "3                        0              0              None              None   \n",
      "4                        0              0              None              None   \n",
      "\n",
      "  TOTALSEASONSSCANNED TOTALVISITS VISITMOSTINPEAK  \n",
      "0                   0           0               0  \n",
      "1                   0           0               0  \n",
      "2                   0           0               0  \n",
      "3                   0           0               0  \n",
      "4                   0           0               0  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "INFO:root:Loading Sklearn Object to: ./models/train_xgb_basicLocalRunTestNOHYPEROPT.pkl\n",
      "INFO:data_system_utilities.azure.storage:Downloading projects/LTBP/FY23/experiments/NOHYPEROPT/modeling/train_xgb_basic/train_xgb_basicLocalRunTestNOHYPEROPT.pkl to ./models/train_xgb_basicLocalRunTestNOHYPEROPT.pkl\n",
      "INFO:data_system_utilities.azure.storage:Download complete\n",
      "INFO:root:Sklearn Object Loaded\n",
      "INFO:root:preview predictions being added:\n",
      "       ECID SEASONYEAR  PROBABILITY CI_COMMIT_SHA         DATE_CREATED  \\\n",
      "0   2944820    2021/22     0.022440  LocalRunTest  2022-11-03 16:08:38   \n",
      "1  57022042    2021/22     0.024810  LocalRunTest  2022-11-03 16:08:38   \n",
      "2  35572199    2021/22     0.021653  LocalRunTest  2022-11-03 16:08:38   \n",
      "\n",
      "   EXPERIMENT  \n",
      "0  NOHYPEROPT  \n",
      "1  NOHYPEROPT  \n",
      "2  NOHYPEROPT  \n",
      "INFO:root:preview predictions values addes:\n",
      "['2944820' '2021/22' 0.022440042 'LocalRunTest' '2022-11-03 16:08:38'\n",
      " 'NOHYPEROPT']\n",
      "INFO:root:preview predictions being added columns:\n",
      "Index(['ECID', 'SEASONYEAR', 'PROBABILITY', 'CI_COMMIT_SHA', 'DATE_CREATED',\n",
      "       'EXPERIMENT'],\n",
      "      dtype='object')\n",
      "INFO:data_system_utilities.azure.storage:Uploading predictions_LocalRunTestNOHYPEROPT.csv, to Azure Storage projects/LTBP/FY23/experiments/NOHYPEROPT/predictions/train_xgb_basic/predictions_LocalRunTestNOHYPEROPT.csv\n",
      "INFO:data_system_utilities.azure.storage:Azure Upload Complete\n",
      "INFO:data_system_utilities.snowflake.utils:connection to snowflake established...\n",
      "INFO:data_system_utilities.snowflake.query:executing query\n",
      "INFO:data_system_utilities.snowflake.query:data loaded from snowflake\n",
      "INFO:data_system_utilities.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:root:Pushing Forecasted Season from ADLS to Snowflake\n",
      "INFO:data_system_utilities.snowflake.copyinto:\n",
      "copy into MACHINELEARNINGOUTPUTS.LTBP.LTBP_PREDICTIONS_FY23\n",
      "from 'azure://vaildtscadls.blob.core.windows.net/vailadls/projects/LTBP/FY23/experiments/NOHYPEROPT/predictions/train_xgb_basic/predictions_LocalRunTestNOHYPEROPT.csv'\n",
      "file_format = (type = csv     skip_header = 1)\n",
      "credentials= (azure_sas_token = '?sv=2019-12-12&ss=bfqt&srt=sco&sp=rwdlacupx&se=2031-01-22T06:17:14Z&st=2021-01-21T22:17:14Z&spr=https&sig=kIHogByJjyVWyL6XupA0CBUB1iw12%2FeXWFQiOj5fB5c%3D')\n",
      "pattern = '.*.csv';\n",
      "INFO:data_system_utilities.snowflake.utils:connection to snowflake established...\n",
      "INFO:data_system_utilities.snowflake.query:executing query\n",
      "INFO:data_system_utilities.snowflake.query:data loaded from snowflake\n",
      "INFO:data_system_utilities.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:data_system_utilities.snowflake.utils:connection to snowflake established...\n",
      "INFO:data_system_utilities.snowflake.query:executing query\n",
      "INFO:data_system_utilities.snowflake.query:data loaded from snowflake\n",
      "INFO:data_system_utilities.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:root:preview of queried table being added:\n",
      "       ECID SEASONYEAR  PROBABILITY CI_COMMIT_SHA         DATE_CREATED  \\\n",
      "0   2944820    2021/22     0.022440  LocalRunTest  2022-11-03 16:08:38   \n",
      "1  57022042    2021/22     0.024810  LocalRunTest  2022-11-03 16:08:38   \n",
      "2  35572199    2021/22     0.021653  LocalRunTest  2022-11-03 16:08:38   \n",
      "\n",
      "   EXPERIMENT  \n",
      "0  NOHYPEROPT  \n",
      "1  NOHYPEROPT  \n",
      "2  NOHYPEROPT  \n",
      "INFO:root:preview predictions values addes:\n",
      "['2944820' '2021/22' 0.022440042 'LocalRunTest' '2022-11-03 16:08:38'\n",
      " 'NOHYPEROPT']\n"
     ]
    }
   ],
   "source": [
    "#| skip\n",
    "experiment_name = 'BASELINE'\n",
    "experiment = True\n",
    "yaml_file_list = ['features.yaml', 'udf_inputs.yaml', 'etl.yaml', 'models.yaml']\n",
    "sfSchema='LTBP'\n",
    "\n",
    "_, udf_inputs, etl_dict, models_dict = get_yaml_dicts(yaml_file_list)\n",
    "\n",
    "adls_paths = []\n",
    "model_names = []\n",
    "experiment_names = []\n",
    "experiments = []\n",
    "if sfSchema.lower() != 'dev':\n",
    "    sf = snowflake_query(sfSchema=sfSchema)\n",
    "    prod_model = sf.run_sql_str(f'''SELECT * \n",
    "    FROM MACHINELEARNINGOUTPUTS.{sfSchema}.{models_dict['tracking_table']}\n",
    "    WHERE PRODUCTION_MODEL\n",
    "    ''')\n",
    "    for i, v in prod_model.iterrows():\n",
    "        adls_path = os.path.join(\n",
    "        (os.path.join(etl_dict['data_lake_path'], 'experiments', v['EXPERIMENT_NAME'])\n",
    "          if v['EXPERIMENT']\n",
    "          else os.path.join(\n",
    "              etl_dict['data_lake_path'], v['COMMITID'], v['EXPERIMENT_NAME'])))\n",
    "        adls_paths.append(adls_path)\n",
    "        model_name = (models_dict[v['EXPERIMENT_NAME']]['model_trainer']+\n",
    "                      v['COMMITID']+\n",
    "                      v['EXPERIMENT_NAME']+'.pkl'\n",
    "                     )\n",
    "        model_names.append(model_name)\n",
    "        experiment_names.append(v['EXPERIMENT_NAME'])\n",
    "        experiments.append(v['EXPERIMENT'])\n",
    "else:\n",
    "    adls_path = os.path.join(\n",
    "            (os.path.join(etl_dict['data_lake_path'], 'experiments', experiment_name)\n",
    "             if experiment\n",
    "             else os.path.join(\n",
    "                etl_dict['data_lake_path'], os.environ.get('CI_COMMIT_SHA', 'LocalRunNBS'))))\n",
    "    adls_paths.append(adls_path)\n",
    "    model_name = (models_dict[experiment_name]['model_trainer']\n",
    "                  + os.environ.get('CI_COMMIT_SHA', 'LocalRunNBS')\n",
    "                  + experiment_name+'.pkl'\n",
    "                  )\n",
    "    model_names.append(model_name)\n",
    "    experiment_names.append(experiment_name)\n",
    "    experiments.append(experiment)\n",
    "\n",
    "for adl_path, model_name, exp_name in zip(adls_paths, model_names, experiment_names):\n",
    "    df_infer = create_stage_and_query_stage_sf(\n",
    "        sf=sf,\n",
    "        etl=etl_dict,\n",
    "        udf_inputs=udf_inputs,\n",
    "        train_or_inference='INFERENCE',\n",
    "        experiment_name=exp_name,\n",
    "        experiment=experiment,\n",
    "        indentification=models_dict['identification'],\n",
    "        extra_statement='LIMIT 1000'  # Can add limit when experimenting 'LIMIT 1000'\n",
    "    )\n",
    "    model = pull_sklearn_object_from_adls(\n",
    "            adls_path=os.path.join(adl_path,\n",
    "                                   models_dict['modeling_adls_path'],\n",
    "                                   models_dict[exp_name]['model_trainer']\n",
    "                                  ) + '/',\n",
    "            file_name=model_name,\n",
    "            drop_local_path='./models/',\n",
    "            container_name=etl_dict['azure_container'],\n",
    "            connection_str=os.environ[models_dict['connection_str']]\n",
    "        )\n",
    "    sf_df = df_infer[models_dict['identification']].copy()\n",
    "    # Change Here Name change for a regression and to predict or multi-labled needs some work\n",
    "    sf_df['PROBABILITY'] = model.predict_proba(df_infer)[:, 1]\n",
    "    date_created = datetime.datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    sf_df['CI_COMMIT_SHA'] = os.environ.get('CI_COMMIT_SHA', 'LocalRunNBS')\n",
    "    sf_df['DATE_CREATED'] = datetime.datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    sf_df['EXPERIMENT'] = exp_name\n",
    "    file_name = f\"predictions_{os.environ.get('CI_COMMIT_SHA','LocalRunNBS')+exp_name}.csv\"\n",
    "    # Saving as a .csv for simple reading from adls download using dask would be best here\n",
    "    sf_df.to_csv(file_name, index=False)\n",
    "    logging.info(f'preview predictions being added:\\n{sf_df.head(3)}')\n",
    "    logging.info(f'preview predictions values addes:\\n{sf_df.iloc[0].values}')\n",
    "    logging.info(f'preview predictions being added columns:\\n{sf_df.columns}')\n",
    "    az = FileHandling(os.environ[models_dict['connection_str']])\n",
    "    az.upload_file(\n",
    "        azure_file_path=os.path.join(adl_path,\n",
    "                                     models_dict['predictions_adls_path'],\n",
    "                                     models_dict[exp_name]['model_trainer']),\n",
    "        local_file_path=file_name,\n",
    "        container_name=etl_dict['azure_container'],\n",
    "        overwrite=True,\n",
    "    )\n",
    "    os.unlink(file_name)\n",
    "    stage_url = f\"azure://{etl_dict['azure_account']}.blob.core.windows.net/{etl_dict['azure_container']}/\"\n",
    "    preds_file_path = os.path.join(adl_path,\n",
    "                                   models_dict['predictions_adls_path'],\n",
    "                                   models_dict[exp_name]['model_trainer'],\n",
    "                                   file_name)\n",
    "\n",
    "    sf = snowflake_query(sfSchema=sfSchema)\n",
    "    if models_dict['inference_sf_table_name'].upper() not in sf.run_sql_str(\"show tables;\").name.tolist():\n",
    "        sf.run_sql_str(create_table_query_from_df(sf_df, table_name_sf=models_dict['inference_sf_table_name'], varchar=False))\n",
    "    logging.info(\"Pushing Forecasted Season from ADLS to Snowflake\")\n",
    "    adls_query = adls_url_to_sf_query_generator(\n",
    "        azure_path=os.path.join(stage_url, preds_file_path),\n",
    "        azure_sas_token=os.environ[models_dict['sas_token']],\n",
    "        table_name=models_dict['inference_sf_table_name'],\n",
    "        database=sf.connection_inputs['database'],\n",
    "        schema=sf.connection_inputs['schema'],\n",
    "        skip_header='1',\n",
    "        file_type='csv',\n",
    "        pattern='.*.csv')\n",
    "    sf.run_sql_str(adls_query)\n",
    "\n",
    "    exp_table = sf.run_sql_str(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM {models_dict['inference_sf_table_name']}\n",
    "    WHERE DATE_CREATED = '{date_created}'\n",
    "    AND EXPERIMENT = '{exp_name}'\n",
    "    LIMIT 3\n",
    "    \"\"\")\n",
    "    logging.info(f'preview of queried table being added:\\n{exp_table.head(3)}')\n",
    "    logging.info(f'preview predictions values addes:\\n{exp_table.iloc[0].values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
