{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Utils\n",
    "\n",
    "> Functions Used In Modeling Efforts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp modeling.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeremydemlow/miniforge3/envs/ltbp/lib/python3.9/site-packages/snowflake/connector/options.py:96: UserWarning: You have an incompatible version of 'pyarrow' installed (6.0.0), please install a version that adheres to: 'pyarrow<8.1.0,>=8.0.0; extra == \"pandas\"'\n",
      "  warn_incompatible_dep(\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "from data_system_utilities.azure.storage import FileHandling\n",
    "from data_system_utilities.snowflake.utils import make_stage_query_generator\n",
    "\n",
    "from machine_learning_utilities import preprocessing\n",
    "\n",
    "from LTBP.data.utils import snowflake_query, get_yaml_dicts, generate_data_lake_query\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from rfpimp import *  # noqa:\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def return_dict_type(\n",
    "    pre_process_type: dict  # {k:v} dictionary of columns name and tranformation type\n",
    "):\n",
    "    \"\"\"\n",
    "    Simplify the standard process for sklearn preprocessing pipelines\n",
    "    \"\"\"\n",
    "    for k, v in pre_process_type.items():\n",
    "        if v == \"OrdinalEncoder\":\n",
    "            pre_process_dict = {\n",
    "                f\"{k}\": {\n",
    "                    \"transformation\": {\n",
    "                        \"name\": \"OrdinalEncoder\",\n",
    "                        \"args\": {\n",
    "                            \"handle_unknown\": \"use_encoded_value\",\n",
    "                            \"unknown_value\": -1,\n",
    "                        },\n",
    "                    },\n",
    "                    \"variable_type\": \"cat\",\n",
    "                }\n",
    "            }\n",
    "        if v == \"OneHotEncoder\":\n",
    "            pre_process_dict = {\n",
    "                f\"{k}\": {\n",
    "                    \"transformation\": {\n",
    "                        \"name\": \"OneHotEncoder\",\n",
    "                        \"args\": {\"handle_unknown\": \"ignore\", \"sparse\": False},\n",
    "                    },\n",
    "                    \"variable_type\": \"cat\",\n",
    "                }\n",
    "            }\n",
    "        if v == \"StandardScaler\":\n",
    "            pre_process_dict = {\n",
    "                f\"{k}\": {\n",
    "                    \"transformation\": {\"name\": \"StandardScaler\", \"args\": {}},\n",
    "                    \"variable_type\": \"cont\",\n",
    "                }\n",
    "            }\n",
    "        if v == \"RobustScaler\":\n",
    "            pre_process_dict = {\n",
    "                f\"{k}\": {\n",
    "                    \"transformation\": {\"name\": \"RobustScaler\", \"args\": {}},\n",
    "                    \"variable_type\": \"cont\",\n",
    "                }\n",
    "            }\n",
    "    return pre_process_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'destinationgeoafinitylabel': {'transformation': {'name': 'OrdinalEncoder',\n",
       "   'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}},\n",
       "  'variable_type': 'cat'}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data_system_utilities.file_parsers import yaml\n",
    "\n",
    "features=yaml.yaml_reader('./LTBP/files/yaml_files/features.yaml')\n",
    "experiment_name='BASELINE'\n",
    "cat_vars =[{f.lower() : values['transformation'][experiment_name]} for f, values in features.items() \n",
    "            if values['var_type'][experiment_name] == 'cat'\n",
    "            and values['input_definition'] != 'LABEL']\n",
    "\n",
    "return_dict_type(cat_vars[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq(return_dict_type(cat_vars[0]).keys(), cat_vars[0].keys())\n",
    "test_eq(return_dict_type(cat_vars[0])[list(cat_vars[0].keys())[0]].keys(), ['transformation', 'variable_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def create_sklearn_preprocess_baseline_dict(\n",
    "    cat_vars: list,  # list of categorical variables with sklearn transformer\n",
    "    cont_vars: list,  # list of continous variables with sklearn transformer\n",
    "):\n",
    "    \"\"\"wrapper around ``return_dict_type`` to go through cat and cont vars\n",
    "    \"\"\"\n",
    "    final_dict = {}\n",
    "    if cat_vars is None:\n",
    "        cat_vars = []\n",
    "    if cont_vars is None:\n",
    "        cont_vars = []\n",
    "    for item in cat_vars + cont_vars:\n",
    "        final_dict.update(return_dict_type(item))\n",
    "    return final_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:{'destinationgeoafinitylabel': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'evercorepass': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'everpass': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'gendercode': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'guestbehavior': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'isepicmixactivated': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'marketingzone': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'mostcommonticketcomp': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'mostsubseasonvisited': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'mostvisitedregion': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'mostvisitedresort': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'onlysingleresortkey': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'partnerresortscannerflag': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'skierabilitylabel': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'totalseasonsscanned': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'visitmostinpeak': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'age': {'transformation': {'name': 'StandardScaler', 'args': {}}, 'variable_type': 'cont'}, 'avgvisitperseason': {'transformation': {'name': 'StandardScaler', 'args': {}}, 'variable_type': 'cont'}, 'resortsvisited': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'subseasonsperyear': {'transformation': {'name': 'StandardScaler', 'args': {}}, 'variable_type': 'cont'}, 'totalvisits': {'transformation': {'name': 'StandardScaler', 'args': {}}, 'variable_type': 'cont'}}\n"
     ]
    }
   ],
   "source": [
    "features=yaml.yaml_reader('./LTBP/files/yaml_files/features.yaml')\n",
    "experiment_name='BASELINE'\n",
    "\n",
    "cat_vars =[{f.lower() : values['transformation'][experiment_name]} for f, values in features.items() \n",
    "            if values['var_type'][experiment_name] == 'cat'\n",
    "            and values['input_definition'] != 'LABEL']\n",
    "cont_vars =[{f.lower(): values['transformation'][experiment_name]} for f, values in features.items() \n",
    "            if values['var_type'][experiment_name] == 'cont'\n",
    "            and values['input_definition'] != 'LABEL']\n",
    "\n",
    "feature_dict = create_sklearn_preprocess_baseline_dict(cat_vars=cat_vars, \n",
    "                                                       cont_vars=cont_vars)\n",
    "logging.info(feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq(feature_dict[list(cat_vars[0].keys())[0]].keys(), ['transformation', 'variable_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def return_list_of_vars(variables):\n",
    "    \"\"\"returns lists key\"\"\"\n",
    "    if variables is None:\n",
    "        return None\n",
    "    vars_list = []\n",
    "    for item in variables:\n",
    "        for k in item.keys():\n",
    "            vars_list.append(k)\n",
    "    return vars_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:categorical variables: \n",
      " ['destinationgeoafinitylabel', 'evercorepass', 'everpass', 'gendercode', 'guestbehavior', 'isepicmixactivated', 'marketingzone', 'mostcommonticketcomp', 'mostsubseasonvisited', 'mostvisitedregion', 'mostvisitedresort', 'onlysingleresortkey', 'partnerresortscannerflag', 'skierabilitylabel', 'totalseasonsscanned', 'visitmostinpeak']\n",
      "INFO:root:continous variables: \n",
      " ['age', 'avgvisitperseason', 'resortsvisited', 'subseasonsperyear', 'totalvisits']\n"
     ]
    }
   ],
   "source": [
    "cat_vars = return_list_of_vars(cat_vars)\n",
    "cont_vars = return_list_of_vars(cont_vars)\n",
    "logging.info(f'categorical variables: \\n {cat_vars}')\n",
    "logging.info(f'continous variables: \\n {cont_vars}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "feature_dict=yaml.yaml_reader('./LTBP/files/yaml_files/features.yaml')\n",
    "test_eq(cat_vars, [f.lower() for f, values in features.items() \n",
    "                    if values['var_type'][experiment_name] == 'cat'\n",
    "                    and values['input_definition'] != 'LABEL'])\n",
    "test_eq(cont_vars, [f.lower() for f, values in features.items() \n",
    "                    if values['var_type'][experiment_name] == 'cont'\n",
    "                    and values['input_definition'] != 'LABEL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def prepare_training_set(df: pd.DataFrame,\n",
    "                         y_var: list,\n",
    "                         y_scaler_type: object,\n",
    "                         sklearn_pipe: object,\n",
    "                         etl_dict: dict,\n",
    "                         models_dict: dict,\n",
    "                         adls_path: str,\n",
    "                         experiment_name: str,\n",
    "                         connection_str: str,\n",
    "                         identifiers: list = ['ECID'],\n",
    "                         test_set: bool = True,\n",
    "                         validation_split: float = .20,\n",
    "                         test_split: float = .15,\n",
    "                         seed: int = 1320,\n",
    "                         as_type=int):\n",
    "    \"\"\"TODO: Working on Multi-Col Labels split and preprocess data set for model training purposes\"\"\"\n",
    "    scaler = y_scaler_type\n",
    "    # Sklearn basic split method\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(df,\n",
    "                                                          df[y_var].astype(as_type),\n",
    "                                                          test_size=validation_split,\n",
    "                                                          random_state=seed)\n",
    "    if test_set is True:\n",
    "        X_valid, X_test, y_valid, y_test = train_test_split(X_valid,\n",
    "                                                            y_valid,\n",
    "                                                            test_size=test_split,\n",
    "                                                            random_state=seed)\n",
    "        logging.info(f'Successfully Spilt Data\\nTrain: {X_train.shape}, {y_train.shape}\\nValid: {X_valid.shape}, {y_valid.shape}\\nTest: {X_test.shape}, {y_test.shape}')\n",
    "    else:\n",
    "        y_test = None\n",
    "        X_test = None\n",
    "        logging.info(f'Successfully Spilt Data\\nTrain: {X_train.shape}, {y_train.shape}\\nValid: {X_valid.shape}, {y_valid.shape}')\n",
    "    id_list = X_test[identifiers] if test_set is True else X_valid[identifiers]\n",
    "    logging.info(f'Size of the id_list for the hold set {id_list.shape}')\n",
    "    if scaler:\n",
    "        y_train = scaler.fit_transform(y_train.reset_index()[y_var[0]])\n",
    "        y_train = pd.DataFrame(y_train)\n",
    "        y_train.columns = [y_var]\n",
    "        y_valid = scaler.transform(y_valid.reset_index()[y_var])\n",
    "        y_valid = pd.DataFrame(y_valid)\n",
    "        y_valid.columns = [y_var]\n",
    "        if test_set is True:\n",
    "            y_test = scaler.transform(y_test.reset_index()[y_var])\n",
    "            y_test = pd.DataFrame(y_test)\n",
    "            y_test.columns = [y_var]\n",
    "    else:\n",
    "        logging.info('This project relies on the query to have accurate labels with no preprocessing..')\n",
    "        y_train = y_train.reset_index()[y_var]\n",
    "        y_train = pd.DataFrame(y_train)\n",
    "        y_train.columns = [y_var]\n",
    "        y_valid = y_valid.reset_index()[y_var]\n",
    "        y_valid = pd.DataFrame(y_valid)\n",
    "        y_valid.columns = [y_var]\n",
    "        if test_set is True:\n",
    "            y_test = y_test.reset_index()[y_var]\n",
    "            y_test = pd.DataFrame(y_test)\n",
    "            y_test.columns = [y_var]\n",
    "\n",
    "    if scaler:\n",
    "        logging.info('saving y_var scaler to adls')\n",
    "        save_sklearn_object_to_data_lake(save_object=scaler,\n",
    "                                         file_name=(os.environ.get('CI_COMMIT_SHA', 'LocalRunNBS')\n",
    "                                                    + models_dict[experiment_name]['y_preprocess_object_name']),\n",
    "                                         adls_path=adls_path,\n",
    "                                         container_name=etl_dict['azure_container'],\n",
    "                                         connection_str=connection_str)\n",
    "\n",
    "    X_train = sklearn_pipe.fit_transform(X_train)\n",
    "    cols = preprocessing.get_column_names_from_transformer(sklearn_pipe)\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    X_train.columns = cols\n",
    "\n",
    "    X_valid = sklearn_pipe.transform(X_valid)\n",
    "    cols = preprocessing.get_column_names_from_transformer(sklearn_pipe)\n",
    "    X_valid = pd.DataFrame(X_valid)\n",
    "    X_valid.columns = cols\n",
    "    if test_set is True:\n",
    "        X_test = sklearn_pipe.transform(X_test)\n",
    "        cols = preprocessing.get_column_names_from_transformer(sklearn_pipe)\n",
    "        X_test = pd.DataFrame(X_test)\n",
    "        X_test.columns = cols\n",
    "\n",
    "    save_sklearn_object_to_data_lake(save_object=sklearn_pipe,\n",
    "                                     file_name=(os.environ.get('CI_COMMIT_SHA', 'LocalRunNBS')\n",
    "                                                + models_dict[experiment_name]['x_preprocess_object_name']),\n",
    "                                     adls_path=adls_path,\n",
    "                                     container_name=etl_dict['azure_container'],\n",
    "                                     connection_str=connection_str)\n",
    "    return X_train, X_valid, X_test, y_train, y_valid, y_test, sklearn_pipe, scaler, id_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler_type = None\n",
    "# test_set=True\n",
    "# experiment_name='BASELINE'\n",
    "# sklearn_pipe=pipe\n",
    "# etl=etl\n",
    "# models=models\n",
    "\n",
    "# adls_path = os.path.join((os.path.join(etl['data_lake_path'], 'experiments', experiment_name)\n",
    "#     if experiment \n",
    "#     else os.path.join(etl_dict['data_lake_path'], \n",
    "#     os.environ.get('CI_COMMIT_SHA', 'LocalRunNBS')))\n",
    "#     , models['preprocessors_adls_path'])\n",
    "# result = prepare_training_set(df,\n",
    "#                               y_var=[k.upper() for k, v in features.items() if v['input_definition'] == 'LABEL'],\n",
    "#                               y_scaler_type=models[experiment_name]['y_scaler_type'],\n",
    "#                               adls_path=adls_path,\n",
    "#                               sklearn_pipe=pipe,\n",
    "#                               test_set=True,\n",
    "#                               etl_dict=etl,\n",
    "#                               models_dict=models,\n",
    "#                               connection_str=os.environ[models[\"connection_str\"]],\n",
    "#                               identifiers=['ECID']\n",
    "#                               )\n",
    "# X_train, X_valid, X_test, y_train, y_valid, y_test, sklearn_pipe, scaler, id_list = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one needs a test, but as of right now holding off on this test as this test would take a little big more time than I want to spend on the documentation of this process at the current moment\n",
    "\n",
    "The idea of this function is to ensure that the user is using the pre-processor in the correct fashion so that the validation set is not being considered in the pre-processing dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def preprocess_data(X_train, X_valid, X_test, sklearn_pipe):\n",
    "    X_train = sklearn_pipe.fit_transform(X_train)\n",
    "    cols = preprocessing.get_column_names_from_transformer(sklearn_pipe)\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    X_train.columns = cols\n",
    "\n",
    "    X_valid = sklearn_pipe.transform(X_valid)\n",
    "    cols = preprocessing.get_column_names_from_transformer(sklearn_pipe)\n",
    "    X_valid = pd.DataFrame(X_valid)\n",
    "    X_valid.columns = cols\n",
    "\n",
    "    X_test = sklearn_pipe.transform(X_test)\n",
    "    cols = preprocessing.get_column_names_from_transformer(sklearn_pipe)\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "    X_test.columns = cols\n",
    "\n",
    "    return X_train, X_valid, X_test, sklearn_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save_sklearn_object_to_data_lake\n",
    "\n",
    "This function is simply wrapping DSU functionality together to allow for a model to be pushed to adls these tests are written and evaluated inside of DSU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def save_sklearn_object_to_data_lake(\n",
    "    save_object, file_name, adls_path, container_name, connection_str\n",
    "):\n",
    "    \"\"\"moves a sklearn object to azure data lake as a pickle file at a given path\"\"\"\n",
    "    logging.info(\n",
    "        f\"Pushing Sklearn Object to Azure: {os.path.join(adls_path, file_name)}\"\n",
    "    )\n",
    "    with open(file_name, \"wb\") as f:\n",
    "        pickle.dump(save_object, f)\n",
    "    az = FileHandling(connection_str)\n",
    "    az.upload_file(\n",
    "        azure_file_path=adls_path,\n",
    "        local_file_path=file_name,\n",
    "        container_name=container_name,\n",
    "        overwrite=True,\n",
    "    )\n",
    "    os.unlink(file_name)\n",
    "    logging.info(f\"{file_name} successfully pushed to {adls_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will generate a test for this at a later time as LTBP doesn't need this type of massaging\n",
    "\n",
    "\n",
    "\n",
    "The y_var inside of LTBP isn't a great use case, but RVF where we might want to scale the y_var in regression or time series using MinMaxScaler or StandardScaler this will give the flexibility needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| skip\n",
    "\n",
    "\n",
    "def y_var_scaler(y_train, y_valid, y_test, y_var, scaler_type):\n",
    "    \"\"\"Write Doc String\"\"\"\n",
    "    y_train = scaler_type.fit_transform(y_train.reset_index()[[y_var]])\n",
    "    y_train = pd.DataFrame(y_train)\n",
    "    y_train.columns = [y_var]\n",
    "    y_valid = scaler_type.transform(y_valid.reset_index()[[y_var]])\n",
    "    y_valid = pd.DataFrame(y_valid)\n",
    "    y_valid.columns = [y_var]\n",
    "    y_test = scaler_type.transform(y_test.reset_index()[[y_var]])\n",
    "    y_test = pd.DataFrame(y_test)\n",
    "    y_test.columns = [y_var]\n",
    "    return y_train, y_valid, y_test, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def create_stage_and_query_stage_sf(\n",
    "    sf,  # Snowflake connection\n",
    "    etl: dict,  # template etl input expected format\n",
    "    udf_inputs: dict,  # template udf input expected format\n",
    "    train_or_inference: str,  # training or inference\n",
    "    experiment_name: str,  # name of experiment being ran\n",
    "    indentification: list = ['ECID'],  # list of identification defaults to ECID\n",
    "    experiment: bool = True,  # Boolean fed to function from script to say if its an experiment\n",
    "    extra_statement: str = None,  # defaults to None to allow for experimentation\n",
    "):\n",
    "    features, udf_inputs, etl = get_yaml_dicts(['features.yaml', 'udf_inputs.yaml', 'etl.yaml'])\n",
    "    stage_url = f\"\"\"azure://{etl['azure_account']}.blob.core.windows.net/{etl['azure_container']}/{etl['data_lake_path']}{(os.path.join('experiments', experiment_name)\n",
    "        if experiment else os.path.join('LocalRunTest'))}\"\"\".replace(' ', '')\n",
    "    stage_query = make_stage_query_generator(\n",
    "        stage_name=etl[\"stage_name\"] + etl['FY_folder'] + os.environ.get('CI_COMMIT_SHA', 'LocalRunTest'),\n",
    "        url=stage_url,\n",
    "        sas_token=os.environ[\"DATALAKE_SAS_TOKEN_SECRET\"],\n",
    "        file_type=\"parquet\",\n",
    "    )\n",
    "    sf = snowflake_query()\n",
    "    _ = sf.run_sql_str(stage_query)\n",
    "    # TODO: Figure out a identification feature like season year\n",
    "    # Udf grain is ECID, which is easy to get, but season year isn't obivous some thought is needed\n",
    "    indentification = indentification if indentification is not None else [col.split('.')[-1] for col in udf_inputs[train_or_inference]['UDF_GRAIN']]\n",
    "    columns = [col.upper() for col in features.keys()]\n",
    "    query = generate_data_lake_query(stage_name=(etl[\"stage_name\"]\n",
    "                                                 + etl['FY_folder']\n",
    "                                                 + os.environ.get('CI_COMMIT_SHA', 'LocalRunTest')),\n",
    "                                     stage_path=train_or_inference.lower()+'_data/',\n",
    "                                     columns=indentification + columns,\n",
    "                                     extra_statement=extra_statement)\n",
    "    logging.info(f'adls snowflake stage query {query}')\n",
    "    sf = snowflake_query()\n",
    "    df = sf.run_sql_str(query)\n",
    "    logging.info(f'Preview dataframe queried {df.head()}')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip\n",
    "# stage_url = f\"\"\"azure://{etl_dict['azure_account']}.blob.core.windows.net/\n",
    "# {etl['azure_container']}/{etl_dict['data_lake_path']}{\n",
    "# (os.path.join('experiments', etl_dict['exp_name'])\n",
    "# if experiment \n",
    "# else os.path.join(os.environ.get('CI_COMMIT_SHA', 'LocalRunTest')))}\"\"\".replace('\\n', '')\n",
    "# stage_query = make_stage_query_generator(\n",
    "#     stage_name=etl[\"stage_name\"] + etl['FY_folder'] + os.environ.get('CI_COMMIT_SHA', 'LocalRunTest'),\n",
    "#     url=stage_url,\n",
    "#     sas_token=os.environ[\"DATALAKE_SAS_TOKEN_SECRET\"],\n",
    "#     file_type=\"parquet\",\n",
    "# )\n",
    "# _ = sf.run_sql_str(stage_query)\n",
    "# # TODO: Figure out a identification feature like season year \n",
    "# # Udf grain is ECID, which is easy to get, but season year isn't obivous some thought is needed\n",
    "# indentification = [col.split('.')[-1] for col in udf_inputs[train_or_inference]['UDF_GRAIN']]\n",
    "# columns = [col.upper() for col in features.keys()]\n",
    "# query = generate_data_lake_query(stage_name=(etl[\"stage_name\"] \n",
    "#                                              + etl['FY_folder'] \n",
    "#                                              + os.environ.get('CI_COMMIT_SHA', 'LocalRunTest')),\n",
    "#      stage_path=train_or_inference.lower()+'_data/',\n",
    "#      columns=indentification + columns,\n",
    "#      extra_statement=None)\n",
    "# logging.info(f'adls snowflake stage query {query}')\n",
    "# df = sf.run_sql_str(query)\n",
    "# logging.info(f'Preview dataframe queried {df.head()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
