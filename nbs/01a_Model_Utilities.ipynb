{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Utils\n",
    "\n",
    "> Functions Used In Modeling Efforts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp modeling.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeremydemlow/miniforge3/envs/ltbp/lib/python3.9/site-packages/snowflake/connector/options.py:96: UserWarning: You have an incompatible version of 'pyarrow' installed (6.0.0), please install a version that adheres to: 'pyarrow<8.1.0,>=8.0.0; extra == \"pandas\"'\n",
      "  warn_incompatible_dep(\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "from data_system_utilities.azure.storage import FileHandling\n",
    "from data_system_utilities.snowflake.utils import make_stage_query_generator\n",
    "\n",
    "from machine_learning_utilities import preprocessing\n",
    "\n",
    "from LTBP.data.utils import snowflake_query, generate_data_lake_query\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from rfpimp import *  # noqa:\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def return_dict_type(\n",
    "    pre_process_type: dict  # {k:v} dictionary of columns name and tranformation type\n",
    "):\n",
    "    \"\"\"\n",
    "    Simplify the standard process for sklearn preprocessing pipelines\n",
    "    \"\"\"\n",
    "    for k, v in pre_process_type.items():\n",
    "        if v == \"OrdinalEncoder\":\n",
    "            pre_process_dict = {\n",
    "                f\"{k}\": {\n",
    "                    \"transformation\": {\n",
    "                        \"name\": \"OrdinalEncoder\",\n",
    "                        \"args\": {\n",
    "                            \"handle_unknown\": \"use_encoded_value\",\n",
    "                            \"unknown_value\": -1,\n",
    "                        },\n",
    "                    },\n",
    "                    \"variable_type\": \"cat\",\n",
    "                }\n",
    "            }\n",
    "        if v == \"OneHotEncoder\":\n",
    "            pre_process_dict = {\n",
    "                f\"{k}\": {\n",
    "                    \"transformation\": {\n",
    "                        \"name\": \"OneHotEncoder\",\n",
    "                        \"args\": {\"handle_unknown\": \"ignore\", \"sparse\": False},\n",
    "                    },\n",
    "                    \"variable_type\": \"cat\",\n",
    "                }\n",
    "            }\n",
    "        if v == \"StandardScaler\":\n",
    "            pre_process_dict = {\n",
    "                f\"{k}\": {\n",
    "                    \"transformation\": {\"name\": \"StandardScaler\", \"args\": {}},\n",
    "                    \"variable_type\": \"cont\",\n",
    "                }\n",
    "            }\n",
    "        if v == \"RobustScaler\":\n",
    "            pre_process_dict = {\n",
    "                f\"{k}\": {\n",
    "                    \"transformation\": {\"name\": \"RobustScaler\", \"args\": {}},\n",
    "                    \"variable_type\": \"cont\",\n",
    "                }\n",
    "            }\n",
    "    return pre_process_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'destinationgeoafinitylabel': {'transformation': {'name': 'OrdinalEncoder',\n",
       "   'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}},\n",
       "  'variable_type': 'cat'}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data_system_utilities.file_parsers import yaml\n",
    "\n",
    "features=yaml.yaml_reader('./LTBP/files/yaml_files/features.yaml')\n",
    "experiment_name='BASELINE'\n",
    "cat_vars =[{f.lower() : values['transformation'][experiment_name]} for f, values in features.items() \n",
    "            if values['var_type'][experiment_name] == 'cat'\n",
    "            and values['input_definition'] != 'LABEL']\n",
    "\n",
    "return_dict_type(cat_vars[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq(return_dict_type(cat_vars[0]).keys(), cat_vars[0].keys())\n",
    "test_eq(return_dict_type(cat_vars[0])[list(cat_vars[0].keys())[0]].keys(), ['transformation', 'variable_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def create_sklearn_preprocess_baseline_dict(\n",
    "    cat_vars: list,  # list of categorical variables with sklearn transformer\n",
    "    cont_vars: list,  # list of continous variables with sklearn transformer\n",
    "):\n",
    "    \"\"\"wrapper around ``return_dict_type`` to go through cat and cont vars\n",
    "    \"\"\"\n",
    "    final_dict = {}\n",
    "    if cat_vars is None:\n",
    "        cat_vars = []\n",
    "    if cont_vars is None:\n",
    "        cont_vars = []\n",
    "    for item in cat_vars + cont_vars:\n",
    "        final_dict.update(return_dict_type(item))\n",
    "    return final_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:{'destinationgeoafinitylabel': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'evercorepass': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'everpass': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'gendercode': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'guestbehavior': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'isepicmixactivated': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'marketingzone': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'mostcommonticketcomp': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'mostsubseasonvisited': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'mostvisitedregion': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'mostvisitedresort': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'onlysingleresortkey': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'partnerresortscannerflag': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'skierabilitylabel': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'totalseasonsscanned': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'visitmostinpeak': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'age': {'transformation': {'name': 'StandardScaler', 'args': {}}, 'variable_type': 'cont'}, 'avgvisitperseason': {'transformation': {'name': 'StandardScaler', 'args': {}}, 'variable_type': 'cont'}, 'resortsvisited': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'subseasonsperyear': {'transformation': {'name': 'StandardScaler', 'args': {}}, 'variable_type': 'cont'}, 'totalvisits': {'transformation': {'name': 'StandardScaler', 'args': {}}, 'variable_type': 'cont'}}\n"
     ]
    }
   ],
   "source": [
    "features=yaml.yaml_reader('./LTBP/files/yaml_files/features.yaml')\n",
    "experiment_name='BASELINE'\n",
    "\n",
    "cat_vars =[{f.lower() : values['transformation'][experiment_name]} for f, values in features.items() \n",
    "            if values['var_type'][experiment_name] == 'cat'\n",
    "            and values['input_definition'] != 'LABEL']\n",
    "cont_vars =[{f.lower(): values['transformation'][experiment_name]} for f, values in features.items() \n",
    "            if values['var_type'][experiment_name] == 'cont'\n",
    "            and values['input_definition'] != 'LABEL']\n",
    "\n",
    "feature_dict = create_sklearn_preprocess_baseline_dict(cat_vars=cat_vars, \n",
    "                                                       cont_vars=cont_vars)\n",
    "logging.info(feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq(feature_dict[list(cat_vars[0].keys())[0]].keys(), ['transformation', 'variable_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def return_list_of_vars(variables):\n",
    "    \"\"\"returns lists key\"\"\"\n",
    "    if variables is None:\n",
    "        return None\n",
    "    vars_list = []\n",
    "    for item in variables:\n",
    "        for k in item.keys():\n",
    "            vars_list.append(k)\n",
    "    return vars_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:categorical variables: \n",
      " ['destinationgeoafinitylabel', 'evercorepass', 'everpass', 'gendercode', 'guestbehavior', 'isepicmixactivated', 'marketingzone', 'mostcommonticketcomp', 'mostsubseasonvisited', 'mostvisitedregion', 'mostvisitedresort', 'onlysingleresortkey', 'partnerresortscannerflag', 'skierabilitylabel', 'totalseasonsscanned', 'visitmostinpeak']\n",
      "INFO:root:continous variables: \n",
      " ['age', 'avgvisitperseason', 'resortsvisited', 'subseasonsperyear', 'totalvisits']\n"
     ]
    }
   ],
   "source": [
    "cat_vars = return_list_of_vars(cat_vars)\n",
    "cont_vars = return_list_of_vars(cont_vars)\n",
    "logging.info(f'categorical variables: \\n {cat_vars}')\n",
    "logging.info(f'continous variables: \\n {cont_vars}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "feature_dict=yaml.yaml_reader('./LTBP/files/yaml_files/features.yaml')\n",
    "test_eq(cat_vars, [f.lower() for f, values in features.items() \n",
    "                    if values['var_type'][experiment_name] == 'cat'\n",
    "                    and values['input_definition'] != 'LABEL'])\n",
    "test_eq(cont_vars, [f.lower() for f, values in features.items() \n",
    "                    if values['var_type'][experiment_name] == 'cont'\n",
    "                    and values['input_definition'] != 'LABEL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def prepare_training_set(df: pd.DataFrame,\n",
    "                         y_var: list,\n",
    "                         y_scaler_type: object,\n",
    "                         sklearn_pipe: object,\n",
    "                         etl_dict: dict,\n",
    "                         models_dict: dict,\n",
    "                         adls_path: str,\n",
    "                         experiment_name: str,\n",
    "                         connection_str: str,\n",
    "                         identifiers: list = ['ECID'],\n",
    "                         test_set: bool = True,\n",
    "                         validation_split: float = .20,\n",
    "                         test_split: float = .15,\n",
    "                         seed: int = 1320,\n",
    "                         as_type=int):\n",
    "    \"\"\"TODO: Working on Multi-Col Labels split and preprocess data set for model training purposes\"\"\"\n",
    "    scaler = y_scaler_type\n",
    "    # Sklearn basic split method\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(df,\n",
    "                                                          df[y_var].astype(as_type),\n",
    "                                                          test_size=validation_split,\n",
    "                                                          random_state=seed)\n",
    "    if test_set is True:\n",
    "        X_valid, X_test, y_valid, y_test = train_test_split(X_valid,\n",
    "                                                            y_valid,\n",
    "                                                            test_size=test_split,\n",
    "                                                            random_state=seed)\n",
    "        logging.info(f'Successfully Spilt Data\\nTrain: {X_train.shape}, {y_train.shape}\\nValid: {X_valid.shape}, {y_valid.shape}\\nTest: {X_test.shape}, {y_test.shape}')\n",
    "    else:\n",
    "        y_test = None\n",
    "        X_test = None\n",
    "        logging.info(f'Successfully Spilt Data\\nTrain: {X_train.shape}, {y_train.shape}\\nValid: {X_valid.shape}, {y_valid.shape}')\n",
    "    id_list = X_test[identifiers] if test_set is True else X_valid[identifiers]\n",
    "    logging.info(f'Size of the id_list for the hold set {id_list.shape}')\n",
    "    if scaler:\n",
    "        y_train = scaler.fit_transform(y_train.reset_index()[y_var[0]])\n",
    "        y_train = pd.DataFrame(y_train)\n",
    "        y_train.columns = [y_var]\n",
    "        y_valid = scaler.transform(y_valid.reset_index()[y_var])\n",
    "        y_valid = pd.DataFrame(y_valid)\n",
    "        y_valid.columns = [y_var]\n",
    "        if test_set is True:\n",
    "            y_test = scaler.transform(y_test.reset_index()[y_var])\n",
    "            y_test = pd.DataFrame(y_test)\n",
    "            y_test.columns = [y_var]\n",
    "    else:\n",
    "        logging.info('This project relies on the query to have accurate labels with no preprocessing..')\n",
    "        y_train = y_train.reset_index()[y_var]\n",
    "        y_train = pd.DataFrame(y_train)\n",
    "        y_train.columns = [y_var]\n",
    "        y_valid = y_valid.reset_index()[y_var]\n",
    "        y_valid = pd.DataFrame(y_valid)\n",
    "        y_valid.columns = [y_var]\n",
    "        if test_set is True:\n",
    "            y_test = y_test.reset_index()[y_var]\n",
    "            y_test = pd.DataFrame(y_test)\n",
    "            y_test.columns = [y_var]\n",
    "\n",
    "    if scaler:\n",
    "        logging.info('saving y_var scaler to adls')\n",
    "        save_sklearn_object_to_data_lake(save_object=scaler,\n",
    "                                         file_name=(os.environ.get('CI_COMMIT_SHA', 'LocalRunNBS')\n",
    "                                                    + models_dict[experiment_name]['y_preprocess_object_name']),\n",
    "                                         adls_path=adls_path,\n",
    "                                         container_name=etl_dict['azure_container'],\n",
    "                                         connection_str=connection_str)\n",
    "\n",
    "    X_train = sklearn_pipe.fit_transform(X_train)\n",
    "    cols = preprocessing.get_column_names_from_transformer(sklearn_pipe)\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    X_train.columns = cols\n",
    "\n",
    "    X_valid = sklearn_pipe.transform(X_valid)\n",
    "    cols = preprocessing.get_column_names_from_transformer(sklearn_pipe)\n",
    "    X_valid = pd.DataFrame(X_valid)\n",
    "    X_valid.columns = cols\n",
    "    if test_set is True:\n",
    "        X_test = sklearn_pipe.transform(X_test)\n",
    "        cols = preprocessing.get_column_names_from_transformer(sklearn_pipe)\n",
    "        X_test = pd.DataFrame(X_test)\n",
    "        X_test.columns = cols\n",
    "\n",
    "    save_sklearn_object_to_data_lake(save_object=sklearn_pipe,\n",
    "                                     file_name=(os.environ.get('CI_COMMIT_SHA', 'LocalRunNBS')\n",
    "                                                + models_dict[experiment_name]['x_preprocess_object_name']),\n",
    "                                     adls_path=adls_path,\n",
    "                                     container_name=etl_dict['azure_container'],\n",
    "                                     connection_str=connection_str)\n",
    "    return X_train, X_valid, X_test, y_train, y_valid, y_test, sklearn_pipe, scaler, id_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler_type = None\n",
    "# test_set=True\n",
    "# experiment_name='BASELINE'\n",
    "# sklearn_pipe=pipe\n",
    "# etl=etl\n",
    "# models=models\n",
    "\n",
    "# adls_path = os.path.join((os.path.join(etl['data_lake_path'], 'experiments', experiment_name)\n",
    "#     if experiment \n",
    "#     else os.path.join(etl_dict['data_lake_path'], \n",
    "#     os.environ.get('CI_COMMIT_SHA', 'LocalRunNBS')))\n",
    "#     , models['preprocessors_adls_path'])\n",
    "# result = prepare_training_set(df,\n",
    "#                               y_var=[k.upper() for k, v in features.items() if v['input_definition'] == 'LABEL'],\n",
    "#                               y_scaler_type=models[experiment_name]['y_scaler_type'],\n",
    "#                               adls_path=adls_path,\n",
    "#                               sklearn_pipe=pipe,\n",
    "#                               test_set=True,\n",
    "#                               etl_dict=etl,\n",
    "#                               models_dict=models,\n",
    "#                               connection_str=os.environ[models[\"connection_str\"]],\n",
    "#                               identifiers=['ECID']\n",
    "#                               )\n",
    "# X_train, X_valid, X_test, y_train, y_valid, y_test, sklearn_pipe, scaler, id_list = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one needs a test, but as of right now holding off on this test as this test would take a little big more time than I want to spend on the documentation of this process at the current moment\n",
    "\n",
    "The idea of this function is to ensure that the user is using the pre-processor in the correct fashion so that the validation set is not being considered in the pre-processing dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def preprocess_data(X_train, X_valid, X_test, sklearn_pipe):\n",
    "    X_train = sklearn_pipe.fit_transform(X_train)\n",
    "    cols = preprocessing.get_column_names_from_transformer(sklearn_pipe)\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    X_train.columns = cols\n",
    "\n",
    "    X_valid = sklearn_pipe.transform(X_valid)\n",
    "    cols = preprocessing.get_column_names_from_transformer(sklearn_pipe)\n",
    "    X_valid = pd.DataFrame(X_valid)\n",
    "    X_valid.columns = cols\n",
    "\n",
    "    X_test = sklearn_pipe.transform(X_test)\n",
    "    cols = preprocessing.get_column_names_from_transformer(sklearn_pipe)\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "    X_test.columns = cols\n",
    "\n",
    "    return X_train, X_valid, X_test, sklearn_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save_sklearn_object_to_data_lake\n",
    "\n",
    "This function is simply wrapping DSU functionality together to allow for a model to be pushed to adls these tests are written and evaluated inside of DSU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def save_sklearn_object_to_data_lake(\n",
    "    save_object, file_name, adls_path, container_name, connection_str\n",
    "):\n",
    "    \"\"\"moves a sklearn object to azure data lake as a pickle file at a given path\"\"\"\n",
    "    logging.info(\n",
    "        f\"Pushing Sklearn Object to Azure: {os.path.join(adls_path, file_name)}\"\n",
    "    )\n",
    "    with open(file_name, \"wb\") as f:\n",
    "        pickle.dump(save_object, f)\n",
    "    az = FileHandling(connection_str)\n",
    "    az.upload_file(\n",
    "        azure_file_path=adls_path,\n",
    "        local_file_path=file_name,\n",
    "        container_name=container_name,\n",
    "        overwrite=True,\n",
    "    )\n",
    "    os.unlink(file_name)\n",
    "    logging.info(f\"{file_name} successfully pushed to {adls_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will generate a test for this at a later time as LTBP doesn't need this type of massaging\n",
    "\n",
    "\n",
    "\n",
    "The y_var inside of LTBP isn't a great use case, but RVF where we might want to scale the y_var in regression or time series using MinMaxScaler or StandardScaler this will give the flexibility needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| skip\n",
    "\n",
    "\n",
    "def y_var_scaler(y_train, y_valid, y_test, y_var, scaler_type):\n",
    "    \"\"\"Write Doc String\"\"\"\n",
    "    y_train = scaler_type.fit_transform(y_train.reset_index()[[y_var]])\n",
    "    y_train = pd.DataFrame(y_train)\n",
    "    y_train.columns = [y_var]\n",
    "    y_valid = scaler_type.transform(y_valid.reset_index()[[y_var]])\n",
    "    y_valid = pd.DataFrame(y_valid)\n",
    "    y_valid.columns = [y_var]\n",
    "    y_test = scaler_type.transform(y_test.reset_index()[[y_var]])\n",
    "    y_test = pd.DataFrame(y_test)\n",
    "    y_test.columns = [y_var]\n",
    "    return y_train, y_valid, y_test, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def create_stage_and_query_stage_sf(\n",
    "    sf,  # Snowflake connection\n",
    "    features: dict,  # template feature input expected format\n",
    "    etl: dict,  # template etl input expected format\n",
    "    udf_inputs: dict,  # template udf input expected format\n",
    "    train_or_inference: str,  # training or inference\n",
    "    experiment_name: str,  # name of experiment being ran\n",
    "    indentification: list = ['ECID'],  # list of identification defaults to ECID\n",
    "    experiment: bool = True,  # Boolean fed to function from script to say if its an experiment\n",
    "    extra_statement: str = None,  # defaults to None to allow for experimentation\n",
    "):\n",
    "    stage_url = f\"\"\"azure://{etl['azure_account']}.blob.core.windows.net/{\n",
    "        etl['azure_container']}/{etl['data_lake_path']}{\n",
    "        (os.path.join('experiments', experiment_name)\n",
    "        if experiment else os.path.join(os.getenv('CI_COMMIT_SHA', 'LocalRunTest'), experiment_name))}\"\"\".replace(' ', '')+'/'\n",
    "    stage_query = make_stage_query_generator(\n",
    "        stage_name=etl[\"stage_name\"] + etl['FY_folder'] + os.environ.get('CI_COMMIT_SHA', 'LocalRunTest'),\n",
    "        url=stage_url,\n",
    "        sas_token=os.environ[\"DATALAKE_SAS_TOKEN_SECRET\"],\n",
    "        file_type=\"parquet\",\n",
    "    )\n",
    "    sf = snowflake_query()\n",
    "    _ = sf.run_sql_str(stage_query)\n",
    "    # TODO: Figure out a identification feature like season year\n",
    "    # Udf grain is ECID, which is easy to get, but season year isn't obivous some thought is needed\n",
    "    indentification = indentification if indentification is not None else [col.split('.')[-1] for col in udf_inputs[train_or_inference]['UDF_GRAIN']]\n",
    "    columns = [col.upper() for col in features.keys()]\n",
    "    query = generate_data_lake_query(stage_name=(etl[\"stage_name\"]\n",
    "                                                 + etl['FY_folder']\n",
    "                                                 + os.environ.get('CI_COMMIT_SHA', 'LocalRunTest')),\n",
    "                                     stage_path=train_or_inference.lower()+'_data/',\n",
    "                                     columns=indentification + columns,\n",
    "                                     extra_statement=extra_statement)\n",
    "    logging.info(f'adls snowflake stage query {query}')\n",
    "    df = sf.run_sql_str(query)\n",
    "    logging.info(f'Preview dataframe queried {df.head()}')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| skip\n",
    "from LTBP.data.utils import get_yaml_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:data_system_utilities.snowflake.utils:stage_query: \n",
      " create or replace stage ltbpFY23LocalRunTest\n",
      "url='azure://vaildtscadls.blob.core.windows.net/vailadls/projects/LTBP/FY23/LocalRunTest/BASELINE/'\n",
      "credentials=(azure_sas_token='**MASKED**')\n",
      "encryption=(type= 'NONE')\n",
      "file_format = (type = parquet        )\n",
      "INFO:data_system_utilities.snowflake.utils:connection to snowflake established...\n",
      "INFO:data_system_utilities.snowflake.query:executing query\n",
      "INFO:data_system_utilities.snowflake.query:data loaded from snowflake\n",
      "INFO:data_system_utilities.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:data_system_utilities.snowflake.query:Stage area LTBPFY23LOCALRUNTEST successfully created.\n",
      "INFO:root:adls snowflake stage query \n",
      "    select\n",
      "    $1:\"ECID\"::varchar as ECID\n",
      ", $1:\"AGE\"::varchar as AGE\n",
      ", $1:\"AVGVISITPERSEASON\"::varchar as AVGVISITPERSEASON\n",
      ", $1:\"BOUGHTPASS\"::varchar as BOUGHTPASS\n",
      ", $1:\"DESTINATIONGEOAFINITYLABEL\"::varchar as DESTINATIONGEOAFINITYLABEL\n",
      ", $1:\"EVERCOREPASS\"::varchar as EVERCOREPASS\n",
      ", $1:\"EVERPASS\"::varchar as EVERPASS\n",
      ", $1:\"GENDERCODE\"::varchar as GENDERCODE\n",
      ", $1:\"GUESTBEHAVIOR\"::varchar as GUESTBEHAVIOR\n",
      ", $1:\"ISEPICMIXACTIVATED\"::varchar as ISEPICMIXACTIVATED\n",
      ", $1:\"MARKETINGZONE\"::varchar as MARKETINGZONE\n",
      ", $1:\"MOSTCOMMONTICKETCOMP\"::varchar as MOSTCOMMONTICKETCOMP\n",
      ", $1:\"MOSTSUBSEASONVISITED\"::varchar as MOSTSUBSEASONVISITED\n",
      ", $1:\"MOSTVISITEDREGION\"::varchar as MOSTVISITEDREGION\n",
      ", $1:\"MOSTVISITEDRESORT\"::varchar as MOSTVISITEDRESORT\n",
      ", $1:\"ONLYSINGLERESORTKEY\"::varchar as ONLYSINGLERESORTKEY\n",
      ", $1:\"PARTNERRESORTSCANNERFLAG\"::varchar as PARTNERRESORTSCANNERFLAG\n",
      ", $1:\"RESORTSVISITED\"::varchar as RESORTSVISITED\n",
      ", $1:\"SKIERABILITYLABEL\"::varchar as SKIERABILITYLABEL\n",
      ", $1:\"SUBSEASONSPERYEAR\"::varchar as SUBSEASONSPERYEAR\n",
      ", $1:\"TOTALSEASONSSCANNED\"::varchar as TOTALSEASONSSCANNED\n",
      ", $1:\"TOTALVISITS\"::varchar as TOTALVISITS\n",
      ", $1:\"VISITMOSTINPEAK\"::varchar as VISITMOSTINPEAK\n",
      "\n",
      "    from @ltbpFY23LocalRunTest/inference_data/\n",
      "    None\n",
      "    \n",
      "INFO:data_system_utilities.snowflake.utils:connection to snowflake established...\n",
      "INFO:data_system_utilities.snowflake.query:executing query\n",
      "INFO:data_system_utilities.snowflake.query:connection to snowflake has been turned off\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [51], line 32\u001b[0m\n\u001b[1;32m     25\u001b[0m query \u001b[38;5;241m=\u001b[39m generate_data_lake_query(stage_name\u001b[38;5;241m=\u001b[39m(etl[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     26\u001b[0m                                              \u001b[38;5;241m+\u001b[39m etl[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFY_folder\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     27\u001b[0m                                              \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCI_COMMIT_SHA\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLocalRunTest\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[1;32m     28\u001b[0m                                  stage_path\u001b[38;5;241m=\u001b[39mtrain_or_inference\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_data/\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     29\u001b[0m                                  columns\u001b[38;5;241m=\u001b[39mindentification \u001b[38;5;241m+\u001b[39m columns,\n\u001b[1;32m     30\u001b[0m                                  extra_statement\u001b[38;5;241m=\u001b[39mextra_statement)\n\u001b[1;32m     31\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madls snowflake stage query \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43msf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_sql_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPreview dataframe queried \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mhead()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/ltbp/lib/python3.9/site-packages/data_system_utilities/snowflake/query.py:73\u001b[0m, in \u001b[0;36mSnowflake.run_sql_str\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m     71\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexecuting query\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     72\u001b[0m     query \u001b[38;5;241m=\u001b[39m query \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(query)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m query \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 73\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sql_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata loaded from snowflake\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/ltbp/lib/python3.9/site-packages/pandas/io/sql.py:397\u001b[0m, in \u001b[0;36mread_sql_query\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize, dtype)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;124;03mRead SQL query into a DataFrame.\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;124;03mparameter will be converted to UTC.\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    396\u001b[0m pandas_sql \u001b[38;5;241m=\u001b[39m pandasSQL_builder(con)\n\u001b[0;32m--> 397\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/ltbp/lib/python3.9/site-packages/pandas/io/sql.py:2092\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[0;34m(self, sql, index_col, coerce_float, params, parse_dates, chunksize, dtype)\u001b[0m\n\u001b[1;32m   2082\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_query_iterator(\n\u001b[1;32m   2083\u001b[0m         cursor,\n\u001b[1;32m   2084\u001b[0m         chunksize,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2089\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   2090\u001b[0m     )\n\u001b[1;32m   2091\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2092\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetchall_as_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2093\u001b[0m     cursor\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m   2095\u001b[0m     frame \u001b[38;5;241m=\u001b[39m _wrap_result(\n\u001b[1;32m   2096\u001b[0m         data,\n\u001b[1;32m   2097\u001b[0m         columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2101\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   2102\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/ltbp/lib/python3.9/site-packages/pandas/io/sql.py:2106\u001b[0m, in \u001b[0;36mSQLiteDatabase._fetchall_as_list\u001b[0;34m(self, cur)\u001b[0m\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fetchall_as_list\u001b[39m(\u001b[38;5;28mself\u001b[39m, cur):\n\u001b[0;32m-> 2106\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mcur\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetchall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m   2108\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(result)\n",
      "File \u001b[0;32m~/miniforge3/envs/ltbp/lib/python3.9/site-packages/snowflake/connector/cursor.py:1150\u001b[0m, in \u001b[0;36mSnowflakeCursor.fetchall\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1148\u001b[0m ret \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1150\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetchone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m row \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1152\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ltbp/lib/python3.9/site-packages/snowflake/connector/cursor.py:1116\u001b[0m, in \u001b[0;36mSnowflakeCursor.fetchone\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result_state \u001b[38;5;241m=\u001b[39m ResultState\u001b[38;5;241m.\u001b[39mVALID\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_result_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ltbp/lib/python3.9/site-packages/snowflake/connector/cursor.py:1093\u001b[0m, in \u001b[0;36mSnowflakeCursor._result_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;124;03m\"\"\"Yields the elements from _result and raises an exception when appropriate.\"\"\"\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1093\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _next \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result:\n\u001b[1;32m   1094\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(_next, \u001b[38;5;167;01mException\u001b[39;00m):\n\u001b[1;32m   1095\u001b[0m             Error\u001b[38;5;241m.\u001b[39merrorhandler_wrapper_from_ready_exception(\n\u001b[1;32m   1096\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection,\n\u001b[1;32m   1097\u001b[0m                 \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1098\u001b[0m                 _next,\n\u001b[1;32m   1099\u001b[0m             )\n",
      "File \u001b[0;32m~/miniforge3/envs/ltbp/lib/python3.9/site-packages/snowflake/connector/result_set.py:85\u001b[0m, in \u001b[0;36mresult_set_iterator\u001b[0;34m(first_batch_iter, unconsumed_batches, unfetched_batches, final, prefetch_thread_num, **kw)\u001b[0m\n\u001b[1;32m     82\u001b[0m future \u001b[38;5;241m=\u001b[39m unconsumed_batches\u001b[38;5;241m.\u001b[39mpopleft()\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# this will raise an exception if one has occurred\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m batch_iterator \u001b[38;5;241m=\u001b[39m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser began consuming result batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m batch_iterator\n",
      "File \u001b[0;32m~/miniforge3/envs/ltbp/lib/python3.9/concurrent/futures/_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/miniforge3/envs/ltbp/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#| skip\n",
    "yaml_file_list = ['features.yaml', 'udf_inputs.yaml', 'etl.yaml', 'models.yaml']\n",
    "experiment = False\n",
    "experiment_name = 'BASELINE'\n",
    "indentification = ['ECID']\n",
    "train_or_inference = 'INFERENCE'\n",
    "extra_statement=None\n",
    "\n",
    "stage_url = f\"\"\"azure://{etl['azure_account']}.blob.core.windows.net/{\n",
    "        etl['azure_container']}/{etl['data_lake_path']}{\n",
    "        (os.path.join('experiments', experiment_name)\n",
    "        if experiment else os.path.join(os.getenv('CI_COMMIT_SHA', 'LocalRunTest'), experiment_name))}\"\"\".replace(' ', '')+'/'\n",
    "stage_query = make_stage_query_generator(\n",
    "        stage_name=etl[\"stage_name\"] + etl['FY_folder'] + os.environ.get('CI_COMMIT_SHA', 'LocalRunTest'),\n",
    "        url=stage_url,\n",
    "        sas_token=os.environ[\"DATALAKE_SAS_TOKEN_SECRET\"],\n",
    "        file_type=\"parquet\",\n",
    "    )\n",
    "sf = snowflake_query()\n",
    "_ = sf.run_sql_str(stage_query)\n",
    "# TODO: Figure out a identification feature like season year\n",
    "# Udf grain is ECID, which is easy to get, but season year isn't obivous some thought is needed\n",
    "indentification = indentification if indentification is not None else [col.split('.')[-1] for col in udf_inputs[train_or_inference]['UDF_GRAIN']]\n",
    "columns = [col.upper() for col in features.keys()]\n",
    "query = generate_data_lake_query(stage_name=(etl[\"stage_name\"]\n",
    "                                             + etl['FY_folder']\n",
    "                                             + os.environ.get('CI_COMMIT_SHA', 'LocalRunTest')),\n",
    "                                 stage_path=train_or_inference.lower()+'_data/',\n",
    "                                 columns=indentification + columns,\n",
    "                                 extra_statement=extra_statement)\n",
    "logging.info(f'adls snowflake stage query {query}')\n",
    "df = sf.run_sql_str(query)\n",
    "logging.info(f'Preview dataframe queried {df.head()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
