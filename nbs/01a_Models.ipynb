{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "> Models Available For This Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp modeling.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *\n",
    "from data_system_utilities.file_parsers import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from hyperopt import hp\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from rfpimp import * # noqa:\n",
    "\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import logging\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger(\"azure.core\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"urllib3.connectionpool\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"snowflake.connector\").setLevel(logging.WARNING)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the DSDE standard process for using Xboost with hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def train_xgb(X_train, X_valid, X_test, y_train, y_valid, y_test,\n",
    "              evals=20, sub=200000, train=1000000, early_stop=10, verbose=False):\n",
    "    \"\"\"\n",
    "    Binary Classifiation Xgboost DSDE hyper opt approach this should be\n",
    "    reviewed and customized for your use case.\n",
    "    \"\"\"\n",
    "    parameter_space = {\n",
    "        'max_depth': hp.choice('max_depth', np.arange(21, dtype=int) + 3),\n",
    "        'n_estimators': hp.choice('n_estimators', np.arange(301, dtype=int) + 100),\n",
    "        'gamma': hp.uniform('gamma', 0, 5),\n",
    "        'learning_rate': hp.loguniform('learning_rate', np.log(0.005), np.log(0.2)),\n",
    "        'colsample_bytree': hp.quniform('colsample_bytree', 0.1, 1, 0.01),\n",
    "        'subsample': hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "        'base_score': hp.quniform('base_score', 0.3, 0.8, 0.01),\n",
    "        'objective': hp.choice('objective', ['binary:logistic']),\n",
    "        'eval_metric': hp.choice('eval_metric', ['logloss', 'error', 'auc', 'aucpr', 'map']),\n",
    "        'use_label_encoder': hp.choice('use_label_encoder', [False]),\n",
    "        'gpu_id': hp.choice('gpu_id', [-1]),\n",
    "    }\n",
    "    logger.info(f'Hyper tuning on {X_train[0:sub].shape[0]} rows')\n",
    "    opt = HpOptBinary(X_train[0:sub], X_test[0:sub], y_train[0:sub], y_test[0:sub], parameter_space=parameter_space)\n",
    "    best = opt.optimize(max_evals=evals)\n",
    "    logger.info(f'Full training on {X_train[0:train].shape[0]} rows')\n",
    "    model = xgb.XGBClassifier(**best, n_jobs=-1)\n",
    "    eval_set = [(X_valid, y_valid)]\n",
    "    model.fit(X_train[0:train], y_train[0:train], eval_set=eval_set, early_stopping_rounds=early_stop, verbose=verbose)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def train_xgb_basic(X_train, X_valid, y_train, y_valid, early_stop=10, verbose=True):\n",
    "    \"\"\"\n",
    "    Binary Classifiation Xgboost Sklearn API Call Basic HyperParameters\n",
    "    \"\"\"\n",
    "    logger.info(f'Training on {X_train.shape[0]} rows')\n",
    "    model = xgb.XGBClassifier(n_jobs=-1)\n",
    "    eval_set = [(X_valid, y_valid)]\n",
    "    model.fit(X_train, y_train, eval_set=eval_set, early_stopping_rounds=early_stop, verbose=verbose)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def train_logistic(X_train, X_test, y_train, y_test, model=LogisticRegression, evals=20, sub=200000, train=1000000):\n",
    "    \"\"\"Logistic Regression Example to show how simple this can be to switch the model being used in this template\"\"\"\n",
    "    parameter_space = {\n",
    "        'class_weight': hp.choice('class_weight', [None, 'balanced']),\n",
    "        'warm_start': hp.choice('warm_start', [True, False]),\n",
    "        'fit_intercept': hp.choice('fit_intercept', [True, False]),\n",
    "        'tol': hp.uniform('tol', 0.00001, 0.0001),\n",
    "        'C': hp.uniform('C', 0.05, 3),\n",
    "        'solver': hp.choice('solver', ['lbfgs']),  # , 'liblinear', 'sag', 'saga']),\n",
    "        'max_iter': hp.choice('max_iter', range(5, 1000))\n",
    "    }\n",
    "    logger.info(f'Hyper tuning on {X_test[0:sub].shape[0]} rows')\n",
    "    opt = HpOptBinary(X_train[0:sub], X_test[0:sub], y_train[0:sub],\n",
    "                      y_test[0:sub], model=model, parameter_space=parameter_space)\n",
    "    best = opt.optimize(max_evals=evals)\n",
    "    logger.info(f'Full training on {X_test[0:train].shape[0]} rows')\n",
    "    model = LogisticRegression(**best, n_jobs=-1)\n",
    "    model.fit(X_train[0:train], y_train[0:train])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'?sv=2019-12-12&ss=bfqt&srt=sco&sp=rwdlacupx&se=2031-01-22T06:17:14Z&st=2021-01-21T22:17:14Z&spr=https&sig=kIHogByJjyVWyL6XupA0CBUB1iw12%2FeXWFQiOj5fB5c%3D'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"DATALAKE_SAS_TOKEN_SECRET\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
