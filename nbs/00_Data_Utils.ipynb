{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f57f3d1",
   "metadata": {},
   "source": [
    "# Library Utils\n",
    "\n",
    "> Functions Used In Modeling Efforts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb9cdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c51cb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02798351",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from data_system_utilities.snowflake.query import Snowflake\n",
    "from data_system_utilities.snowflake.copyinto import sf_to_adls_url_query_generator\n",
    "from data_system_utilities.file_parsers import yaml\n",
    "\n",
    "from fastcore.xtras import is_listy \n",
    "from LTBP import files\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfe75b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def get_yaml_dicts(yaml_file_names: list):\n",
    "    \"\"\"\n",
    "    Give a list of files to this function from the /files\n",
    "    in a library and it will read/Parse yaml files and\n",
    "    return a list of dictionaries to unpack\n",
    "\n",
    "    How to use:\n",
    "    yaml_file_list = ['dataset.yaml', 'etl.yaml', 'experiment.yaml']\n",
    "    data, etl, exp = get_yaml_dicts(yaml_file_list)\n",
    "\n",
    "    Args:\n",
    "    * yaml_file_names (list):\n",
    "\n",
    "    Returns:\n",
    "    *  list : list of dictionaries\n",
    "    \"\"\"\n",
    "    yaml_file_names = yaml_file_names if is_listy(yaml_file_names) else list(yaml_file_names)\n",
    "    yaml_dicts = []\n",
    "    for yf in yaml_file_names:\n",
    "        yaml_dict = yaml.yaml_reader(os.path.join(files.__path__[0], 'yaml_files', yf))\n",
    "        yaml_dicts.append(yaml_dict)\n",
    "    return yaml_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f084430",
   "metadata": {},
   "outputs": [],
   "source": [
    "features, inputs = get_yaml_dicts(['features.yaml', 'udf_inputs.yaml'])\n",
    "test_eq(inputs.keys(), ['INFERENCE', 'TRAINING'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3911736",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def generate_data_lake_query(\n",
    "    stage_name, stage_path, columns, header=True, extra_statement=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Given the columns names are provided this query will query out parquet data\n",
    "    from azure datalake all in varchar this is the basic approach.\n",
    "\n",
    "    Args:\n",
    "        stage_name (str): Snowflake stage name\n",
    "        stage_path (str): Snowflake stage path\n",
    "        columns (list): list/dict of column names\n",
    "        extra_statement (str, optional): Extra snowflake command. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        str: Query produced through this function\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "    select\n",
    "    FEATURES_HERE\n",
    "    from @{os.path.join(stage_name, stage_path)}\n",
    "    {extra_statement}\n",
    "    \"\"\"\n",
    "    if header is True:\n",
    "        for ind, feature in enumerate(columns):\n",
    "            if ind == 0:\n",
    "                query = query.replace(\n",
    "                    \"FEATURES_HERE\",\n",
    "                    f'$1:\"{feature}\"::varchar as {feature}\\nFEATURES_HERE',\n",
    "                )\n",
    "            else:\n",
    "                query = query.replace(\n",
    "                    \"FEATURES_HERE\",\n",
    "                    f', $1:\"{feature}\"::varchar as {feature}\\nFEATURES_HERE',\n",
    "                )\n",
    "    else:\n",
    "        for ind, feature in enumerate(columns):\n",
    "            if ind == 0:\n",
    "                query = query.replace(\n",
    "                    \"FEATURES_HERE\",\n",
    "                    f'$1:\"_COL_{ind}\"::varchar as {feature}\\nFEATURES_HERE',\n",
    "                )\n",
    "            else:\n",
    "                query = query.replace(\n",
    "                    \"FEATURES_HERE\",\n",
    "                    f', $1:\"_COL_{ind}\"::varchar as {feature}\\nFEATURES_HERE',\n",
    "                )\n",
    "    query = query.replace(\"FEATURES_HERE\", \"\")\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e890e0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    select\n",
      "    $1:\"destinationgeoafinitylabel\"::varchar as destinationgeoafinitylabel\n",
      ", $1:\"evercorepass\"::varchar as evercorepass\n",
      ", $1:\"everpass\"::varchar as everpass\n",
      ", $1:\"gendercode\"::varchar as gendercode\n",
      ", $1:\"guestbehavior\"::varchar as guestbehavior\n",
      ", $1:\"isepicmixactivated\"::varchar as isepicmixactivated\n",
      ", $1:\"marketingzone\"::varchar as marketingzone\n",
      ", $1:\"mostcommonticketcomp\"::varchar as mostcommonticketcomp\n",
      ", $1:\"mostsubseasonvisited\"::varchar as mostsubseasonvisited\n",
      ", $1:\"mostvisitedregion\"::varchar as mostvisitedregion\n",
      ", $1:\"mostvisitedresort\"::varchar as mostvisitedresort\n",
      ", $1:\"onlysingleresortkey\"::varchar as onlysingleresortkey\n",
      ", $1:\"partnerresortscannerflag\"::varchar as partnerresortscannerflag\n",
      ", $1:\"skierabilitylabel\"::varchar as skierabilitylabel\n",
      ", $1:\"totalseasonsscanned\"::varchar as totalseasonsscanned\n",
      ", $1:\"visitmostinpeak\"::varchar as visitmostinpeak\n",
      ", $1:\"age\"::varchar as age\n",
      ", $1:\"avgvisitperseason\"::varchar as avgvisitperseason\n",
      ", $1:\"resortsvisited\"::varchar as resortsvisited\n",
      ", $1:\"subseasonsperyear\"::varchar as subseasonsperyear\n",
      ", $1:\"totalvisits\"::varchar as totalvisits\n",
      "\n",
      "    from @LTBPLocalRunTest/projects/LTBP/FY23/experiments/ltbp_nbs_testing/training_data/\n",
      "    None\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "columns = ['destinationgeoafinitylabel','evercorepass','everpass','gendercode','guestbehavior',\n",
    "           'isepicmixactivated','marketingzone','mostcommonticketcomp','mostsubseasonvisited',\n",
    "           'mostvisitedregion','mostvisitedresort','onlysingleresortkey','partnerresortscannerflag',\n",
    "           'skierabilitylabel','totalseasonsscanned','visitmostinpeak','age','avgvisitperseason',\n",
    "           'resortsvisited','subseasonsperyear','totalvisits']\n",
    "\n",
    "query = generate_data_lake_query(stage_name='LTBP'+os.environ.get('CI_COMMIT_SHA', 'LocalNBS'),\n",
    "     stage_path='projects/LTBP/FY23/experiments/<EXPERIMENT_NAME>/training_data/'.replace('<EXPERIMENT_NAME>', 'ltbp_nbs_testing'),\n",
    "     columns=columns,\n",
    "     extra_statement=None)\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d21120",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq(query.replace(' ', '').replace('\\n', ''), \"\"\"select    $1:\"destinationgeoafinitylabel\"::varchar as destinationgeoafinitylabel, $1:\"evercorepass\"::varchar as evercorepass, $1:\"everpass\"::varchar as everpass, $1:\"gendercode\"::varchar as gendercode, $1:\"guestbehavior\"::varchar as guestbehavior, $1:\"isepicmixactivated\"::varchar as isepicmixactivated, $1:\"marketingzone\"::varchar as marketingzone, $1:\"mostcommonticketcomp\"::varchar as mostcommonticketcomp, $1:\"mostsubseasonvisited\"::varchar as mostsubseasonvisited, $1:\"mostvisitedregion\"::varchar as mostvisitedregion, $1:\"mostvisitedresort\"::varchar as mostvisitedresort, $1:\"onlysingleresortkey\"::varchar as onlysingleresortkey, $1:\"partnerresortscannerflag\"::varchar as partnerresortscannerflag, $1:\"skierabilitylabel\"::varchar as skierabilitylabel, $1:\"totalseasonsscanned\"::varchar as totalseasonsscanned, $1:\"visitmostinpeak\"::varchar as visitmostinpeak, $1:\"age\"::varchar as age, $1:\"avgvisitperseason\"::varchar as avgvisitperseason, $1:\"resortsvisited\"::varchar as resortsvisited, $1:\"subseasonsperyear\"::varchar as subseasonsperyear, $1:\"totalvisits\"::varchar as totalvisits    from @LTBPLocalRunTest/projects/LTBP/FY23/experiments/ltbp_nbs_testing/training_data/None\"\"\".replace(' ', '').replace('\\n', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3723c33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def read_sfQueries_txt_sql_file(file_name):\n",
    "    \"\"\"Simple utilty to read query files\"\"\"\n",
    "    with open(os.path.join(files.__path__[0], 'sql_files', file_name)) as f:\n",
    "        read_data = ''.join(f.readlines())\n",
    "        f.close()\n",
    "    return read_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5709bd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def return_sf_type(dtype: str, varchar: bool):\n",
    "    \"\"\"\n",
    "    simple function to convert dytpes to snowflake dtypes this\n",
    "    will be come a very useful thing to have as this will dtype\n",
    "    Args:\n",
    "    * dtype (str): dtype from a df in sting form\n",
    "    * varchar (bool): to default all variables to VARCHAR\n",
    "    this happens due to bad vendor data and can't be resloved\n",
    "    with out reading in the whole data set with low_memory=False\n",
    "\n",
    "    Returns:\n",
    "    * str: snowflake dtype\n",
    "    \"\"\"\n",
    "    if varchar is True:\n",
    "        dtype = 'VARCHAR'\n",
    "    elif 'int' in dtype.lower():\n",
    "        dtype = 'NUMBER'\n",
    "    elif 'float' in dtype.lower():\n",
    "        dtype = 'FLOAT'\n",
    "    elif 'object' in dtype.lower():\n",
    "        dtype = 'VARCHAR'\n",
    "    elif 'bool' in dtype.lower():\n",
    "        dtype = 'VARCHAR'  # TODO: Limitation found before change once resloved by sf\n",
    "    elif 'date' in dtype.lower():\n",
    "        dtype = 'DATETIME'  # TODO: Might break with certain datetimes most generic\n",
    "    else:\n",
    "        logging.error('odd dtype not seen needs to be resloved...')\n",
    "        sys.exit()\n",
    "    return dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e26747",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def snowflake_query(sfAccount: str = os.environ.get('sfAccount', None),\n",
    "                    sfUser: str = os.environ.get('sfUser', None),\n",
    "                    sfPswd: str = os.environ.get('sfPswd', None),\n",
    "                    sfWarehouse: str = os.environ.get('sfWarehouse', None),\n",
    "                    sfDatabase: str = os.environ.get('sfDatabase', None),\n",
    "                    sfSchema: str = os.environ.get('sfSchema', None),\n",
    "                    sfRole: str = os.environ.get('sfRole', None)):\n",
    "    \"\"\"Easy Connection To SnowFlake When Environs are set\"\"\"\n",
    "    sf = Snowflake(sfAccount, sfUser, sfPswd, sfWarehouse,\n",
    "                       sfDatabase, sfSchema, sfRole)\n",
    "    return sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13bfac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def query_feature_sets_to_adls_parquet_sf_fs(\n",
    "    sf_connection,\n",
    "    sf_query:str,\n",
    "    azure_account: str,\n",
    "    azure_container: str,\n",
    "    data_lake_path: str,\n",
    "    query_file_path: str,\n",
    "    data_lake_sas_token: str = os.environ[\"DATALAKE_SAS_TOKEN_SECRET\"],\n",
    "    partition_by: str = None,\n",
    "    max_file_size: str = \"3200000\",\n",
    "    header: str = \"True\",\n",
    "    over_write: str = \"True\",\n",
    "):\n",
    "    # Creating Query to create ADLS Stage for Snowflake\n",
    "    url = f\"azure://{azure_account}.blob.core.windows.net/{azure_container}/{data_lake_path}\"\n",
    "    sf_to_adls_query = sf_to_adls_url_query_generator(\n",
    "        azure_path=url,\n",
    "        azure_sas_token=data_lake_sas_token,\n",
    "        sf_query=sf_query,\n",
    "        max_file_size=max_file_size,\n",
    "        file_type=\"parquet\",\n",
    "        partition_by=partition_by,\n",
    "        header=header,\n",
    "        overwrite=over_write,\n",
    "    )\n",
    "    # Execute\n",
    "    _ = sf_connection.run_sql_str(sf_to_adls_query)\n",
    "    logging.info(f\"data has been delivered from sf to adls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1d4c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
