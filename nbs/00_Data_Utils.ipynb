{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d5388e",
   "metadata": {},
   "source": [
    "# LTBP Utils Functions\n",
    "\n",
    "> Functions Used In Modeling Efforts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed0f891",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec5c48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16bf504",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from data_system_utilities.snowflake.query import Snowflake\n",
    "from data_system_utilities.snowflake.copyinto import sf_to_adls_url_query_generator\n",
    "from data_system_utilities.file_parsers import yaml\n",
    "from machine_learning_utilities.dataset_creation.snowflake import select_static_features, select_additional_features, create_base_query\n",
    "\n",
    "from fastcore.xtras import is_listy\n",
    "\n",
    "import LTBP.files as files\n",
    "import os\n",
    "import logging\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680ba141",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def get_yaml_dicts(\n",
    "    yaml_file_names: list  # list of yaml file(s) names that are all in the yaml_files section of the repo\n",
    "):\n",
    "    \"\"\"\n",
    "    Give a list of files to this function from the /files\n",
    "    in a library and it will read/Parse yaml files and\n",
    "    return a list of dictionaries to unpack\n",
    "    \"\"\"\n",
    "    yaml_file_names = yaml_file_names if is_listy(yaml_file_names) else list(yaml_file_names)\n",
    "    yaml_dicts = []\n",
    "    for yf in yaml_file_names:\n",
    "        yaml_dict = yaml.yaml_reader(os.path.join(files.__path__[0], 'yaml_files', yf))\n",
    "        yaml_dicts.append(yaml_dict)\n",
    "    return yaml_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d4e06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features, inputs = get_yaml_dicts(['features.yaml', 'udf_inputs.yaml'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4198add4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq(inputs.keys(), ['INFERENCE', 'TRAINING'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779c4aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def generate_data_lake_query(\n",
    "    stage_name: str,  # name of the sf stage to be created is just a file location pointer\n",
    "    stage_path: str,  # adls file path location to start the stage location\n",
    "    columns: list,  # columns to query parquet files in adls\n",
    "    header=True,  # parquet files have headers\n",
    "    extra_statement: str = None  # adding sql statements if desired\n",
    "):\n",
    "    \"\"\"\n",
    "    Given the columns names are provided this query will query out parquet data\n",
    "    from azure datalake all in varchar this is the basic approach.\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "    select\n",
    "    FEATURES_HERE\n",
    "    from @{os.path.join(stage_name, stage_path)}\n",
    "    {extra_statement}\n",
    "    \"\"\"\n",
    "    if header is True:\n",
    "        for ind, feature in enumerate(columns):\n",
    "            if ind == 0:\n",
    "                query = query.replace(\n",
    "                    \"FEATURES_HERE\",\n",
    "                    f'$1:\"{feature}\"::varchar as {feature}\\nFEATURES_HERE',\n",
    "                )\n",
    "            else:\n",
    "                query = query.replace(\n",
    "                    \"FEATURES_HERE\",\n",
    "                    f', $1:\"{feature}\"::varchar as {feature}\\nFEATURES_HERE',\n",
    "                )\n",
    "    else:\n",
    "        for ind, feature in enumerate(columns):\n",
    "            if ind == 0:\n",
    "                query = query.replace(\n",
    "                    \"FEATURES_HERE\",\n",
    "                    f'$1:\"_COL_{ind}\"::varchar as {feature}\\nFEATURES_HERE',\n",
    "                )\n",
    "            else:\n",
    "                query = query.replace(\n",
    "                    \"FEATURES_HERE\",\n",
    "                    f', $1:\"_COL_{ind}\"::varchar as {feature}\\nFEATURES_HERE',\n",
    "                )\n",
    "    query = query.replace(\"FEATURES_HERE\", \"\")\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfd17f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    select\n",
      "    $1:\"destinationgeoafinitylabel\"::varchar as destinationgeoafinitylabel\n",
      ", $1:\"evercorepass\"::varchar as evercorepass\n",
      ", $1:\"everpass\"::varchar as everpass\n",
      ", $1:\"gendercode\"::varchar as gendercode\n",
      ", $1:\"guestbehavior\"::varchar as guestbehavior\n",
      ", $1:\"isepicmixactivated\"::varchar as isepicmixactivated\n",
      ", $1:\"marketingzone\"::varchar as marketingzone\n",
      ", $1:\"mostcommonticketcomp\"::varchar as mostcommonticketcomp\n",
      ", $1:\"mostsubseasonvisited\"::varchar as mostsubseasonvisited\n",
      ", $1:\"mostvisitedregion\"::varchar as mostvisitedregion\n",
      ", $1:\"mostvisitedresort\"::varchar as mostvisitedresort\n",
      ", $1:\"onlysingleresortkey\"::varchar as onlysingleresortkey\n",
      ", $1:\"partnerresortscannerflag\"::varchar as partnerresortscannerflag\n",
      ", $1:\"skierabilitylabel\"::varchar as skierabilitylabel\n",
      ", $1:\"totalseasonsscanned\"::varchar as totalseasonsscanned\n",
      ", $1:\"visitmostinpeak\"::varchar as visitmostinpeak\n",
      ", $1:\"age\"::varchar as age\n",
      ", $1:\"avgvisitperseason\"::varchar as avgvisitperseason\n",
      ", $1:\"resortsvisited\"::varchar as resortsvisited\n",
      ", $1:\"subseasonsperyear\"::varchar as subseasonsperyear\n",
      ", $1:\"totalvisits\"::varchar as totalvisits\n",
      "\n",
      "    from @LTBPLocalRunTest/projects/LTBP/FY23/experiments/ltbp_nbs_testing/training_data/\n",
      "    None\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "columns = ['destinationgeoafinitylabel','evercorepass','everpass','gendercode','guestbehavior',\n",
    "           'isepicmixactivated','marketingzone','mostcommonticketcomp','mostsubseasonvisited',\n",
    "           'mostvisitedregion','mostvisitedresort','onlysingleresortkey','partnerresortscannerflag',\n",
    "           'skierabilitylabel','totalseasonsscanned','visitmostinpeak','age','avgvisitperseason',\n",
    "           'resortsvisited','subseasonsperyear','totalvisits']\n",
    "\n",
    "query = generate_data_lake_query(stage_name='LTBPLocalRunTest',\n",
    "     stage_path='projects/LTBP/FY23/experiments/<EXPERIMENT_NAME>/training_data/'.replace('<EXPERIMENT_NAME>', 'ltbp_nbs_testing'),\n",
    "     columns=columns,\n",
    "     extra_statement=None)\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e84dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq(query.replace(' ', '').replace('\\n', ''), \"\"\"select    $1:\"destinationgeoafinitylabel\"::varchar as destinationgeoafinitylabel, $1:\"evercorepass\"::varchar as evercorepass, $1:\"everpass\"::varchar as everpass, $1:\"gendercode\"::varchar as gendercode, $1:\"guestbehavior\"::varchar as guestbehavior, $1:\"isepicmixactivated\"::varchar as isepicmixactivated, $1:\"marketingzone\"::varchar as marketingzone, $1:\"mostcommonticketcomp\"::varchar as mostcommonticketcomp, $1:\"mostsubseasonvisited\"::varchar as mostsubseasonvisited, $1:\"mostvisitedregion\"::varchar as mostvisitedregion, $1:\"mostvisitedresort\"::varchar as mostvisitedresort, $1:\"onlysingleresortkey\"::varchar as onlysingleresortkey, $1:\"partnerresortscannerflag\"::varchar as partnerresortscannerflag, $1:\"skierabilitylabel\"::varchar as skierabilitylabel, $1:\"totalseasonsscanned\"::varchar as totalseasonsscanned, $1:\"visitmostinpeak\"::varchar as visitmostinpeak, $1:\"age\"::varchar as age, $1:\"avgvisitperseason\"::varchar as avgvisitperseason, $1:\"resortsvisited\"::varchar as resortsvisited, $1:\"subseasonsperyear\"::varchar as subseasonsperyear, $1:\"totalvisits\"::varchar as totalvisits    from @LTBPLocalRunTest/projects/LTBP/FY23/experiments/ltbp_nbs_testing/training_data/None\"\"\".replace(' ', '').replace('\\n', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b87d385",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def read_sfQueries_txt_sql_file(\n",
    "    file_name: str  # sql file name to read\n",
    "):\n",
    "    \"\"\"Simple utilty to read query files\"\"\"\n",
    "    with open(os.path.join(files.__path__[0], 'sql_files', file_name)) as f:\n",
    "        read_data = ''.join(f.readlines())\n",
    "        f.close()\n",
    "    return read_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85689528",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def return_sf_type(\n",
    "    dtype: str,  # data type from a df in string form\n",
    "    varchar: bool = True  # to default all variables to VARCHAR\n",
    "):\n",
    "    \"\"\"\n",
    "    simple function to convert dytpes to snowflake dtypes this\n",
    "    will be come a very useful thing to have as this will dtype\n",
    "    \"\"\"\n",
    "    if varchar is True:\n",
    "        dtype = 'VARCHAR'\n",
    "    elif 'int' in dtype.lower():\n",
    "        dtype = 'NUMBER'\n",
    "    elif 'float' in dtype.lower():\n",
    "        dtype = 'FLOAT'\n",
    "    elif 'object' in dtype.lower():\n",
    "        dtype = 'VARCHAR'\n",
    "    elif 'bool' in dtype.lower():\n",
    "        dtype = 'VARCHAR'  # TODO: Limitation found before change once resloved by sf\n",
    "    elif 'date' in dtype.lower():\n",
    "        dtype = 'DATETIME'  # TODO: Might break with certain datetimes most generic\n",
    "    else:\n",
    "        logging.error('odd dtype not seen needs to be resloved...')\n",
    "        sys.exit()\n",
    "    return dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681eb276",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def snowflake_query(sfAccount: str = os.environ.get('sfAccount', None),\n",
    "                    sfUser: str = os.environ.get('sfUser', None),\n",
    "                    sfPswd: str = os.environ.get('sfPswd', None),\n",
    "                    sfWarehouse: str = os.environ.get('sfWarehouse', None),\n",
    "                    sfDatabase: str = os.environ.get('sfDatabase', None),\n",
    "                    sfSchema: str = os.environ.get('sfSchema', None),\n",
    "                    sfRole: str = os.environ.get('sfRole', None)):\n",
    "    \"\"\"Easy Connection To SnowFlake When Environs are set\"\"\"\n",
    "    sf = Snowflake(sfAccount, sfUser, sfPswd, sfWarehouse,\n",
    "                   sfDatabase, sfSchema, sfRole)\n",
    "    return sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0e9a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def query_feature_sets_to_adls_parquet_sf_fs(\n",
    "    sf_connection,  # established snowflake connection\n",
    "    sf_query: str,  # sql query desired to be pushed to adls\n",
    "    azure_account: str,  # azure acount name\n",
    "    azure_container: str,  # azure container name to push results to\n",
    "    data_lake_path: str,  # adls file path location to dump files to\n",
    "    data_lake_sas_token: str,  # most project use os.environ[\"DATALAKE_SAS_TOKEN_SECRET\"]\n",
    "    partition_by: str = None,  # save data in a paritions manner\n",
    "    max_file_size: str = \"3200000\",  # choose the file size you would like to save your parquet files\n",
    "    header: str = \"True\",  # saving with headers or not needs to be a string\n",
    "    over_write: str = \"True\",  # if file(s) exisit this will earse exisiting files\n",
    "):\n",
    "    # Creating Query to create ADLS Stage for Snowflake\n",
    "    url = f\"azure://{azure_account}.blob.core.windows.net/{azure_container}/{data_lake_path}\"\n",
    "    sf_to_adls_query = sf_to_adls_url_query_generator(\n",
    "        azure_path=url,\n",
    "        azure_sas_token=data_lake_sas_token,\n",
    "        sf_query=sf_query,\n",
    "        max_file_size=max_file_size,\n",
    "        file_type=\"parquet\",\n",
    "        partition_by=partition_by,\n",
    "        header=header,\n",
    "        overwrite=over_write,\n",
    "    )\n",
    "    # Execute\n",
    "    _ = sf_connection.run_sql_str(sf_to_adls_query)\n",
    "    logging.info(f\"data has been delivered from sf to adls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd40294",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def clean_special_chars(text):\n",
    "    \"\"\"\n",
    "    small nlp clean up tool to take odd characters that could be\n",
    "    in vendor data inside of column names and then replaces empty\n",
    "    spaces with ``_``\n",
    "\n",
    "    Args:\n",
    "    * text (str): dataframe column names as strings\n",
    "\n",
    "    Returns:\n",
    "    * str: clean column name\n",
    "    \"\"\"\n",
    "    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'  # noqa:\n",
    "    punct += '©^®` <→°€™› ♥←×§″′Â█½à…“★”–●â►−¢²¬░¶↑±¿▾═¦║―¥▓—‹─▒：¼⊕▼▪†■’▀¨▄♫☆é¯♦¤▲è¸¾Ã⋅‘∞∙）↓、│（»，♪╩╚³・╦╣╔╗▬❤ïØ¹≤‡√'  # noqa:\n",
    "    for p in punct:\n",
    "        text = text.replace(p, '')\n",
    "        text = text.replace(' ', '')\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe1f02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def select_multi_input_udfs(feature_dict: dict,\n",
    "                            udf_inputs: dict,\n",
    "                            query: str,\n",
    "                            sf_database: str,\n",
    "                            sf_schema: str,\n",
    "                            iteration: int,\n",
    "                            exp_name: str):\n",
    "    \"\"\"\n",
    "    utility function called by `pull_features_from_snowflake`\n",
    "    to create multi input snowflake udf calls UDF_NAME(Input 1, Input 2)\n",
    "    a common example is at the ecid grain ISEPICMIXACTIVATED(ECID, 20201001, 20211001)\n",
    "\n",
    "    Args:\n",
    "        feature_dict (dict): multi key dictionary of features with\n",
    "            information the function will end up using to create a dynamic call\n",
    "        udf_inputs (dict): all information needed for the udf inputs to be created.\n",
    "        query (str): query string being manipulated\n",
    "        sf_database (str): snowflake database the udfs live in\n",
    "        sf_schema (str): snowflake schema the udfs live in\n",
    "        iteration (int): iteration give the function the ability to know what inputs\n",
    "            are going to be passed to the udf\n",
    "\n",
    "    Returns:\n",
    "        str: manipulated query string\n",
    "    \"\"\"\n",
    "    i = 1\n",
    "    udf_grain = ', '.join([str(v) for v in udf_inputs['UDF_GRAIN']])\n",
    "    udf_location = f'{sf_database}.{sf_schema}'\n",
    "    for temp_feat in feature_dict.keys():\n",
    "        i += 1\n",
    "        feat_dict = feature_dict[temp_feat]\n",
    "        input_def = udf_inputs[feat_dict['input_definition']][exp_name][feat_dict['input_type']][iteration]\n",
    "        input_def = [input_def] if not isinstance(input_def, list) else input_def\n",
    "        inputs = ', '.join([str(v) for v in input_def])\n",
    "        inputs = ', ' + inputs if len(inputs) > 0 else inputs\n",
    "        result = ''\n",
    "        if 'iterable_inputs' in feat_dict.keys():\n",
    "            for iter_idx, iter_input in enumerate(feat_dict['iterable_inputs']):\n",
    "                if iter_idx + 1 < len(feat_dict['iterable_inputs']):\n",
    "                    result += f\", {udf_location}.{feat_dict['udf_name']}({udf_grain}{inputs}, {iter_input}) as {temp_feat}_{clean_special_chars(iter_input)}\\n\"\n",
    "                else:\n",
    "                    result += f\", {udf_location}.{feat_dict['udf_name']}({udf_grain}{inputs}, {iter_input}) as {temp_feat}_{clean_special_chars(iter_input)}\\n\"\n",
    "        else:\n",
    "            if i > len(feature_dict.keys()):\n",
    "                result = f\", {udf_location}.{feat_dict['udf_name']}({udf_grain}{inputs}) as {temp_feat}\\n\"\n",
    "            else:\n",
    "                result = f\", {udf_location}.{feat_dict['udf_name']}({udf_grain}{inputs}) as {temp_feat}\"\n",
    "        query = query.replace('<FEATURES>', result + '<FEATURES>')\n",
    "\n",
    "    query = query.replace('<FEATURES>', '')\n",
    "    return query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61da8262",
   "metadata": {},
   "source": [
    "These functions are small twist on MSU already created calls to incorporate experimentation names to the mix to allow for sequential or concurrent calls to python scripts to allow for a smoother and faster iteration of experiments to happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ab2964",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def pull_features_from_snowflake(\n",
    "    feature_dict: dict,\n",
    "    udf_inputs: dict,\n",
    "    filepath_to_grain_list_query: str = None,\n",
    "    sf_database: str = 'MACHINELEARNINGFEATURES',\n",
    "    sf_schema: str = 'PROD',\n",
    "    feature_table_join: str = None,\n",
    "    extra_statement: str = None,\n",
    "    experiment_name: str = 'BASELINE'\n",
    "):\n",
    "    \"\"\"\n",
    "    a function to allow a user dynamically create snowflake queries that\n",
    "    creates a feature set that will be transformed into inputs that are\n",
    "    suitable for ML like task. this function starts with a pre-determined list\n",
    "    that will define a grain (most commonly used at the ECID grain) that will\n",
    "    dynamically call snowflake sql udf(s) and possibliy join a table that has\n",
    "    static like features that will create a dataset.\n",
    "\n",
    "    To have more understanding of this function and the useability please visit\n",
    "    (https://vailresorts.gitlab.io/data-science/machine_learning_utilities/)\n",
    "\n",
    "    Args:\n",
    "        feature_dict (dict): this dictionary will have information on all the features\n",
    "            a user is looking to gather in one feature set. Each feature will need a feature_type,\n",
    "            input_type, input_definition, and a udf_name.\n",
    "        udf_inputs (dict): this dictionary will have all the information needed for non \"static\"\n",
    "            like features that are being created via the joined table. This dictionary will need to have\n",
    "            a key UDF_GRAIN, FEATURE, LABEL, BASE_QUERY.\n",
    "        filepath_to_grain_list_query (str, optional): file location of sql files to read.\n",
    "        sf_database (str, optional): snowflake database UDFs live. Defaults to 'MACHINELEARNINGFEATURES'.\n",
    "        sf_schema (str, optional): snowflake schema UDFs live. Defaults to 'PROD'.\n",
    "        feature_table_join (str, optional): a table other than the feature store to join to this dynamic\n",
    "            call that will have all the static features needed to complete the feature set note the table alias\n",
    "            on the join statment must be joined. Defaults to None.\n",
    "        extra_statement (str, optional): allows for additional sql statements i.e LIMIT 1000. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        str: mainpulated string query ready to be sent to snowflake\n",
    "    \"\"\"\n",
    "    static_features = {k: v for k, v in feature_dict.items() if \"STATIC\" in v.values()\n",
    "                       if experiment_name in v['experiment_list']}\n",
    "    temporal_features = {k: v for k, v in feature_dict.items() if \"TEMP\" in v.values()\n",
    "                         if experiment_name in v['experiment_list']}\n",
    "    logging.info(f'static features in data set: \\n {list(static_features.keys())}')\n",
    "    logging.info(f'temporal features in data set: \\n {list(temporal_features.keys())}')\n",
    "    query_structure = \"\"\"select\n",
    "    <MODEL_GRAIN>\n",
    "    <FEATURES>\n",
    "    <ADDITIONAL_COLUMNS>\n",
    "    from\n",
    "        (base_query) base\n",
    "    \"\"\".replace(\" \", \"\")\n",
    "\n",
    "    # Bring In all Information From Base Query\n",
    "    base_query = query_structure.replace(\"<MODEL_GRAIN>\", \"base.*\")\n",
    "    if feature_table_join is None:\n",
    "        join_feat_table = \"inner join machinelearningfeatures.prod.featurestore_ecid joined on joined.ecid = base.ecid\"\n",
    "    else:\n",
    "        join_feat_table = feature_table_join\n",
    "    base_query = base_query + join_feat_table if len(static_features) > 0 else base_query\n",
    "\n",
    "    # functional call\n",
    "    if len(static_features) > 0:\n",
    "        base_query = select_static_features(feature_dict=static_features,\n",
    "                                            query=base_query)\n",
    "        logging.info(f\"Finished appending static features\")\n",
    "\n",
    "    final_query = ''\n",
    "    for i in range(len(udf_inputs['BASE_QUERY'][experiment_name].keys())):\n",
    "        # functional call\n",
    "        logging.info(f'reading {udf_inputs[\"BASE_QUERY\"][experiment_name][i]} for base query...')\n",
    "        feature_query = create_base_query(\n",
    "            file_path=os.path.join(filepath_to_grain_list_query, udf_inputs[\"BASE_QUERY\"][experiment_name][i]),\n",
    "            query=base_query)\n",
    "\n",
    "        # functional call\n",
    "        if len(temporal_features) > 0:\n",
    "            feature_query = select_multi_input_udfs(feature_dict=temporal_features,\n",
    "                                                    udf_inputs=udf_inputs,\n",
    "                                                    query=feature_query,\n",
    "                                                    sf_database=sf_database,\n",
    "                                                    sf_schema=sf_schema,\n",
    "                                                    iteration=i,\n",
    "                                                    exp_name=experiment_name)\n",
    "\n",
    "        # functional call\n",
    "        if 'ADDITIONAL_COLUMNS' in udf_inputs:\n",
    "            feature_query = select_additional_features(feature_dict=udf_inputs['ADDITIONAL_COLUMNS'],\n",
    "                                                       query=feature_query,\n",
    "                                                       iteration=i)\n",
    "        else:\n",
    "            feature_query = feature_query.replace('<ADDITIONAL_COLUMNS>', '')\n",
    "\n",
    "        if i + 1 < len(udf_inputs['BASE_QUERY'][experiment_name].keys()):\n",
    "            feature_query += \" \\nUNION ALL\\n\"\n",
    "        final_query += feature_query\n",
    "\n",
    "    if extra_statement is not None:\n",
    "        final_query += f'\\n{extra_statement}'\n",
    "\n",
    "    logging.info(f'final query output: \\n {final_query}')\n",
    "    return final_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e3207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
