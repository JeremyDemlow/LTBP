{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Utilities\n",
    "\n",
    "> Functions Used In Modeling Efforts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp modeling.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from data_system_utilities.azure.storage import FileHandling\n",
    "from machine_learning_utilities import preprocessing\n",
    "\n",
    "from LTBP.data.utils import snowflake_query, get_yaml_dicts\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from rfpimp import *\n",
    "\n",
    "import sklearn.preprocessing as y_transform\n",
    "import os\n",
    "import logging\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def return_dict_type(\n",
    "    pre_process_type:dict # {k:v} dictionary of columns name and tranformation type\n",
    "                    ):\n",
    "    \"\"\"\n",
    "    Simplify the standard process for sklearn preprocessing pipelines\n",
    "    \"\"\"\n",
    "    for k, v in pre_process_type.items():\n",
    "        if v == \"OrdinalEncoder\":\n",
    "            pre_process_dict = {\n",
    "                f\"{k}\": {\n",
    "                    \"transformation\": {\n",
    "                        \"name\": \"OrdinalEncoder\",\n",
    "                        \"args\": {\n",
    "                            \"handle_unknown\": \"use_encoded_value\",\n",
    "                            \"unknown_value\": -1,\n",
    "                        },\n",
    "                    },\n",
    "                    \"variable_type\": \"cat\",\n",
    "                }\n",
    "            }\n",
    "        if v == \"OneHotEncoder\":\n",
    "            pre_process_dict = {\n",
    "                f\"{k}\": {\n",
    "                    \"transformation\": {\n",
    "                        \"name\": \"OneHotEncoder\",\n",
    "                        \"args\": {\"handle_unknown\": \"ignore\", \"sparse\": False},\n",
    "                    },\n",
    "                    \"variable_type\": \"cat\",\n",
    "                }\n",
    "            }\n",
    "        if v == \"StandardScaler\":\n",
    "            pre_process_dict = {\n",
    "                f\"{k}\": {\n",
    "                    \"transformation\": {\"name\": \"StandardScaler\", \"args\": {}},\n",
    "                    \"variable_type\": \"cont\",\n",
    "                }\n",
    "            }\n",
    "        if v == \"RobustScaler\":\n",
    "            pre_process_dict = {\n",
    "                f\"{k}\": {\n",
    "                    \"transformation\": {\"name\": \"RobustScaler\", \"args\": {}},\n",
    "                    \"variable_type\": \"cont\",\n",
    "                }\n",
    "            }\n",
    "    return pre_process_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_system_utilities.file_parsers import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'destinationgeoafinitylabel': {'transformation': {'name': 'OrdinalEncoder',\n",
       "   'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}},\n",
       "  'variable_type': 'cat'}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_dict=yaml.yaml_reader('./LTBP/files/yaml_files/features.yaml')\n",
    "\n",
    "cat_vars =[{f.lower() : values['transformation']} for f, values in feature_dict.items() \n",
    "            if values['var_type'] == 'cat'\n",
    "            and values['input_definition'] != 'LABEL']\n",
    "\n",
    "return_dict_type(cat_vars[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq(return_dict_type(cat_vars[0]).keys(), cat_vars[0].keys())\n",
    "test_eq(return_dict_type(cat_vars[0])[list(cat_vars[0].keys())[0]].keys(), ['transformation', 'variable_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def create_sklearn_preprocess_baseline_dict(\n",
    "    cat_vars:list, # list of categorical variables with sklearn transformer\n",
    "    cont_vars:list, # list of continous variables with sklearn transformer\n",
    "):\n",
    "    \"\"\"wrapper around ``return_dict_type`` to go through cat and cont vars\n",
    "    \"\"\"\n",
    "    final_dict = {}\n",
    "    if cat_vars is None:\n",
    "        cat_vars = []\n",
    "    if cont_vars is None:\n",
    "        cont_vars = []\n",
    "    for item in cat_vars + cont_vars:\n",
    "        final_dict.update(return_dict_type(item))\n",
    "    return final_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:{'destinationgeoafinitylabel': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'evercorepass': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'everpass': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'gendercode': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'guestbehavior': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'isepicmixactivated': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'marketingzone': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'mostcommonticketcomp': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'mostsubseasonvisited': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'mostvisitedregion': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'mostvisitedresort': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'onlysingleresortkey': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'partnerresortscannerflag': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'skierabilitylabel': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'totalseasonsscanned': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'visitmostinpeak': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'age': {'transformation': {'name': 'StandardScaler', 'args': {}}, 'variable_type': 'cont'}, 'avgvisitperseason': {'transformation': {'name': 'StandardScaler', 'args': {}}, 'variable_type': 'cont'}, 'resortsvisited': {'transformation': {'name': 'OrdinalEncoder', 'args': {'handle_unknown': 'use_encoded_value', 'unknown_value': -1}}, 'variable_type': 'cat'}, 'subseasonsperyear': {'transformation': {'name': 'StandardScaler', 'args': {}}, 'variable_type': 'cont'}, 'totalvisits': {'transformation': {'name': 'StandardScaler', 'args': {}}, 'variable_type': 'cont'}}\n"
     ]
    }
   ],
   "source": [
    "feature_dict=yaml.yaml_reader('./LTBP/files/yaml_files/features.yaml')\n",
    "\n",
    "cat_vars =[{f.lower() : values['transformation']} for f, values in feature_dict.items() \n",
    "            if values['var_type'] == 'cat'\n",
    "            and values['input_definition'] != 'LABEL']\n",
    "cont_vars =[{f.lower(): values['transformation']} for f, values in feature_dict.items() \n",
    "            if values['var_type'] == 'cont'\n",
    "            and values['input_definition'] != 'LABEL']\n",
    "\n",
    "feature_dict = create_sklearn_preprocess_baseline_dict(cat_vars=cat_vars, \n",
    "                                                       cont_vars=cont_vars)\n",
    "logging.info(feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq(feature_dict[list(cat_vars[0].keys())[0]].keys(), ['transformation', 'variable_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def return_list_of_vars(variables):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        variables (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    if variables is None:\n",
    "        return None\n",
    "    vars_list = []\n",
    "    for item in variables:\n",
    "        for k in item.keys():\n",
    "            vars_list.append(k)\n",
    "    return vars_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:categorical variables: \n",
      " ['destinationgeoafinitylabel', 'evercorepass', 'everpass', 'gendercode', 'guestbehavior', 'isepicmixactivated', 'marketingzone', 'mostcommonticketcomp', 'mostsubseasonvisited', 'mostvisitedregion', 'mostvisitedresort', 'onlysingleresortkey', 'partnerresortscannerflag', 'skierabilitylabel', 'totalseasonsscanned', 'visitmostinpeak']\n",
      "INFO:root:continous variables: \n",
      " ['age', 'avgvisitperseason', 'resortsvisited', 'subseasonsperyear', 'totalvisits']\n"
     ]
    }
   ],
   "source": [
    "cat_vars = return_list_of_vars(cat_vars)\n",
    "cont_vars = return_list_of_vars(cont_vars)\n",
    "logging.info(f'categorical variables: \\n {cat_vars}')\n",
    "logging.info(f'continous variables: \\n {cont_vars}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "feature_dict=yaml.yaml_reader('./LTBP/files/yaml_files/features.yaml')\n",
    "test_eq(cat_vars, [f.lower() for f, values in feature_dict.items()  \n",
    "                   if values['var_type'] == 'cat' \n",
    "                   and values['input_definition'] != 'LABEL'])\n",
    "test_eq(cont_vars, [f.lower() for f, values in feature_dict.items() \n",
    "                   if values['var_type'] == 'cont' \n",
    "                   and values['input_definition'] != 'LABEL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def prepare_training_set(df,\n",
    "                         y_var,\n",
    "                         sklearn_pipe,\n",
    "                         model_dict: dict,\n",
    "                         connection_str: str,\n",
    "                         model_grain: str,\n",
    "                         scaler_type,\n",
    "                         test_set=True):\n",
    "    \"\"\"split and preprocess data set\"\"\"\n",
    "    if scaler_type == 'MinMaxScaler':\n",
    "        scaler = MinMaxScaler()\n",
    "    else:\n",
    "        # TODO: use sklearn get attr\n",
    "        # will always expect a sklearn like api\n",
    "        scaler = scaler_type\n",
    "\n",
    "    # Sklearn basic split method\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(df, df[y_var], test_size=.20, random_state=1320)\n",
    "    if test_set is True:\n",
    "        X_valid, X_test, y_valid, y_test = train_test_split(X_valid, y_valid, test_size=.15, random_state=1320)\n",
    "        logging.info(f'Successfully Spilt Data\\nTrain: {X_train.shape}, {y_train.shape}\\nValid: {X_valid.shape}, {y_valid.shape}\\nTest: {X_test.shape}, {y_test.shape}')\n",
    "    else:\n",
    "        y_test = None\n",
    "        X_test = None\n",
    "        logging.info(f'Successfully Spilt Data\\nTrain: {X_train.shape}, {y_train.shape}\\nValid: {X_valid.shape}, {y_valid.shape}')\n",
    "    y_train = scaler.fit_transform(y_train.reset_index()[[y_var]])\n",
    "    y_train = pd.DataFrame(y_train)\n",
    "    y_train.columns = [y_var]\n",
    "    y_valid = scaler.transform(y_valid.reset_index()[[y_var]])\n",
    "    y_valid = pd.DataFrame(y_valid)\n",
    "    y_valid.columns = [y_var]\n",
    "    if test_set is True:\n",
    "        y_test = scaler.transform(y_test.reset_index()[[y_var]])\n",
    "        y_test = pd.DataFrame(y_test)\n",
    "        y_test.columns = [y_var]\n",
    "    save_sklearn_object_to_data_lake(save_object=scaler,\n",
    "                                     file_name=model_dict['y_preprocess_object_name'],\n",
    "                                     adls_path=os.path.join(os.environ['CI_COMMIT_SHA'],\n",
    "                                                            model_dict['preprocessors_adls_path'], model_grain),\n",
    "                                     container_name=model_dict['azure_container'],\n",
    "                                     connection_str=connection_str)\n",
    "    X_train = sklearn_pipe.fit_transform(X_train)\n",
    "    cols = preprocessing.get_column_names_from_transformer(sklearn_pipe)\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    X_train.columns = cols\n",
    "\n",
    "    X_valid = sklearn_pipe.transform(X_valid)\n",
    "    cols = preprocessing.get_column_names_from_transformer(sklearn_pipe)\n",
    "    X_valid = pd.DataFrame(X_valid)\n",
    "    X_valid.columns = cols\n",
    "    if test_set is True:\n",
    "        X_test = sklearn_pipe.transform(X_test)\n",
    "        cols = preprocessing.get_column_names_from_transformer(sklearn_pipe)\n",
    "        X_test = pd.DataFrame(X_test)\n",
    "        X_test.columns = cols\n",
    "    save_sklearn_object_to_data_lake(save_object=sklearn_pipe,\n",
    "                                     file_name=model_dict['x_preprocess_object_name'],\n",
    "                                     adls_path=os.path.join(os.environ['CI_COMMIT_SHA'],\n",
    "                                                            model_dict['preprocessors_adls_path'], model_grain),\n",
    "                                     container_name=model_dict['azure_container'],\n",
    "                                     connection_str=connection_str)\n",
    "    return X_train, X_valid, X_test, y_train, y_valid, y_test, sklearn_pipe, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``preprocess_data`` \n",
    "\n",
    "This one needs a test, but as of right now holding off on this test as this test would take a little big more time than I want to spend on the documentation of this process at the current moment\n",
    "\n",
    "The idea of this function is to ensure that the user is using the pre-processor in the correct fashion so that the validation set is not being considered in the pre-processing dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def preprocess_data(X_train, X_valid, X_test, sklearn_pipe):\n",
    "    X_train = sklearn_pipe.fit_transform(X_train)\n",
    "    cols = preprocessing.get_column_names_from_transformer(sklearn_pipe)\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    X_train.columns = cols\n",
    "\n",
    "    X_valid = sklearn_pipe.transform(X_valid)\n",
    "    cols = preprocessing.get_column_names_from_transformer(sklearn_pipe)\n",
    "    X_valid = pd.DataFrame(X_valid)\n",
    "    X_valid.columns = cols\n",
    "\n",
    "    X_test = sklearn_pipe.transform(X_test)\n",
    "    cols = preprocessing.get_column_names_from_transformer(sklearn_pipe)\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "    X_test.columns = cols\n",
    "\n",
    "    return X_train, X_valid, X_test, sklearn_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``save_sklearn_object_to_data_lake``\n",
    "\n",
    "This function is simply wrapping DSU functionality together to allow for a model to be pushed to adls these tests are written and evaluated inside of DSU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def save_sklearn_object_to_data_lake(\n",
    "    save_object, file_name, adls_path, container_name, connection_str\n",
    "):\n",
    "    \"\"\"moves a sklearn object to azure data lake as a pickle file at a given path\"\"\"\n",
    "    logging.info(\n",
    "        f\"Pushing Sklearn Object to Azure: {os.path.join(adls_path, file_name)}\"\n",
    "    )\n",
    "    with open(file_name, \"wb\") as f:\n",
    "        pickle.dump(save_object, f)\n",
    "    az = FileHandling(connection_str)\n",
    "    az.upload_file(\n",
    "        azure_file_path=adls_path,\n",
    "        local_file_path=file_name,\n",
    "        container_name=container_name,\n",
    "        overwrite=True,\n",
    "    )\n",
    "    os.unlink(file_name)\n",
    "    logging.info(f\"{file_name} successfully pushed to {adls_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def evaluate(model, X_valid, y_valid, y_var, feature_importance=True):\n",
    "    \"\"\"\n",
    "    Utlity to give experiment table information about the model\n",
    "    this is fully customizable and can be changed to be regression\n",
    "    RMSE, R2, MSE for example and changing the columns this function\n",
    "    isn't a dynamic function it needs to be written for a specific use\n",
    "    case.\n",
    "\n",
    "    Args:\n",
    "    * model (classifer): sklearn model for this\n",
    "    * X_valid (np.array): Validation set Traing\n",
    "    * y_valid (np.array): Actuals for Validation\n",
    "    * y_var (str): variable name being predicted\n",
    "\n",
    "    Returns:\n",
    "    * dict: dependent on return statement\n",
    "    \"\"\"\n",
    "    y_pred_proba = model.predict_proba(X_valid)\n",
    "    y_pred = model.predict(X_valid)\n",
    "    auc = metrics.roc_auc_score(y_valid, y_pred_proba[:, 1])\n",
    "    acc = metrics.accuracy_score(y_valid, y_pred)\n",
    "    bacc = metrics.balanced_accuracy_score(y_valid, y_pred)\n",
    "    columns = ['auc', 'acc', 'bacc']\n",
    "    logging.info(f'{y_var} AUC: {auc:.3f}    Accuracy: {acc:.3f}    Balanced Accuracy: {bacc:.3f}')\n",
    "    if feature_importance is True:\n",
    "        feat_df = pd.read_csv('features.csv')\n",
    "        logging.info(f'features: /n {feat_df.columns}')\n",
    "        feat_df = dict(zip(feat_df.dropna()['featurenames'], feat_df.dropna()['dtypes']))\n",
    "        logging.info(f'reduced to independent variables: /n {feat_df}')\n",
    "        fi_permutation = importances(model, pd.DataFrame(X_valid, columns=list(feat_df.keys())), pd.DataFrame(y_valid)) # noqa:\n",
    "        fi_permutation = (fi_permutation\n",
    "                          .reset_index()\n",
    "                          .rename({'Feature': 'cols', 'Importance': 'imp'}, axis=1))\n",
    "        logging.info(f'Feature Importance df: \\n {fi_permutation}')\n",
    "        fi_permutation.to_csv('fi_permutation.csv', index=False)\n",
    "    return auc, acc, bacc, columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def evaluate_test(model, X_test, y_test, y_var, ecid_list, table_name,\n",
    "                  grain_name=None, grain_value=None, container_name='vailadls',\n",
    "                  blob_dest='./', connection_str=os.environ['DATALAKE_CONN_STR_SECRET'],\n",
    "                  file_output='./holdout.csv', sfSchema='DEV'):\n",
    "    \"\"\"\n",
    "    Utlity to give us the scores on a complete test hold out\n",
    "\n",
    "    Args:\n",
    "    * sf (sf_connection): snowflake connection\n",
    "    * model (classifer): sklearn model for this\n",
    "    * X_valid (np.array): Validation set Traing\n",
    "    * y_valid (np.array): Actuals for Validation\n",
    "    * y_var (str): variable name being predicted\n",
    "    * ecid_list (list): customer id list with season year\n",
    "    \"\"\"\n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "    y_pred = model.predict(X_test)\n",
    "    auc = metrics.roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    bacc = metrics.balanced_accuracy_score(y_test, y_pred)\n",
    "    logging.info(f'For Test Set Unseen Data {y_var} AUC: {auc:.3f} Accuracy: {acc:.3f} Balanced Accuracy: {bacc:.3f}')\n",
    "    logging.info(f'Size of Test Data set: {len(y_test)}')\n",
    "    test_df_results = pd.DataFrame(ecid_list)\n",
    "    if grain_value is not None:\n",
    "        test_df_results['SUBSEASON'] = grain_value\n",
    "    test_df_results['PROBABILITY'] = y_pred_proba[:, 1]\n",
    "    logging.info(f'Preview Test Push df \\n {test_df_results.head(2)}')\n",
    "    logging.info(f'Preview Test Size: \\n {test_df_results.shape[0]}')\n",
    "    sf = snowflake_query(sfSchema=sfSchema)\n",
    "    sf.infer_to_snowflake(test_df_results,\n",
    "                          table_name=table_name)\n",
    "    logging.info('saving test prediction file')\n",
    "    test_df_results.to_csv(file_output, index=False)\n",
    "    logging.info(f'sending prediction file to azure {container_name} to {blob_dest}')\n",
    "    # blob_pusher(file_path=[file_output],\n",
    "    #             container_name=container_name,\n",
    "    #             blob_dest=[blob_dest],\n",
    "    #             connection_str=connection_str,\n",
    "    #             overwrite=True)\n",
    "    # os.unlink(file_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def move_dev_holdout_table_to_prod_location(sf, exp):\n",
    "    logging.info('Replacing Prod HoldOut With Newest Promoted')\n",
    "    sf.run_str_query(f\"\"\"\n",
    "                      CREATE OR REPLACE TABLE MACHINELEARNINGOUTPUTS.ltbp.{exp['holdout_tb_name']} AS\n",
    "                      SELECT * FROM MACHINELEARNINGOUTPUTS.DEV.{exp['holdout_tb_name']};\n",
    "                      \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def holdout_set_to_prod(connection_str: str, yaml_files: list = ['etl.yaml', 'experiment.yaml'], sfschema: str = 'ltbp'):\n",
    "    sf = snowflake_query(sfSchema=sfschema)\n",
    "    etl, exp = get_yaml_dicts(yaml_files)\n",
    "    fh = FileHandling(connection_str)\n",
    "    if exp['holdout_tb_name'].upper() in sf.run_str_query(\"show tables;\").name.tolist():\n",
    "        logging.info(f\"Deleting current prod hold out {exp['holdout_tb_name'].upper()}\")\n",
    "        sf.run_str_query(f\"DROP TABLE MACHINELEARNINGOUTPUTS.{sfschema}.{exp['holdout_tb_name']};\")\n",
    "    logging.info(\"inference only pipeline pushing hold out data set to prodction table\")\n",
    "    # timestamp = query_production_timestamp(exp, etl)\n",
    "    path = [etl['data_lake']['stage_path'].replace('COMMITID', \"TODO:\") + '/holdout/']\n",
    "    ls_list = fh.ls_blob(container_name='vailadls',\n",
    "                         path=path[0],\n",
    "                         recursive=True)\n",
    "    for p in [path[0] + x for x in ls_list]:\n",
    "        logging.warning('if the model and data are not in alphabetical order this method will not work will need to customize')\n",
    "        logging.info(f'holdout data set {p[0]}')\n",
    "        # blob_puller(files=[p],\n",
    "        #             connection_str=connection_str,\n",
    "        #             container_name='vailadls',\n",
    "        #             drop_location='.',\n",
    "        #             overwrite=True\n",
    "        #             )\n",
    "        df = pd.read_csv(p.split('/')[-1])\n",
    "        logging.info(f'Preview Test \\n {df.head(2)}')\n",
    "        logging.info(f'Preview Test Size: \\n {df.shape[0]}')\n",
    "        sf.infer_to_snowflake(df,\n",
    "                              table_name=exp['holdout_tb_name'].upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will generate a test for this at a later time as LTBP doesn't need this type of massaging\n",
    "\n",
    "\n",
    "\n",
    "The y_var inside of LTBP isn't a great use case, but RVF where we might want to scale the y_var in regression or time series using MinMaxScaler or StandardScaler this will give the flexibility needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| skip\n",
    "\n",
    "\n",
    "def y_var_scaler(y_train, y_valid, y_test, y_var, scaler_type):\n",
    "    \"\"\"Write Doc String\"\"\"\n",
    "    y_train = scaler_type.fit_transform(y_train.reset_index()[[y_var]])\n",
    "    y_train = pd.DataFrame(y_train)\n",
    "    y_train.columns = [y_var]\n",
    "    y_valid = scaler_type.transform(y_valid.reset_index()[[y_var]])\n",
    "    y_valid = pd.DataFrame(y_valid)\n",
    "    y_valid.columns = [y_var]\n",
    "    y_test = scaler_type.transform(y_test.reset_index()[[y_var]])\n",
    "    y_test = pd.DataFrame(y_test)\n",
    "    y_test.columns = [y_var]\n",
    "    return y_train, y_valid, y_test, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
