{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "> Models Available For This Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp modeling.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *\n",
    "from data_system_utilities.file_parsers import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "from machine_learning_utilities.training.hypertuning import * # noqa:\n",
    "from sklearn import metrics\n",
    "from hyperopt import hp\n",
    "from hyperopt import fmin, tpe, STATUS_OK, Trials, space_eval\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from rfpimp import * # noqa:\n",
    "\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger(\"azure.core\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"urllib3.connectionpool\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"snowflake.connector\").setLevel(logging.WARNING)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't in MSU yet so I am adding it here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class HpOptBinary:\n",
    "    \"\"\"Class that hypertunes an arbitrary model for binary classification\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        parameter_space=None,\n",
    "        model=xgb.XGBClassifier,\n",
    "        loss_function=metrics.roc_auc_score,\n",
    "        loss_params={},\n",
    "        loss_pred_proba=True,\n",
    "    ):\n",
    "        \"\"\"Initialization takes in a test and train set and an optional hyperparameter space\n",
    "\n",
    "        Args:\n",
    "            X_train (array): training features\n",
    "            X_test (array): testing features\n",
    "            y_train (array): testing labels\n",
    "            y_test (array): testing labels\n",
    "            parameter_space (dict): hyperopt compatible parameter space\n",
    "            model (module pointer): machine learning model compatiable with parameter space\n",
    "            loss_function (object): function to calculate the desired loss. Note loss will be inverted in output\n",
    "            loss_params (dict): additional arguments to be passed to loss function\n",
    "            loss_pred_proba (bool): whether loss function uses predicted probs (True) or predicted values (False)\n",
    "        \"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.model = model\n",
    "        self.loss_function = loss_function\n",
    "        self.loss_params = loss_params\n",
    "        self.loss_pred_proba = loss_pred_proba\n",
    "\n",
    "        if parameter_space is None:\n",
    "            self.parameter_space = {\n",
    "                \"n_estimators\": hp.choice(\n",
    "                    \"n_estimators\", np.arange(10, dtype=int) * 30 + 50\n",
    "                ),\n",
    "                \"criterion\": hp.choice(\n",
    "                    \"criterion\",\n",
    "                    np.array([\"gini\", \"entropy\"]),\n",
    "                ),\n",
    "                \"max_depth\": hp.choice(\"max_depth\", np.arange(21, dtype=int) + 2),\n",
    "                \"min_samples_split\": hp.uniform(\"min_samples_split\", 0.0, 0.05),\n",
    "                \"max_samples\": hp.uniform(\"max_samples\", 0.5, 0.95),\n",
    "                \"n_jobs\": hp.choice(\"n_jobs\", [-1]),\n",
    "                \"random_state\": hp.choice(\"random_state\", [1738]),\n",
    "                \"bootstrap\": hp.choice(\"bootstrap\", [True]),\n",
    "            }\n",
    "        else:\n",
    "            self.parameter_space = parameter_space\n",
    "\n",
    "    def objective(self, params):\n",
    "        \"\"\"Objective function for loss that is provided to perform the MINLP\n",
    "        optimizaiton in hyperopt\n",
    "\n",
    "        Args:\n",
    "            params (dict): hyperopt formated dictionary of hyperparameters\n",
    "\n",
    "        Returns:\n",
    "            dict: loss and status for hyperopt optimization\n",
    "        \"\"\"\n",
    "        model = self.model(**params)\n",
    "        model.fit(self.X_train, self.y_train)\n",
    "        if self.loss_pred_proba:\n",
    "            pred = model.predict_proba(self.X_test)\n",
    "        else:\n",
    "            pred = model.predict(self.X_test)\n",
    "        loss = 1 - self.loss_function(self.y_test, pred[:, 1], **self.loss_params)\n",
    "        return {\"loss\": loss, \"status\": STATUS_OK}\n",
    "\n",
    "    def optimize(self, max_evals=20):\n",
    "        \"\"\"optimizes the hyperparameter space in the object\n",
    "\n",
    "        Args:\n",
    "            max_evals (int): number of hyperopt iterations\n",
    "\n",
    "        Returns:\n",
    "            dict: best hyperparameters\n",
    "        \"\"\"\n",
    "        trials = Trials()\n",
    "        best = fmin(\n",
    "            fn=self.objective,\n",
    "            space=self.parameter_space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=max_evals,\n",
    "            trials=trials,\n",
    "        )\n",
    "        return space_eval(self.parameter_space, best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the DSDE standard process for using Xboost with hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def train_xgb(X_train,\n",
    "              X_valid,\n",
    "              y_train,\n",
    "              y_valid,\n",
    "              evals=20,\n",
    "              sub=200000,\n",
    "              train=1000000,\n",
    "              early_stop=10,\n",
    "              verbose=False,\n",
    "              **kwargs):\n",
    "    \"\"\"\n",
    "    Binary Classifiation Xgboost DSDE hyper opt approach this should be\n",
    "    reviewed and customized for your use case.\n",
    "    \"\"\"\n",
    "    parameter_space = {\n",
    "        'max_depth': hp.choice('max_depth', np.arange(21, dtype=int) + 3),\n",
    "        'n_estimators': hp.choice('n_estimators', np.arange(301, dtype=int) + 100),\n",
    "        'gamma': hp.uniform('gamma', 0, 5),\n",
    "        'learning_rate': hp.loguniform('learning_rate', np.log(0.005), np.log(0.2)),\n",
    "        'colsample_bytree': hp.quniform('colsample_bytree', 0.1, 1, 0.01),\n",
    "        'subsample': hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "        'base_score': hp.quniform('base_score', 0.3, 0.8, 0.01),\n",
    "        'objective': hp.choice('objective', ['binary:logistic']),\n",
    "        'eval_metric': hp.choice('eval_metric', ['logloss', 'error', 'auc', 'aucpr', 'map']),\n",
    "        'use_label_encoder': hp.choice('use_label_encoder', [False]),\n",
    "        'gpu_id': hp.choice('gpu_id', [-1]),\n",
    "    }\n",
    "    logging.info(f'Hyper tuning on {X_train[0:sub].shape[0]} rows')\n",
    "    opt = HpOptBinary(\n",
    "        X_train=X_train[0:sub].to_numpy(),\n",
    "        X_test=X_valid.to_numpy(),\n",
    "        y_train=y_train[0:sub].to_numpy(),\n",
    "        y_test=y_valid.to_numpy(),\n",
    "        parameter_space=parameter_space\n",
    "    )\n",
    "    best = opt.optimize(max_evals=evals)\n",
    "    logging.info(f'Full training on {X_train[0:train].shape[0]} rows')\n",
    "    model = xgb.XGBClassifier(**best, n_jobs=-1)\n",
    "    eval_set = [(X_valid, y_valid)]\n",
    "    model.fit(X_train[0:train],\n",
    "              y_train[0:train],\n",
    "              eval_set=eval_set,\n",
    "              early_stopping_rounds=early_stop,\n",
    "              verbose=verbose)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_xgb_basic(X_train, X_valid, y_train, y_valid, early_stop=10, verbose=True, *args):\n",
    "    \"\"\"\n",
    "    Binary Classifiation Xgboost Sklearn API Call Basic HyperParameters\n",
    "    \"\"\"\n",
    "    logger.info(f'Training on {X_train.shape[0]} rows')\n",
    "    model = xgb.XGBClassifier(n_jobs=-1)\n",
    "    eval_set = [(X_valid, y_valid)]\n",
    "    model.fit(X_train, y_train, eval_set=eval_set, early_stopping_rounds=early_stop, verbose=verbose)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_logistic(X_train, X_test, y_train, y_test, model=LogisticRegression, evals=20, sub=200000, train=1000000):\n",
    "    \"\"\"Logistic Regression Example to show how simple this can be to switch the model being used in this template\"\"\"\n",
    "    parameter_space = {\n",
    "        'class_weight': hp.choice('class_weight', [None, 'balanced']),\n",
    "        'warm_start': hp.choice('warm_start', [True, False]),\n",
    "        'fit_intercept': hp.choice('fit_intercept', [True, False]),\n",
    "        'tol': hp.uniform('tol', 0.00001, 0.0001),\n",
    "        'C': hp.uniform('C', 0.05, 3),\n",
    "        'solver': hp.choice('solver', ['lbfgs']),  # , 'liblinear', 'sag', 'saga']),\n",
    "        'max_iter': hp.choice('max_iter', range(5, 1000))\n",
    "    }\n",
    "    logger.info(f'Hyper tuning on {X_test[0:sub].shape[0]} rows')\n",
    "    opt = HpOptBinary(X_train[0:sub], X_test[0:sub], y_train[0:sub],\n",
    "                      y_test[0:sub], model=model, parameter_space=parameter_space)\n",
    "    best = opt.optimize(max_evals=evals)\n",
    "    logger.info(f'Full training on {X_test[0:train].shape[0]} rows')\n",
    "    model = LogisticRegression(**best, n_jobs=-1)\n",
    "    model.fit(X_train[0:train], y_train[0:train])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
