{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Utilities\n",
    "\n",
    "> Functions Used In Modeling Efforts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp modeling.custom_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeremydemlow/miniforge3/envs/ltbp/lib/python3.9/site-packages/snowflake/connector/options.py:96: UserWarning: You have an incompatible version of 'pyarrow' installed (6.0.0), please install a version that adheres to: 'pyarrow<8.1.0,>=8.0.0; extra == \"pandas\"'\n",
      "  warn_incompatible_dep(\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "from data_system_utilities.azure.storage import FileHandling\n",
    "from data_system_utilities.file_parsers import yaml\n",
    "from data_system_utilities.snowflake.utils import make_stage_query_generator\n",
    "\n",
    "from machine_learning_utilities import preprocessing\n",
    "\n",
    "from LTBP.data.utils import snowflake_query, get_yaml_dicts, generate_data_lake_query\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from rfpimp import *\n",
    "from datetime import datetime\n",
    "\n",
    "import sklearn.preprocessing as y_transform\n",
    "import os\n",
    "import logging\n",
    "import pickle\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def evaluate(model,\n",
    "             X_valid,\n",
    "             y_valid,\n",
    "             y_var,\n",
    "             feature_importance=True):\n",
    "    \"\"\"\n",
    "    Utlity to give experiment table information about the model\n",
    "    this is fully customizable and can be changed to be regression\n",
    "    RMSE, R2, MSE for example and changing the columns this function\n",
    "    isn't a dynamic function it needs to be written for a specific use\n",
    "    case.\n",
    "\n",
    "    Args:\n",
    "    * model (classifer): sklearn model for this\n",
    "    * X_valid (np.array): Validation set Traing\n",
    "    * y_valid (np.array): Actuals for Validation\n",
    "    * y_var (str): variable name being predicted\n",
    "\n",
    "    Returns:\n",
    "    * dict: dependent on return statement\n",
    "    \"\"\"\n",
    "    y_pred_proba = model.predict_proba(X_valid)\n",
    "    y_pred = model.predict(X_valid)\n",
    "    auc = metrics.roc_auc_score(y_valid, y_pred_proba[:, 1])\n",
    "    acc = metrics.accuracy_score(y_valid, y_pred)\n",
    "    bacc = metrics.balanced_accuracy_score(y_valid, y_pred)\n",
    "    columns = ['auc', 'acc', 'bacc']\n",
    "    logging.info(f'Variable(s) of interest {y_var} AUC: {auc:.3f}    Accuracy: {acc:.3f}    Balanced Accuracy: {bacc:.3f}')\n",
    "    if feature_importance is True:\n",
    "        fi_permutation = importances(model, X_valid, y_valid) # noqa:\n",
    "        fi_permutation = (fi_permutation\n",
    "                          .reset_index()\n",
    "                          .rename({'Feature': 'COLS', 'Importance': 'IMP'}, axis=1))\n",
    "        logging.info(f'Feature Importance df: \\n {fi_permutation}')\n",
    "    return auc, acc, bacc, columns, fi_permutation if feature_importance else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def send_holdout_results_to_sf(sf, \n",
    "                               id_list:list,\n",
    "                               probs,\n",
    "                               experiment,\n",
    "                               experiment_name,\n",
    "                               etl_dict,\n",
    "                               model_dict,\n",
    "                               drop_table:bool=False\n",
    "                              ):\n",
    "    hold_out_df = pd.DataFrame(id_list)\n",
    "    hold_out_df['PROBABILITY'] = probs[:, 1]\n",
    "    hold_out_df['DATECREATED'] = dt.datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    hold_out_df['EXP_COMMIT_CI_SHA'] = experiment_name+'_'+os.environ.get('CI_COMMIT_SHA', 'LocalRunNBS')\n",
    "    logging.info(f'hold out data preview going to snowflake {hold_out_df.head(3)}')\n",
    "    sf = snowflake_query(sfSchema=sfSchema)\n",
    "    if drop_table:\n",
    "        sf.run_sql_str(f\"DROP TABLE {models['hold_out_table']}\")\n",
    "    sf.infer_to_snowflake(test_df_results,\n",
    "                          table_name=models['hold_out_table'])\n",
    "    logging.info('saving test prediction file')\n",
    "    test_df_results.to_csv(f'holdout_{experiment_name}.csv', index=False)\n",
    "    adls_path = os.path.join((os.path.join(etl_dict['data_lake_path'], 'experiments', experiment_name)\n",
    "        if experiment \n",
    "        else os.path.join(etl_dict['data_lake_path'], \n",
    "        os.environ.get('CI_COMMIT_SHA', 'LocalRunNBS')))\n",
    "        , 'holdout_results/')\n",
    "    logging.info(f'sending prediction file to azure to {adls_path}')\n",
    "    az = FileHandling(os.environ[model_dict['connection_str']])\n",
    "    _ = az.upload_file(\n",
    "        azure_file_path=adls_path,\n",
    "        local_file_path=f'holdout_{experiment_name}.csv',\n",
    "        container_name=etl[\"azure_container\"],\n",
    "        overwrite=True,\n",
    "    )\n",
    "    os.unlink(f'holdout_{experiment_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def move_dev_holdout_table_to_prod_location(sf,\n",
    "                                            exp):\n",
    "    logging.info('Replacing Prod HoldOut With Newest Promoted')\n",
    "    sf.run_str_query(f\"\"\"\n",
    "                      CREATE OR REPLACE TABLE MACHINELEARNINGOUTPUTS.ltbp.{exp['holdout_tb_name']} AS\n",
    "                      SELECT * FROM MACHINELEARNINGOUTPUTS.DEV.{exp['holdout_tb_name']};\n",
    "                      \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
