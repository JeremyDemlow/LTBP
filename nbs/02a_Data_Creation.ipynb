{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02fc6a4d",
   "metadata": {},
   "source": [
    "# Creating Feature Set\n",
    "\n",
    "> Functions Used In Modeling Efforts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83f1cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp scripts.data_creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a062342",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "from fastcore.script import Param, call_parse\n",
    "\n",
    "from data_system_utilities.snowflake.query import Snowflake\n",
    "from data_system_utilities.azure.storage import FileHandling\n",
    "from data_system_utilities.snowflake.copyinto import sf_to_adls_url_query_generator\n",
    "\n",
    "from machine_learning_utilities.dataset_creation.snowflake import pull_features_from_snowflake\n",
    "\n",
    "from LTBP.data.utils import query_feature_sets_to_adls_parquet_sf_fs, snowflake_query, get_yaml_dicts\n",
    "from LTBP import files\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6430da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from pathlib import Path\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80e14e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| skip\n",
    "def write_yaml_file(file_path: str, file_name: str, dictionary: dict):\n",
    "    with open(Path(file_path, file_name), 'w') as f:\n",
    "        yaml.dump(dictionary, f)\n",
    "\n",
    "etl_dict = dict({\n",
    " 'azure_account': 'vaildtscadls',\n",
    " 'azure_container': 'vailadls',\n",
    " 'data_lake_path': 'projects/LTBP/FY23/',\n",
    " 'exp_name' : 'status_quo/',\n",
    " 'max_file_size': '32000000',\n",
    " 'over_write': 'True',\n",
    " 'query_file_location': 'sql_files/',\n",
    " 'stage_name': 'resortvisitation',\n",
    "})\n",
    "\n",
    "write_yaml_file('./LTBP/files/yaml_files/', 'etl.yaml', etl_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6b87c4",
   "metadata": {},
   "source": [
    "`data_creation`\n",
    "\n",
    "With the utility for nbdev and fastcore the below function becomes a command line argument that is created in the settings.ini **console_scripts**.\n",
    "\n",
    "go to ./settings.ini find this section and add the scripts that you make.\n",
    "\n",
    "```ini\n",
    "console_scripts = data_creation=buypass.scripts.preprocess:data_creation\n",
    "<name of command line arg> = <library name>.<path to function>.<file name>:<function name>\n",
    "```\n",
    "\n",
    "**What is happpening is this script**\n",
    "\n",
    "Overview:\n",
    "\n",
    "TODO: re-write\n",
    "\n",
    "> Key Note: the train_or_test is the trigger for test/inference data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a81e190",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@call_parse\n",
    "def data_creation(train_or_inference: Param(help=\"YAML section to read\", type=str, default='TRAINING'), # noqa:\n",
    "                  experiment: Param(help=\"YAML section to read\", type=str, default='False')):  # noqa:\n",
    "    \"\"\"Creates a feature set for a experiment data set or a production level run feature set\"\"\"\n",
    "    experiment = True if experiment == 'True' else False\n",
    "    logging.info(f\"This is a {'experiment run' if experiment else 'production run'}\")\n",
    "    logging.info('Loading Yaml Files..')\n",
    "    features, udf_inputs, etl = get_yaml_dicts(['features.yaml', 'udf_inputs.yaml', 'etl.yaml'])\n",
    "    logging.info('Generating Feature Set Query')\n",
    "    query = pull_features_from_snowflake(feature_dict=features,\n",
    "                                         udf_inputs=udf_inputs[train_or_inference.upper()],\n",
    "                                         filepath_to_grain_list_query='./LTBP/files/sql_files/')\n",
    "    data_lake_path = (os.path.join(etl['data_lake_path'], 'experiments', etl['exp_name'])\n",
    "                      if experiment \n",
    "                      else os.path.join(etl['data_lake_path'], \n",
    "                                        os.environ.get('CI_COMMIT_SHA', 'LocalRunNBS')))\n",
    "    logging.info(f'Checking {data_lake_path} to either skip creation for experiment or create a production dataset')\n",
    "    fh = FileHandling(os.environ['DATALAKE_CONN_STR_SECRET'])\n",
    "    ald_files = fh.ls_blob(path=data_lake_path, container_name=etl['azure_container'])\n",
    "    sf = snowflake_query()\n",
    "    if ald_files == []:\n",
    "        query_feature_sets_to_adls_parquet_sf_fs(\n",
    "            sf_connection=sf,\n",
    "            sf_query=query,\n",
    "            query_file_path=os.path.join(files.__path__[0], etl['query_file_path']),\n",
    "            azure_account=etl[\"azure_account\"],\n",
    "            azure_container=etl[\"azure_container\"],\n",
    "            data_lake_path=data_lake_path, # TODO: Think about experiments versus \n",
    "            partition_by=None,\n",
    "            data_lake_sas_token=os.environ[\"DATALAKE_SAS_TOKEN_SECRET\"],\n",
    "        )\n",
    "    else:\n",
    "        logging.warning(f'{data_lake_path} already exists this should be do experimentation runs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d64e0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# experiment = True # this will trigger if the feature set needs to be created\n",
    "# train_or_inference = 'TRAINING'\n",
    "\n",
    "\n",
    "# logging.info('Loading Yaml Files..')\n",
    "# features, udf_inputs, etl = get_yaml_dicts(['features.yaml', 'udf_inputs.yaml', 'etl.yaml'])\n",
    "# logging.info('Generating Feature Set Query')\n",
    "# query = pull_features_from_snowflake(feature_dict=features,\n",
    "#                                      udf_inputs=udf_inputs[train_or_inference.upper()],\n",
    "#                                      filepath_to_grain_list_query='./LTBP/files/sql_files/')\n",
    "\n",
    "# data_lake_path = (os.path.join(etl['data_lake_path'], 'experiments', etl['exp_name'])\n",
    "#                   if experiment \n",
    "#                   else os.path.join(etl['data_lake_path'], \n",
    "#                                     os.environ.get('CI_COMMIT_SHA', 'LocalRunNBS')))\n",
    "# logging.info(f'Checking {data_lake_path} to either skip creation for experiment or create a production dataset')\n",
    "\n",
    "# fh = FileHandling(os.environ['DATALAKE_CONN_STR_SECRET'])\n",
    "# ald_files = fh.ls_blob(path=data_lake_path, container_name=etl['azure_container'])\n",
    "# sf = snowflake_query()\n",
    "# if ald_files == []:\n",
    "#     query_feature_sets_to_adls_parquet_sf_fs(\n",
    "#         sf_connection=sf,\n",
    "#         sf_query=query,\n",
    "#         query_file_path=os.path.join(files.__path__[0], query_file_path),\n",
    "#         azure_account=etl[\"azure_account\"],\n",
    "#         azure_container=etl[\"azure_container\"],\n",
    "#         data_lake_path=data_lake_path, # TODO: Think about experiments versus \n",
    "#         partition_by=None,\n",
    "#         data_lake_sas_token=os.environ[\"DATALAKE_SAS_TOKEN_SECRET\"],\n",
    "#     )\n",
    "# else:\n",
    "#     logging.warning(f'{data_lake_path} already exists this should be do experimentation runs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1e9782",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef1cb04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e81cb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
