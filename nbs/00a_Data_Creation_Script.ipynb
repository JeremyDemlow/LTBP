{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e33e7afd",
   "metadata": {},
   "source": [
    "# Data Creation Script\n",
    "\n",
    "> Snowflake Feature Store Query or Custom SQL File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8cd51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp scripts.data_creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebd9853",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "from fastcore.script import Param, call_parse\n",
    "\n",
    "from data_system_utilities.azure.storage import FileHandling\n",
    "\n",
    "from LTBP.data.utils import (\n",
    "    query_feature_sets_to_adls_parquet_sf_fs, snowflake_query,\n",
    "    get_yaml_dicts, pull_features_from_snowflake\n",
    ")\n",
    "\n",
    "import LTBP.files as files\n",
    "import os\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcc8351",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from pathlib import Path\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2ccd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def write_yaml_file(file_path: str, file_name: str, dictionary: dict):\n",
    "    with open(Path(file_path, file_name), 'w') as f:\n",
    "        yaml.dump(dictionary, f)\n",
    "\n",
    "etl = dict({\n",
    " 'azure_account': 'vaildtscadls',\n",
    " 'azure_container': 'vailadls',\n",
    " 'data_lake_path': 'projects/LTBP/FY23/',\n",
    " 'max_file_size': '32000000',\n",
    " 'over_write': 'True',\n",
    " 'query_file_path': 'sql_files/',\n",
    " 'stage_name': 'ltbp',\n",
    " 'FY_folder' : 'FY23',\n",
    " 'extra_statement' : {\n",
    "    'TRAINING': None,\n",
    "    'INFERENCE': None\n",
    " }\n",
    "})\n",
    "\n",
    "write_yaml_file('./LTBP/files/yaml_files/', 'etl.yaml', etl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87d4494",
   "metadata": {},
   "source": [
    "# Snowflake Feature Store\n",
    "\n",
    "> This should be the default approach that we use so that all projects pull from similar data definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976f2f58",
   "metadata": {},
   "source": [
    "`data_creation`\n",
    "\n",
    "\n",
    "This function was created to the library from the section **console_scripts** settings.ini.\n",
    "\n",
    "\n",
    "To add a new CLI command please go to ./settings.ini find this section and add the scripts that you make.\n",
    "\n",
    "```ini\n",
    "console_scripts = data_creation=buypass.scripts.preprocess:data_creation\n",
    "<name of command line arg> = <library name>.<path to function>.<file name>:<function name>\n",
    "```\n",
    "\n",
    "**What is happpening is this script**\n",
    "\n",
    "What is happpening is this script\n",
    "\n",
    "Overview:\n",
    "\n",
    "1. Generates a snowflake feature story query is generated from the yaml files features and udf_inputs that are needed to query the feature set of interest.\n",
    "\n",
    "    - `get_yaml_dicts` --> `pull_features_from_snowflake` => generates a string query to be queried\n",
    "    \n",
    "2. Then the generated query is sent `query_feature_sets_to_adls_parquet_sf_fs` to then send then send to adls for the library to query in the modeling section.\n",
    "    \n",
    "    > If you have a use case that needs to take advantage of parquet partitioning then the function allows for that, but standard use cases haven't needed them so that would just be a nice enhancement\n",
    "\n",
    "\n",
    "> Key Note: the train_or_test is the trigger for test/inference data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92bdd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@call_parse\n",
    "def data_creation(\n",
    "    yaml_file_list: Param(help=\"YAML files to read\", type=list, default=['features.yaml', 'udf_inputs.yaml', 'etl.yaml']),  # noqa:\n",
    "    train_or_inference: Param(help=\"Upper case training or inference\", type=str, default='TRAINING'),  # noqa\n",
    "    experiment_name: Param(help=\"Experiment name to run case sensetive\", type=str, default='BASELINE'),  # noqa:\n",
    "    experiment: Param(help=\"Boolen if it's a experiment or a run to run for a commit hash\", type=bool, default=True)  # noqa:\n",
    "    ):  # noqa:\n",
    "    \"\"\"Creates a feature set for a experiment data set or a production level run feature set\"\"\"\n",
    "    logging.info(f\"This is a {'experiment run' if experiment else 'production run'}\")\n",
    "    logging.info('Loading Yaml Files..')\n",
    "    features, udf_inputs, etl = get_yaml_dicts(yaml_file_list)\n",
    "    logging.info('Generating Feature Set Query')\n",
    "    query = pull_features_from_snowflake(feature_dict=features,\n",
    "                                         udf_inputs=udf_inputs[train_or_inference.upper()],\n",
    "                                         filepath_to_grain_list_query=os.path.join(files.__path__[0], etl['query_file_path']),\n",
    "                                         experiment_name=experiment_name)\n",
    "    data_lake_path = os.path.join(\n",
    "        (os.path.join(etl['data_lake_path'], 'experiments', experiment_name)\n",
    "         if experiment\n",
    "         else os.path.join(\n",
    "             etl['data_lake_path'],\n",
    "             os.environ.get('CI_COMMIT_SHA', 'LocalRunTest'),\n",
    "             experiment_name\n",
    "        )\n",
    "        ), train_or_inference.lower()+'_data/')\n",
    "    logging.info(f'Checking {data_lake_path} to either skip creation for experiment or create a production dataset')\n",
    "    fh = FileHandling(os.environ['DATALAKE_CONN_STR_SECRET'])\n",
    "    ald_files = fh.ls_blob(path=data_lake_path, container_name=etl['azure_container'])\n",
    "    sf = snowflake_query()\n",
    "    if ald_files == []:\n",
    "        query_feature_sets_to_adls_parquet_sf_fs(\n",
    "            sf_connection=sf,\n",
    "            sf_query=query,\n",
    "            azure_account=etl[\"azure_account\"],\n",
    "            azure_container=etl[\"azure_container\"],\n",
    "            data_lake_path=data_lake_path,  # TODO: Think about experiments versus\n",
    "            partition_by=None,\n",
    "            data_lake_sas_token=os.environ[\"DATALAKE_SAS_TOKEN_SECRET\"],\n",
    "        )\n",
    "    else:\n",
    "        logging.warning(f'{data_lake_path} already exists is the expected behavior for experiments and local notebook runs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe0b8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:This is a production run\n",
      "INFO:root:Loading Yaml Files..\n",
      "INFO:root:Generating Feature Set Query\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'projects/LTBP/FY23/LocalRunTest/BASELINE/training_data/'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | skip\n",
    "experiment = False # this will trigger if the feature set needs to be created\n",
    "train_or_inference = 'TRAINING' # 'INFERENCE'\n",
    "experiment_name='BASELINE'\n",
    "\n",
    "\n",
    "logging.info(f\"This is a {'experiment run' if experiment else 'production run'}\")\n",
    "logging.info('Loading Yaml Files..')\n",
    "features, udf_inputs, etl = get_yaml_dicts(['features.yaml', 'udf_inputs.yaml', 'etl.yaml'])\n",
    "logging.info('Generating Feature Set Query')\n",
    "\n",
    "data_lake_path = os.path.join(\n",
    "    (os.path.join(etl['data_lake_path'], 'experiments', experiment_name)\n",
    "     if experiment\n",
    "     else os.path.join(\n",
    "         etl['data_lake_path'],\n",
    "         os.environ.get('CI_COMMIT_SHA', 'LocalRunTest'),\n",
    "         experiment_name\n",
    "     )\n",
    "    ), train_or_inference.lower()+'_data/')\n",
    "data_lake_path\n",
    "\n",
    "\"\"\"\n",
    "This is to help a user develop locally for the script if things are changing best method is to pull the above\n",
    "cell into this one and begin to develop or you can do this is a .py file, but this is my prefered method\n",
    "\"\"\"\n",
    "experiment = True # this will trigger if the feature set needs to be created\n",
    "train_or_inference = 'TRAINING' # 'INFERENCE'\n",
    "experiment_name='BASELINE'\n",
    "\n",
    "\n",
    "logging.info(f\"This is a {'experiment run' if experiment else 'production run'}\")\n",
    "logging.info('Loading Yaml Files..')\n",
    "features, udf_inputs, etl = get_yaml_dicts(['features.yaml', 'udf_inputs.yaml', 'etl.yaml'])\n",
    "logging.info('Generating Feature Set Query')\n",
    "query = pull_features_from_snowflake(feature_dict=features,\n",
    "                                     udf_inputs=udf_inputs[train_or_inference.upper()],\n",
    "                                     filepath_to_grain_list_query='./LTBP/files/sql_files/',\n",
    "                                     experiment_name=experiment_name)\n",
    "data_lake_path = os.path.join((os.path.join(etl['data_lake_path'], 'experiments', experiment_name)\n",
    "                  if experiment \n",
    "                  else os.path.join(etl['data_lake_path'], \n",
    "                                    os.environ.get('CI_COMMIT_SHA', 'LocalRunTest')))\n",
    "                 , train_or_inference.lower()+'_data/')\n",
    "logging.info(f'Checking {data_lake_path} to either skip creation for experiment or create a production dataset')\n",
    "fh = FileHandling(os.environ['DATALAKE_CONN_STR_SECRET'])\n",
    "\n",
    "ald_files = fh.ls_blob(path=data_lake_path, container_name=etl['azure_container'])\n",
    "sf = snowflake_query()\n",
    "if ald_files == []:\n",
    "    query_feature_sets_to_adls_parquet_sf_fs(\n",
    "        sf_connection=sf,\n",
    "        sf_query=query,\n",
    "        query_file_path=os.path.join(files.__path__[0], etl['query_file_path']),\n",
    "        azure_account=etl[\"azure_account\"],\n",
    "        azure_container=etl[\"azure_container\"],\n",
    "        data_lake_path=data_lake_path, # TODO: Think about experiments versus \n",
    "        partition_by=None,\n",
    "        data_lake_sas_token=os.environ[\"DATALAKE_SAS_TOKEN_SECRET\"],\n",
    "    )\n",
    "else:\n",
    "    logging.warning(f'{data_lake_path} already exists this should be do experimentation runs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5ad4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | skip\n",
    "from LTBP.data.utils import snowflake_query, get_yaml_dicts, generate_data_lake_query\n",
    "from LTBP import files\n",
    "\n",
    "from data_system_utilities.file_parsers import yaml\n",
    "from data_system_utilities.snowflake.utils import make_stage_query_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409cd363",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| skip\n",
    "stage_url = f\"\"\"azure://{etl['azure_account']}.blob.core.windows.net/\n",
    "{etl['azure_container']}/{etl['data_lake_path']}{\n",
    "(os.path.join('experiments', experiment_name)\n",
    "if experiment \n",
    "else os.path.join('LocalRunTest'))}\"\"\".replace('\\n', '')\n",
    "stage_query = make_stage_query_generator(\n",
    "    stage_name=etl[\"stage_name\"] + etl['FY_folder'] + os.environ.get('CI_COMMIT_SHA', 'LocalRunTest'),\n",
    "    url=stage_url,\n",
    "    sas_token=os.environ[\"DATALAKE_SAS_TOKEN_SECRET\"],\n",
    "    file_type=\"parquet\",\n",
    ")\n",
    "_ = sf.run_sql_str(stage_query)\n",
    "# TODO: Figure out a identification feature like season year \n",
    "# Udf grain is ECID, which is easy to get, but season year isn't obivous some thought is needed\n",
    "indentification = [col.split('.')[-1] for col in udf_inputs[train_or_inference]['UDF_GRAIN']]\n",
    "columns = [col.upper() for col in features.keys()]\n",
    "query = generate_data_lake_query(stage_name=(etl[\"stage_name\"] \n",
    "                                             + etl['FY_folder'] \n",
    "                                             + os.environ.get('CI_COMMIT_SHA', 'LocalRunTest')),\n",
    "     stage_path=train_or_inference.lower()+'_data/',\n",
    "     columns=indentification + columns,\n",
    "     extra_statement=None)\n",
    "logging.info(f'adls snowflake stage query {query}')\n",
    "df = sf.run_sql_str(query)\n",
    "logging.info(f'Preview dataframe queried {df.head()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9ef40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:This is a experiment run\n",
      "INFO:root:Loading Yaml Files..\n",
      "INFO:root:Generating Feature Set Query\n",
      "INFO:root:static features in data set: \n",
      " ['DESTINATIONGEOAFINITYLABEL', 'GenderCode']\n",
      "INFO:root:temporal features in data set: \n",
      " ['Age', 'AvgVisitPerSeason', 'BoughtPass', 'EverCorePass', 'EverPass', 'GuestBehavior', 'IsEpicMixActivated', 'MarketingZone', 'MostCommonTicketComp', 'MostSubSeasonVisited', 'MostVisitedRegion', 'MostVisitedResort', 'OnlySingleResortKey', 'PartnerResortScannerFlag', 'ResortsVisited', 'SkierabilityLabel', 'SubSeasonsPerYear', 'TotalSeasonsScanned', 'TotalVisits', 'VisitMostInPeak']\n",
      "INFO:root:Appending static feature DESTINATIONGEOAFINITYLABEL to query\n",
      "INFO:root:Appending static feature GenderCode to query\n",
      "INFO:root:Finished appending static features\n",
      "INFO:root:reading inference_base.sql for base query...\n",
      "INFO:root:final query output: \n",
      " select\n",
      "base.*\n",
      ", joined.DESTINATIONGEOAFINITYLABEL\n",
      ", joined.GenderCode\n",
      ", MACHINELEARNINGFEATURES.PROD.Age_ECID_Temporal(base.ECID, 20190101, 20221005) as Age, MACHINELEARNINGFEATURES.PROD.AvgVisitPerSeason_ECID_Temporal(base.ECID, 20190101, 20221005) as AvgVisitPerSeason, MACHINELEARNINGFEATURES.PROD.BoughtPass_ECID_Temporal(base.ECID, '2021/22') as BoughtPass, MACHINELEARNINGFEATURES.PROD.EverCorePass_ECID_Temporal(base.ECID, 20051001, 20221005) as EverCorePass, MACHINELEARNINGFEATURES.PROD.EverPass_ECID_Temporal(base.ECID, 20051001, 20221005) as EverPass, MACHINELEARNINGFEATURES.PROD.GuestBehavior_ECID_Temporal(base.ECID, '2021/22') as GuestBehavior, MACHINELEARNINGFEATURES.PROD.IsEpicMixActivated_ECID_Temporal(base.ECID, '2021/22') as IsEpicMixActivated, MACHINELEARNINGFEATURES.PROD.MarketingZone_ECID_Temporal(base.ECID, '2021/22') as MarketingZone, MACHINELEARNINGFEATURES.PROD.MostCommonTicketComp_ECID_Temporal(base.ECID, 20190101, 20221005) as MostCommonTicketComp, MACHINELEARNINGFEATURES.PROD.MostSubSeasonVisited_ECID_Temporal(base.ECID, 20190101, 20221005) as MostSubSeasonVisited, MACHINELEARNINGFEATURES.PROD.MostVisitedRegion_ECID_Temporal(base.ECID, 20190101, 20221005) as MostVisitedRegion, MACHINELEARNINGFEATURES.PROD.MostVisitedResort_ECID_Temporal(base.ECID, 20190101, 20221005) as MostVisitedResort, MACHINELEARNINGFEATURES.PROD.OnlySingleResortKey_ECID_Temporal(base.ECID, 20190101, 20221005) as OnlySingleResortKey, MACHINELEARNINGFEATURES.PROD.PartnerResortScannerFlag_ECID_Temporal(base.ECID, 20190101, 20221005) as PartnerResortScannerFlag, MACHINELEARNINGFEATURES.PROD.ResortsVisited_ECID_Temporal(base.ECID, 20190101, 20221005) as ResortsVisited, MACHINELEARNINGFEATURES.PROD.SkierabilityLabel_ECID_Temporal(base.ECID, '2021/22') as SkierabilityLabel, MACHINELEARNINGFEATURES.PROD.SubSeasonsPerYear_ECID_Temporal(base.ECID, 20190101, 20221005) as SubSeasonsPerYear, MACHINELEARNINGFEATURES.PROD.TotalSeasonsScanned_ECID_Temporal(base.ECID, 20190101, 20221005) as TotalSeasonsScanned, MACHINELEARNINGFEATURES.PROD.TotalVisits_ECID_Temporal(base.ECID, 20190101, 20221005) as TotalVisits, MACHINELEARNINGFEATURES.PROD.VisitMostInPeak_ECID_Temporal(base.ECID, 20190101, 20221005) as VisitMostInPeak\n",
      "\n",
      ", '2021/22' as SEASONYEAR\n",
      "from\n",
      "(-- base EDEE team guest behavior\n",
      "  with edee_base as (\n",
      "      SELECT \n",
      "            distinct ECID\n",
      "      from BIDE_EDWDB_CUSTOMERMART_PROD.DBO.CustomerPASSPURCHASEBEHAVIOR cla\n",
      "      where \n",
      "              cla.GuestPassPurchaseBehaviorDetailLabel <> 'Unknown'\n",
      "  ),\n",
      "\n",
      "  -- pass prospects not in guest behavior\n",
      "  prospects as (\n",
      "      Select\n",
      "            distinct ECID\n",
      "      from Vail_Reporting.Prod.GuestBehaviorBase\n",
      "      where \n",
      "              GuestBehavior = 'Prospect'\n",
      "          and salesseason = '2021/22'\n",
      "  ),\n",
      "\n",
      "  -- paid prospects not in guest behavior\n",
      "  other_scans as (\n",
      "      select\n",
      "            distinct a.ECID\n",
      "      from\n",
      "      (\n",
      "          select \n",
      "                distinct ECID\n",
      "              , SeasonYear\n",
      "          from \"BIDE_EDWDB_ARA_PROD\".\"DBO\".\"SCANDAY\"\n",
      "              where Season = 'Winter'\n",
      "      ) a\n",
      "      left join Vail_Reporting.Prod.GuestBehaviorBase b \n",
      "          on a.ECID = b.ECID and a.SeasonYear = b.SalesSeason\n",
      "      where \n",
      "              b.GuestBehavior is null\n",
      "          and a.seasonyear = '2021/22'\n",
      "  )\n",
      "\n",
      "  select\n",
      "        distinct ECID\n",
      "      , 'Inference Set' as SeasonYear\n",
      "  from\n",
      "  (\n",
      "      select\n",
      "            coalesce(base.ecid, pro.ecid, os.ecid) as ecid\n",
      "      from edee_base base\n",
      "      full outer join prospects pro\n",
      "          on pro.ecid = base.ecid\n",
      "      full outer join other_scans os\n",
      "          on os.ecid = base.ecid\n",
      "  ) a)base\n",
      "inner join machinelearningfeatures.prod.featurestore_ecid joined on joined.ecid = base.ecid\n",
      "INFO:root:Checking projects/LTBP/FY23/experiments/BASELINE/inference_data/ to either skip creation for experiment or create a production dataset\n",
      "INFO:data_system_utilities.azure.storage:number of files in container path recursively 0\n",
      "INFO:data_system_utilities.snowflake.copyinto:\n",
      "COPY INTO 'azure://vaildtscadls.blob.core.windows.net/vailadls/projects/LTBP/FY23/experiments/BASELINE/inference_data/'\n",
      "FROM (select\n",
      "base.*\n",
      ", joined.DESTINATIONGEOAFINITYLABEL\n",
      ", joined.GenderCode\n",
      ", MACHINELEARNINGFEATURES.PROD.Age_ECID_Temporal(base.ECID, 20190101, 20221005) as Age, MACHINELEARNINGFEATURES.PROD.AvgVisitPerSeason_ECID_Temporal(base.ECID, 20190101, 20221005) as AvgVisitPerSeason, MACHINELEARNINGFEATURES.PROD.BoughtPass_ECID_Temporal(base.ECID, '2021/22') as BoughtPass, MACHINELEARNINGFEATURES.PROD.EverCorePass_ECID_Temporal(base.ECID, 20051001, 20221005) as EverCorePass, MACHINELEARNINGFEATURES.PROD.EverPass_ECID_Temporal(base.ECID, 20051001, 20221005) as EverPass, MACHINELEARNINGFEATURES.PROD.GuestBehavior_ECID_Temporal(base.ECID, '2021/22') as GuestBehavior, MACHINELEARNINGFEATURES.PROD.IsEpicMixActivated_ECID_Temporal(base.ECID, '2021/22') as IsEpicMixActivated, MACHINELEARNINGFEATURES.PROD.MarketingZone_ECID_Temporal(base.ECID, '2021/22') as MarketingZone, MACHINELEARNINGFEATURES.PROD.MostCommonTicketComp_ECID_Temporal(base.ECID, 20190101, 20221005) as MostCommonTicketComp, MACHINELEARNINGFEATURES.PROD.MostSubSeasonVisited_ECID_Temporal(base.ECID, 20190101, 20221005) as MostSubSeasonVisited, MACHINELEARNINGFEATURES.PROD.MostVisitedRegion_ECID_Temporal(base.ECID, 20190101, 20221005) as MostVisitedRegion, MACHINELEARNINGFEATURES.PROD.MostVisitedResort_ECID_Temporal(base.ECID, 20190101, 20221005) as MostVisitedResort, MACHINELEARNINGFEATURES.PROD.OnlySingleResortKey_ECID_Temporal(base.ECID, 20190101, 20221005) as OnlySingleResortKey, MACHINELEARNINGFEATURES.PROD.PartnerResortScannerFlag_ECID_Temporal(base.ECID, 20190101, 20221005) as PartnerResortScannerFlag, MACHINELEARNINGFEATURES.PROD.ResortsVisited_ECID_Temporal(base.ECID, 20190101, 20221005) as ResortsVisited, MACHINELEARNINGFEATURES.PROD.SkierabilityLabel_ECID_Temporal(base.ECID, '2021/22') as SkierabilityLabel, MACHINELEARNINGFEATURES.PROD.SubSeasonsPerYear_ECID_Temporal(base.ECID, 20190101, 20221005) as SubSeasonsPerYear, MACHINELEARNINGFEATURES.PROD.TotalSeasonsScanned_ECID_Temporal(base.ECID, 20190101, 20221005) as TotalSeasonsScanned, MACHINELEARNINGFEATURES.PROD.TotalVisits_ECID_Temporal(base.ECID, 20190101, 20221005) as TotalVisits, MACHINELEARNINGFEATURES.PROD.VisitMostInPeak_ECID_Temporal(base.ECID, 20190101, 20221005) as VisitMostInPeak\n",
      "\n",
      ", '2021/22' as SEASONYEAR\n",
      "from\n",
      "(-- base EDEE team guest behavior\n",
      "  with edee_base as (\n",
      "      SELECT \n",
      "            distinct ECID\n",
      "      from BIDE_EDWDB_CUSTOMERMART_PROD.DBO.CustomerPASSPURCHASEBEHAVIOR cla\n",
      "      where \n",
      "              cla.GuestPassPurchaseBehaviorDetailLabel <> 'Unknown'\n",
      "  ),\n",
      "\n",
      "  -- pass prospects not in guest behavior\n",
      "  prospects as (\n",
      "      Select\n",
      "            distinct ECID\n",
      "      from Vail_Reporting.Prod.GuestBehaviorBase\n",
      "      where \n",
      "              GuestBehavior = 'Prospect'\n",
      "          and salesseason = '2021/22'\n",
      "  ),\n",
      "\n",
      "  -- paid prospects not in guest behavior\n",
      "  other_scans as (\n",
      "      select\n",
      "            distinct a.ECID\n",
      "      from\n",
      "      (\n",
      "          select \n",
      "                distinct ECID\n",
      "              , SeasonYear\n",
      "          from \"BIDE_EDWDB_ARA_PROD\".\"DBO\".\"SCANDAY\"\n",
      "              where Season = 'Winter'\n",
      "      ) a\n",
      "      left join Vail_Reporting.Prod.GuestBehaviorBase b \n",
      "          on a.ECID = b.ECID and a.SeasonYear = b.SalesSeason\n",
      "      where \n",
      "              b.GuestBehavior is null\n",
      "          and a.seasonyear = '2021/22'\n",
      "  )\n",
      "\n",
      "  select\n",
      "        distinct ECID\n",
      "      , 'Inference Set' as SeasonYear\n",
      "  from\n",
      "  (\n",
      "      select\n",
      "            coalesce(base.ecid, pro.ecid, os.ecid) as ecid\n",
      "      from edee_base base\n",
      "      full outer join prospects pro\n",
      "          on pro.ecid = base.ecid\n",
      "      full outer join other_scans os\n",
      "          on os.ecid = base.ecid\n",
      "  ) a)base\n",
      "inner join machinelearningfeatures.prod.featurestore_ecid joined on joined.ecid = base.ecid)\n",
      "\n",
      "max_file_size = 3200000\n",
      "overwrite = True\n",
      "file_format = (type = parquet          )\n",
      "credentials= (azure_sas_token = '**MASKED**')\n",
      "header = True;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:data_system_utilities.snowflake.utils:connection to snowflake established...\n",
      "INFO:data_system_utilities.snowflake.query:executing query\n",
      "INFO:data_system_utilities.snowflake.query:data loaded from snowflake\n",
      "INFO:data_system_utilities.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:root:data has been delivered from sf to adls\n"
     ]
    }
   ],
   "source": [
    "# | skip\n",
    "\"\"\"\n",
    "This is to help a user develop locally for the script if things are changing best method is to pull the above\n",
    "cell into this one and begin to develop or you can do this is a .py file, but this is my prefered method\n",
    "\"\"\"\n",
    "experiment = 'True' # this will trigger if the feature set needs to be created\n",
    "train_or_inference ='INFERENCE'\n",
    "experiment_name='BASELINE'\n",
    "\n",
    "\n",
    "experiment = True if experiment.lower() == 'true' else False\n",
    "logging.info(f\"This is a {'experiment run' if experiment else 'production run'}\")\n",
    "logging.info('Loading Yaml Files..')\n",
    "features, udf_inputs, etl = get_yaml_dicts(['features.yaml', 'udf_inputs.yaml', 'etl.yaml'])\n",
    "logging.info('Generating Feature Set Query')\n",
    "query = pull_features_from_snowflake(feature_dict=features,\n",
    "                                     udf_inputs=udf_inputs[train_or_inference.upper()],\n",
    "                                     filepath_to_grain_list_query='./LTBP/files/sql_files/',\n",
    "                                     experiment_name=experiment_name)\n",
    "data_lake_path = os.path.join((os.path.join(etl['data_lake_path'], 'experiments', experiment_name)\n",
    "                  if experiment \n",
    "                  else os.path.join(etl['data_lake_path'], \n",
    "                                    os.environ.get('CI_COMMIT_SHA', 'LocalRunTest')))\n",
    "                 , train_or_inference.lower()+'_data/')\n",
    "logging.info(f'Checking {data_lake_path} to either skip creation for experiment or create a production dataset')\n",
    "fh = FileHandling(os.environ['DATALAKE_CONN_STR_SECRET'])\n",
    "\n",
    "ald_files = fh.ls_blob(path=data_lake_path, container_name=etl['azure_container'])\n",
    "sf = snowflake_query()\n",
    "if ald_files == []:\n",
    "    query_feature_sets_to_adls_parquet_sf_fs(\n",
    "        sf_connection=sf,\n",
    "        sf_query=query,\n",
    "        query_file_path=os.path.join(files.__path__[0], etl['query_file_path']),\n",
    "        azure_account=etl[\"azure_account\"],\n",
    "        azure_container=etl[\"azure_container\"],\n",
    "        data_lake_path=data_lake_path, # TODO: Think about experiments versus \n",
    "        partition_by=None,\n",
    "        data_lake_sas_token=os.environ[\"DATALAKE_SAS_TOKEN_SECRET\"],\n",
    "    )\n",
    "else:\n",
    "    logging.warning(f'{data_lake_path} already exists this should be do experimentation runs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a709b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
