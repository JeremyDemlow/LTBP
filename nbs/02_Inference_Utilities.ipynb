{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Utils\n",
    "\n",
    "> Inference utilities used in this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp inference.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from LTBP.data.utils import snowflake_query\n",
    "\n",
    "from data_system_utilities.snowflake.copyinto import adls_url_to_sf_query_generator\n",
    "from data_system_utilities.snowflake.utils import create_table_query_from_df\n",
    "from data_system_utilities.azure.storage import FileHandling\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import logging\n",
    "import datetime\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def pull_sklearn_object_from_adls(adls_path: str,\n",
    "                                  file_name: str,\n",
    "                                  container_name: str,\n",
    "                                  connection_str: str,\n",
    "                                  drop_local_path: str = '.',\n",
    "                                  clean_up: bool = True):\n",
    "    \"\"\"pulls a pickeld sklearn object from azure data lake to memory\n",
    "\n",
    "    Args:\n",
    "        file_name (str): name of file\n",
    "        path (str): data lake path\n",
    "        container (str): data lake container\n",
    "        connection_str (str): azure connection string for the account\n",
    "\n",
    "    Returns:\n",
    "        (sklearn object): sklearn object loaded from azure\n",
    "    \"\"\"\n",
    "    logging.info(f'Loading Sklearn Object to: {os.path.join(drop_local_path, file_name)}')\n",
    "\n",
    "    if not os.path.exists(drop_local_path):\n",
    "        os.makedirs(drop_local_path)\n",
    "\n",
    "    fh = FileHandling(connection_str)\n",
    "    fh.download_file(\n",
    "        azure_file_path=adls_path+file_name,\n",
    "        container_name=container_name,\n",
    "        local_file_path=drop_local_path,\n",
    "        overwrite=True\n",
    "    )\n",
    "\n",
    "    with open(os.path.join(drop_local_path, file_name), 'rb') as f:\n",
    "        pipeline = pickle.load(f)\n",
    "        logging.info('Sklearn Object Loaded')\n",
    "\n",
    "    if clean_up:\n",
    "        shutil.rmtree(drop_local_path)\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the DSDE standard process for using Xboost with hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def prediction_to_adls_and_sf(\n",
    "    df,  # pandas dataframe to infer on\n",
    "    sk_model_pipe,  # Sklearn Pipeline that brings preprocessing and modeling to data\n",
    "    adls_path: str,  # adls root path\n",
    "    models_dict: dict,  # model dict used through out the project classes would avoid this\n",
    "    etl_dict: dict,  # etl dict used through the project\n",
    "    experiment_name: str,  # name of experiment being ran\n",
    "    sfSchema=os.getenv(\"sfSchema\", \"DEV\"),  # defaults to enviornment variable or default\n",
    "):\n",
    "    \"\"\"custom to this project small changes to make it more flexible\"\"\"\n",
    "    sf_df = df[models_dict['identification']].copy()\n",
    "    # Change Here Name change for a regression and to predict or multi-labled needs some work\n",
    "    sf_df['PROBABILITY'] = sk_model_pipe.predict_proba(df)[:, 1]\n",
    "    del df\n",
    "    date_created = datetime.datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    sf_df['CI_COMMIT_SHA'] = os.environ.get('CI_COMMIT_SHA', 'LocalRunNBS')\n",
    "    sf_df['DATE_CREATED'] = datetime.datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    sf_df['EXPERIMENT'] = experiment_name\n",
    "    file_name = f\"predictions_{os.environ.get('CI_COMMIT_SHA','LocalRunNBS')+experiment_name}.csv\"\n",
    "    # Saving as a .csv for simple reading from adls download using dask would be best here\n",
    "    sf_df.to_csv(file_name, index=False)\n",
    "    logging.info(f'preview predictions being added:\\n{sf_df.head(3)}')\n",
    "    logging.info(f'preview predictions values addes:\\n{sf_df.iloc[0].values}')\n",
    "    logging.info(f'preview predictions being added columns:\\n{sf_df.columns}')\n",
    "    az = FileHandling(os.environ[models_dict['connection_str']])\n",
    "    az.upload_file(\n",
    "        azure_file_path=os.path.join(adls_path,\n",
    "                                     models_dict['predictions_adls_path'],\n",
    "                                     models_dict[experiment_name]['model_trainer']),\n",
    "        local_file_path=file_name,\n",
    "        container_name=etl_dict['azure_container'],\n",
    "        overwrite=True,\n",
    "    )\n",
    "    os.unlink(file_name)\n",
    "    stage_url = f\"azure://{etl_dict['azure_account']}.blob.core.windows.net/{etl_dict['azure_container']}/\"\n",
    "    preds_file_path = os.path.join(adls_path,\n",
    "                                   models_dict['predictions_adls_path'],\n",
    "                                   models_dict['BASELINE']['model_trainer'],\n",
    "                                   file_name)\n",
    "\n",
    "    sf = snowflake_query(sfSchema=sfSchema)\n",
    "    if models_dict['inference_sf_table_name'].upper() not in sf.run_sql_str(\"show tables;\").name.tolist():\n",
    "        sf.run_sql_str(create_table_query_from_df(sf_df, table_name_sf=models_dict['inference_sf_table_name'], varchar=False))\n",
    "\n",
    "    logging.info(\"Pushing Forecasted Season from ADLS to Snowflake\")\n",
    "    adls_query = adls_url_to_sf_query_generator(\n",
    "        azure_path=os.path.join(stage_url, preds_file_path),\n",
    "        azure_sas_token=os.environ[models_dict['sas_token']],\n",
    "        table_name=models_dict['inference_sf_table_name'],\n",
    "        database=sf.connection_inputs['database'],\n",
    "        schema=sf.connection_inputs['schema'],\n",
    "        skip_header='1',\n",
    "        file_type='csv',\n",
    "        pattern='.*.csv')\n",
    "    sf.run_sql_str(adls_query)\n",
    "\n",
    "    exp_table = sf.run_sql_str(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM {models_dict['inference_sf_table_name']}\n",
    "    WHERE DATE_CREATED = '{date_created}'\n",
    "    AND EXPERIMENT = '{experiment_name}'\n",
    "    LIMIT 3\n",
    "    \"\"\")\n",
    "    logging.info(f'preview of queried table being added:\\n{exp_table.head(3)}')\n",
    "    logging.info(f'preview predictions values addes:\\n{exp_table.iloc[0].values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| skip\n",
    "experiment_name = 'BASELINE'\n",
    "experiment = True\n",
    "yaml_file_list = ['features.yaml', 'udf_inputs.yaml', 'etl.yaml', 'models.yaml']\n",
    "\n",
    "\n",
    "features, udf_inputs, etl_dict, models_dict = get_yaml_dicts(yaml_file_list)\n",
    "\n",
    "adls_path = os.path.join(\n",
    "    (os.path.join(etl_dict['data_lake_path'], 'experiments', experiment_name)\n",
    "      if experiment\n",
    "      else os.path.join(\n",
    "          etl_dict['data_lake_path'], os.environ.get('CI_COMMIT_SHA', 'LocalRunNBS'))))\n",
    "\n",
    "model_name = (models_dict[experiment_name]['model_trainer']+\n",
    "              os.environ.get('CI_COMMIT_SHA', 'LocalRunNBS')+\n",
    "              experiment_name+'.pkl'\n",
    "             )\n",
    "\n",
    "model = pull_sklearn_object_from_adls(\n",
    "        adls_path=os.path.join(adls_path,\n",
    "                               models_dict['modeling_adls_path'],\n",
    "                               models_dict[experiment_name]['model_trainer']\n",
    "                              ) + '/',\n",
    "        file_name=model_name,\n",
    "        drop_local_path='./models/',\n",
    "        container_name=etl_dict['azure_container'],\n",
    "        connection_str=os.environ[models_dict['connection_str']]\n",
    "    )\n",
    "\n",
    "sf = snowflake_query()\n",
    "df_infer = create_stage_and_query_stage_sf(\n",
    "    sf=sf,\n",
    "    etl=etl_dict,\n",
    "    udf_inputs=udf_inputs,\n",
    "    train_or_inference='INFERENCE',\n",
    "    experiment_name=experiment_name,\n",
    "    experiment=experiment,\n",
    "    indentification=models_dict['identification'],\n",
    "    extra_statement='LIMIT 1000'\n",
    ")\n",
    "logging.info(f'size of test set {df_infer.shape}')\n",
    "logging.info(f'Preview inference data:\\n{df_infer.head(2)}')\n",
    "logging.info(f'Preview inference data values:\\n{df_infer.iloc[0].values}')\n",
    "\n",
    "logging.info('Begining on inference upload process')\n",
    "prediction_to_adls_and_sf(\n",
    "    df=df_infer,\n",
    "    sk_model_pipe=model,\n",
    "    adls_path=adls_path,\n",
    "    models_dict=models_dict,\n",
    "    etl_dict=etl_dict,\n",
    "    experiment_name=experiment_name,\n",
    "    sfSchema=os.getenv(\"sfSchema\", \"DEV\")\n",
    ")\n",
    "logging.info(f'Inference stage complete for {experiment_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| skip\n",
    "# sf.run_sql_str(f\"DROP TABLE {models_dict['tracking_table']}\")\n",
    "# sf.run_sql_str(f\"DROP TABLE MACHINELEARNINGOUTPUTS.dev.{models_dict['hold_out_table']}\")\n",
    "# sf.run_sql_str(f\"DROP TABLE MACHINELEARNINGOUTPUTS.dev.{table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
