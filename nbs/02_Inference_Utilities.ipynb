{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Utils\n",
    "\n",
    "> Inference utilities used in this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp inference.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *\n",
    "from LTBP.data.utils import get_yaml_dicts\n",
    "from LTBP.modeling.utils import create_stage_and_query_stage_sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeremydemlow/miniforge3/envs/ltbp/lib/python3.9/site-packages/snowflake/connector/options.py:96: UserWarning: You have an incompatible version of 'pyarrow' installed (6.0.0), please install a version that adheres to: 'pyarrow<8.1.0,>=8.0.0; extra == \"pandas\"'\n",
      "  warn_incompatible_dep(\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "from LTBP.data.utils import snowflake_query\n",
    "\n",
    "from data_system_utilities.snowflake.copyinto import adls_url_to_sf_query_generator\n",
    "from data_system_utilities.snowflake.utils import create_table_query_from_df\n",
    "from data_system_utilities.azure.storage import FileHandling\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import logging\n",
    "import datetime\n",
    "import shutil\n",
    "import pytz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def pull_sklearn_object_from_adls(adls_path: str,\n",
    "                                  file_name: str,\n",
    "                                  container_name: str,\n",
    "                                  connection_str: str,\n",
    "                                  drop_local_path: str = '.',\n",
    "                                  clean_up: bool = True):\n",
    "    \"\"\"pulls a pickeld sklearn object from azure data lake to memory\n",
    "\n",
    "    Args:\n",
    "        file_name (str): name of file\n",
    "        path (str): data lake path\n",
    "        container (str): data lake container\n",
    "        connection_str (str): azure connection string for the account\n",
    "\n",
    "    Returns:\n",
    "        (sklearn object): sklearn object loaded from azure\n",
    "    \"\"\"\n",
    "    logging.info(f'Loading Sklearn Object to: {os.path.join(drop_local_path, file_name)}')\n",
    "\n",
    "    if not os.path.exists(drop_local_path):\n",
    "        os.makedirs(drop_local_path)\n",
    "\n",
    "    fh = FileHandling(connection_str)\n",
    "    fh.download_file(\n",
    "        azure_file_path=adls_path+file_name,\n",
    "        container_name=container_name,\n",
    "        local_file_path=drop_local_path,\n",
    "        overwrite=True\n",
    "    )\n",
    "\n",
    "    with open(os.path.join(drop_local_path, file_name), 'rb') as f:\n",
    "        pipeline = pickle.load(f)\n",
    "        logging.info('Sklearn Object Loaded')\n",
    "\n",
    "    if clean_up:\n",
    "        shutil.rmtree(drop_local_path)\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the DSDE standard process for using Xboost with hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def prediction_to_adls_and_sf(\n",
    "    df,  # pandas dataframe to infer on\n",
    "    sk_model_pipe,  # Sklearn Pipeline that brings preprocessing and modeling to data\n",
    "    adls_path: str,  # adls root path\n",
    "    models_dict: dict,  # model dict used through out the project classes would avoid this\n",
    "    etl_dict: dict,  # etl dict used through the project\n",
    "    experiment_name: str,  # name of experiment being ran\n",
    "    sfSchema=os.getenv(\"sfSchema\", \"DEV\"),  # defaults to enviornment variable or default\n",
    "):\n",
    "    \"\"\"DEPERCATED WILL BREAK BACK custom to this project small changes to make it more flexible\"\"\"\n",
    "    sf_df = df[models_dict['identification']].copy()\n",
    "    # Change Here Name change for a regression and to predict or multi-labled needs some work\n",
    "    sf_df['PROBABILITY'] = sk_model_pipe.predict_proba(df)[:, 1]\n",
    "    del df\n",
    "    date_created = datetime.datetime.now(pytz.timezone(\"US/Mountain\")).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    sf_df['CI_COMMIT_SHA'] = os.environ.get('CI_COMMIT_SHA', 'LocalRunNBS')\n",
    "    sf_df['DATE_CREATED'] = date_created\n",
    "    sf_df['EXPERIMENT'] = experiment_name\n",
    "    file_name = f\"predictions_{os.environ.get('CI_COMMIT_SHA','LocalRunNBS')+experiment_name}.csv\"\n",
    "    # Saving as a .csv for simple reading from adls download using dask would be best here\n",
    "    sf_df.to_csv(file_name, index=False)\n",
    "    logging.info(f'preview predictions being added:\\n{sf_df.head(3)}')\n",
    "    logging.info(f'preview predictions values addes:\\n{sf_df.iloc[0].values}')\n",
    "    logging.info(f'preview predictions being added columns:\\n{sf_df.columns}')\n",
    "    az = FileHandling(os.environ[models_dict['connection_str']])\n",
    "    az.upload_file(\n",
    "        azure_file_path=os.path.join(adls_path,\n",
    "                                     models_dict['predictions_adls_path'],\n",
    "                                     models_dict[experiment_name]['model_trainer']),\n",
    "        local_file_path=file_name,\n",
    "        container_name=etl_dict['azure_container'],\n",
    "        overwrite=True,\n",
    "    )\n",
    "    os.unlink(file_name)\n",
    "    stage_url = f\"azure://{etl_dict['azure_account']}.blob.core.windows.net/{etl_dict['azure_container']}/\"\n",
    "    preds_file_path = os.path.join(adls_path,\n",
    "                                   models_dict['predictions_adls_path'],\n",
    "                                   models_dict['BASELINE']['model_trainer'],\n",
    "                                   file_name)\n",
    "\n",
    "    sf = snowflake_query(sfSchema=sfSchema)\n",
    "    if models_dict['inference_sf_table_name'].upper() not in sf.run_sql_str(\"show tables;\").name.tolist():\n",
    "        sf.run_sql_str(create_table_query_from_df(sf_df, table_name_sf=models_dict['inference_sf_table_name'], varchar=False))\n",
    "\n",
    "    logging.info(\"Pushing Forecasted Season from ADLS to Snowflake\")\n",
    "    adls_query = adls_url_to_sf_query_generator(\n",
    "        azure_path=os.path.join(stage_url, preds_file_path),\n",
    "        azure_sas_token=os.environ[models_dict['sas_token']],\n",
    "        table_name=models_dict['inference_sf_table_name'],\n",
    "        database=sf.connection_inputs['database'],\n",
    "        schema=sf.connection_inputs['schema'],\n",
    "        skip_header='1',\n",
    "        file_type='csv',\n",
    "        pattern='.*.csv')\n",
    "    sf.run_sql_str(adls_query)\n",
    "\n",
    "    exp_table = sf.run_sql_str(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM {models_dict['inference_sf_table_name']}\n",
    "    WHERE DATE_CREATED = '{date_created}'\n",
    "    AND EXPERIMENT = '{experiment_name}'\n",
    "    LIMIT 3\n",
    "    \"\"\")\n",
    "    logging.info(f'preview of queried table being added:\\n{exp_table.head(3)}')\n",
    "    logging.info(f'preview predictions values addes:\\n{exp_table.iloc[0].values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading Sklearn Object to: ./models/train_xgbLocalRunTestBASELINE.pkl\n",
      "INFO:data_system_utilities.azure.storage:Downloading projects/LTBP/FY23/experiments/BASELINE/modeling/train_xgb/train_xgbLocalRunTestBASELINE.pkl to ./models/train_xgbLocalRunTestBASELINE.pkl\n",
      "INFO:data_system_utilities.azure.storage:Download complete\n",
      "INFO:root:Sklearn Object Loaded\n",
      "INFO:data_system_utilities.snowflake.utils:stage_query: \n",
      " create or replace stage ltbpFY23LocalRunTest\n",
      "url='azure://vaildtscadls.blob.core.windows.net/vailadls/projects/LTBP/FY23/experiments/BASELINE'\n",
      "credentials=(azure_sas_token='**MASKED**')\n",
      "encryption=(type= 'NONE')\n",
      "file_format = (type = parquet        )\n",
      "INFO:data_system_utilities.snowflake.utils:connection to snowflake established...\n",
      "INFO:data_system_utilities.snowflake.query:executing query\n",
      "INFO:data_system_utilities.snowflake.query:data loaded from snowflake\n",
      "INFO:data_system_utilities.snowflake.quidenfication to snowflake has been turned off\n",
      "INFO:data_system_utilities.snowflake.query:Stage area LTBPFY23LOCALRUNTEST successfully created.\n",
      "INFO:root:adls snowflake stage query \n",
      "    select\n",
      "    $1:\"ECID\"::varchar as ECID\n",
      ", $1:\"SEASONYEAR\"::varchar as SEASONYEAR\n",
      ", $1:\"AGE\"::varchar as AGE\n",
      ", $1:\"AVGVISITPERSEASON\"::varchar as AVGVISITPERSEASON\n",
      ", $1:\"BOUGHTPASS\"::varchar as BOUGHTPASS\n",
      ", $1:\"DESTINATIONGEOAFINITYLABEL\"::varchar as DESTINATIONGEOAFINITYLABEL\n",
      ", $1:\"EVERCOREPASS\"::varchar as EVERCOREPASS\n",
      ", $1:\"EVERPASS\"::varchar as EVERPASS\n",
      ", $1:\"GENDERCODE\"::varchar as GENDERCODE\n",
      ", $1:\"GUESTBEHAVIOR\"::varchar as GUESTBEHAVIOR\n",
      ", $1:\"ISEPICMIXACTIVATED\"::varchar as ISEPICMIXACTIVATED\n",
      ", $1:\"MARKETINGZONE\"::varchar as MARKETINGZONE\n",
      ", $1:\"MOSTCOMMONTICKETCOMP\"::varchar as MOSTCOMMONTICKETCOMP\n",
      ", $1:\"MOSTSUBSEASONVISITED\"::varchar as MOSTSUBSEASONVISITED\n",
      ", $1:\"MOSTVISITEDREGION\"::varchar as MOSTVISITEDREGION\n",
      ", $1:\"MOSTVISITEDRESORT\"::varchar as MOSTVISITEDRESORT\n",
      ", $1:\"ONLYSINGLERESORTKEY\"::varchar as ONLYSINGLERESORTKEY\n",
      ", $1:\"PARTNERRESORTSCANNERFLAG\"::varchar as PARTNERRESORTSCANNERFLAG\n",
      ", $1:\"RESORTSVISITED\"::varchar as RESORTSVISITED\n",
      ", $1:\"SKIERABILITYLABEL\"::varchar as SKIERABILITYLABEL\n",
      ", $1:\"SUBSEASONSPERYEAR\"::varchar as SUBSEASONSPERYEAR\n",
      ", $1:\"TOTALSEASONSSCANNED\"::varchar as TOTALSEASONSSCANNED\n",
      ", $1:\"TOTALVISITS\"::varchar as TOTALVISITS\n",
      ", $1:\"VISITMOSTINPEAK\"::varchar as VISITMOSTINPEAK\n",
      "\n",
      "    from @ltbpFY23LocalRunTest/inference_data/\n",
      "    LIMIT 1000\n",
      "    \n",
      "INFO:data_system_utilities.snowflake.utils:connection to snowflake established...\n",
      "INFO:data_system_utilities.snowflake.query:executing query\n",
      "INFO:data_system_utilities.snowflake.query:data loaded from snowflake\n",
      "INFO:data_system_utilities.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:root:Preview dataframe queried        ECID SEASONYEAR AGE AVGVISITPERSEASON BOUGHTPASS  \\\n",
      "0   4274289    2021/22  33                 0          0   \n",
      "1  83442101    2021/22  40                 0          0   \n",
      "2   7327662    2021/22  17                 0          0   \n",
      "3   6579493    2021/22  63                 0          0   \n",
      "4  59180198    2021/22  35                 0          0   \n",
      "\n",
      "  DESTINATIONGEOAFINITYLABEL EVERCOREPASS EVERPASS GENDERCODE GUESTBEHAVIOR  \\\n",
      "0                Destination            0        0          M   Lapsed Paid   \n",
      "1                      Local            0        0          F   Lapsed Paid   \n",
      "2                Destination            0        0          M   Lapsed Paid   \n",
      "3                Destination            0        0          F   Lapsed Paid   \n",
      "4                Destination            0        0          F   Lapsed Paid   \n",
      "\n",
      "   ... MOSTVISITEDREGION MOSTVISITEDRESORT ONLYSINGLERESORTKEY  \\\n",
      "0  ...              None              None                None   \n",
      "1  ...              None              None                None   \n",
      "2  ...              None              None                None   \n",
      "3  ...              None              None                None   \n",
      "4  ...              None              None                None   \n",
      "\n",
      "  PARTNERRESORTSCANNERFLAG RESORTSVISITED SKIERABILITYLABEL SUBSEASONSPERYEAR  \\\n",
      "0                        0              0              None              None   \n",
      "1                        0              0              None              None   \n",
      "2                        0              0              None              None   \n",
      "3                        0              0              None              None   \n",
      "4                        0              0              None              None   \n",
      "\n",
      "  TOTALSEASONSSCANNED TOTALVISITS VISITMOSTINPEAK  \n",
      "0                   0           0               0  \n",
      "1                   0           0               0  \n",
      "2                   0           0               0  \n",
      "3                   0           0               0  \n",
      "4                   0           0               0  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "INFO:root:size of test set (1000, 24)\n",
      "INFO:root:Preview inference data:\n",
      "       ECID SEASONYEAR AGE AVGVISITPERSEASON BOUGHTPASS  \\\n",
      "0   4274289    2021/22  33                 0          0   \n",
      "1  83442101    2021/22  40                 0          0   \n",
      "\n",
      "  DESTINATIONGEOAFINITYLABEL EVERCOREPASS EVERPASS GENDERCODE GUESTBEHAVIOR  \\\n",
      "0                Destination            0        0          M   Lapsed Paid   \n",
      "1                      Local            0        0          F   Lapsed Paid   \n",
      "\n",
      "   ... MOSTVISITEDREGION MOSTVISITEDRESORT ONLYSINGLERESORTKEY  \\\n",
      "0  ...              None              None                None   \n",
      "1  ...              None              None                None   \n",
      "\n",
      "  PARTNERRESORTSCANNERFLAG RESORTSVISITED SKIERABILITYLABEL SUBSEASONSPERYEAR  \\\n",
      "0                        0              0              None              None   \n",
      "1                        0              0              None              None   \n",
      "\n",
      "  TOTALSEASONSSCANNED TOTALVISITS VISITMOSTINPEAK  \n",
      "0                   0           0               0  \n",
      "1                   0           0               0  \n",
      "\n",
      "[2 rows x 24 columns]\n",
      "INFO:root:Preview inference data values:\n",
      "['4274289' '2021/22' '33' '0' '0' 'Destination' '0' '0' 'M' 'Lapsed Paid'\n",
      " None None None None None None None '0' '0' None None '0' '0' '0']\n",
      "INFO:root:Begining on inference upload process\n",
      "INFO:root:preview predictions being added:\n",
      "       ECID SEASONYEAR  PROBABILITY CI_COMMIT_SHA         DATE_CREATED  \\\n",
      "0   4274289    2021/22     0.191546  LocalRunTest  2022-11-03 15:17:48   \n",
      "1  83442101    2021/22     0.180928  LocalRunTest  2022-11-03 15:17:48   \n",
      "2   7327662    2021/22     0.193184  LocalRunTest  2022-11-03 15:17:48   \n",
      "\n",
      "  EXPERIMENT  \n",
      "0   BASELINE  \n",
      "1   BASELINE  \n",
      "2   BASELINE  \n",
      "INFO:root:preview predictions values addes:\n",
      "['4274289' '2021/22' 0.19154589 'LocalRunTest' '2022-11-03 15:17:48'\n",
      " 'BASELINE']\n",
      "INFO:root:preview predictions being added columns:\n",
      "Index(['ECID', 'SEASONYEAR', 'PROBABILITY', 'CI_COMMIT_SHA', 'DATE_CREATED',\n",
      "       'EXPERIMENT'],\n",
      "      dtype='object')\n",
      "INFO:data_system_utilities.azure.storage:Uploading predictions_LocalRunTestBASELINE.csv, to Azure Storage projects/LTBP/FY23/experiments/BASELINE/predictions/train_xgb/predictions_LocalRunTestBASELINE.csv\n",
      "INFO:data_system_utilities.azure.storage:Azure Upload Complete\n",
      "INFO:data_system_utilities.snowflake.utils:connection to snowflake established...\n",
      "INFO:data_system_utilities.snowflake.query:executing query\n",
      "INFO:data_system_utilities.snowflake.query:data loaded from snowflake\n",
      "INFO:data_system_utilities.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:root:Pushing Forecasted Season from ADLS to Snowflake\n",
      "INFO:data_system_utilities.snowflake.copyinto:\n",
      "copy into MACHINELEARNINGOUTPUTS.DEV.LTBP_PREDICTIONS_FY23\n",
      "from 'azure://vaildtscadls.blob.core.windows.net/vailadls/projects/LTBP/FY23/experiments/BASELINE/predictions/train_xgb/predictions_LocalRunTestBASELINE.csv'\n",
      "file_format = (type = csv     skip_header = 1)\n",
      "credentials= (azure_sas_token = '?sv=2019-12-12&ss=bfqt&srt=sco&sp=rwdlacupx&se=2031-01-22T06:17:14Z&st=2021-01-21T22:17:14Z&spr=https&sig=kIHogByJjyVWyL6XupA0CBUB1iw12%2FeXWFQiOj5fB5c%3D')\n",
      "pattern = '.*.csv';\n",
      "INFO:data_system_utilities.snowflake.utils:connection to snowflake established...\n",
      "INFO:data_system_utilities.snowflake.query:executing query\n",
      "INFO:data_system_utilities.snowflake.query:data loaded from snowflake\n",
      "INFO:data_system_utilities.snowflake.query:connection to snowflake has been turned off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:data_system_utilities.snowflake.utils:connection to snowflake established...\n",
      "INFO:data_system_utilities.snowflake.query:executing query\n",
      "INFO:data_system_utilities.snowflake.query:data loaded from snowflake\n",
      "INFO:data_system_utilities.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:root:preview of queried table being added:\n",
      "       ECID SEASONYEAR  PROBABILITY CI_COMMIT_SHA         DATE_CREATED  \\\n",
      "0   4274289    2021/22     0.191546  LocalRunTest  2022-11-03 15:17:48   \n",
      "1  83442101    2021/22     0.180928  LocalRunTest  2022-11-03 15:17:48   \n",
      "2   7327662    2021/22     0.193184  LocalRunTest  2022-11-03 15:17:48   \n",
      "\n",
      "  EXPERIMENT  \n",
      "0   BASELINE  \n",
      "1   BASELINE  \n",
      "2   BASELINE  \n",
      "INFO:root:preview predictions values addes:\n",
      "['4274289' '2021/22' 0.19154589 'LocalRunTest' '2022-11-03 15:17:48'\n",
      " 'BASELINE']\n",
      "INFO:root:Inference stage complete for BASELINE\n"
     ]
    }
   ],
   "source": [
    "#| skip\n",
    "experiment_name = 'BASELINE'\n",
    "experiment = True\n",
    "yaml_file_list = ['features.yaml', 'udf_inputs.yaml', 'etl.yaml', 'models.yaml']\n",
    "\n",
    "\n",
    "features, udf_inputs, etl_dict, models_dict = get_yaml_dicts(yaml_file_list)\n",
    "\n",
    "adls_path = os.path.join(\n",
    "    (os.path.join(etl_dict['data_lake_path'], 'experiments', experiment_name)\n",
    "      if experiment\n",
    "      else os.path.join(\n",
    "          etl_dict['data_lake_path'], os.environ.get('CI_COMMIT_SHA', 'LocalRunNBS'))))\n",
    "\n",
    "model_name = (models_dict[experiment_name]['model_trainer']+\n",
    "              os.environ.get('CI_COMMIT_SHA', 'LocalRunNBS')+\n",
    "              experiment_name+'.pkl'\n",
    "             )\n",
    "\n",
    "model = pull_sklearn_object_from_adls(\n",
    "        adls_path=os.path.join(adls_path,\n",
    "                               models_dict['modeling_adls_path'],\n",
    "                               models_dict[experiment_name]['model_trainer']\n",
    "                              ) + '/',\n",
    "        file_name=model_name,\n",
    "        drop_local_path='./models/',\n",
    "        container_name=etl_dict['azure_container'],\n",
    "        connection_str=os.environ[models_dict['connection_str']]\n",
    "    )\n",
    "\n",
    "sf = snowflake_query()\n",
    "df_infer = create_stage_and_query_stage_sf(\n",
    "    sf=sf,\n",
    "    etl=etl_dict,\n",
    "    udf_inputs=udf_inputs,\n",
    "    train_or_inference='INFERENCE',\n",
    "    experiment_name=experiment_name,\n",
    "    experiment=experiment,\n",
    "    indentification=models_dict['identification'],\n",
    "    extra_statement='LIMIT 1000'\n",
    ")\n",
    "logging.info(f'size of test set {df_infer.shape}')\n",
    "logging.info(f'Preview inference data:\\n{df_infer.head(2)}')\n",
    "logging.info(f'Preview inference data values:\\n{df_infer.iloc[0].values}')\n",
    "\n",
    "logging.info('Begining on inference upload process')\n",
    "prediction_to_adls_and_sf(\n",
    "    df=df_infer,\n",
    "    sk_model_pipe=model,\n",
    "    adls_path=adls_path,\n",
    "    models_dict=models_dict,\n",
    "    etl_dict=etl_dict,\n",
    "    experiment_name=experiment_name,\n",
    "    sfSchema=os.getenv(\"sfSchema\", \"DEV\")\n",
    ")\n",
    "logging.info(f'Inference stage complete for {experiment_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| skip\n",
    "# sf.run_sql_str(f\"DROP TABLE {models_dict['tracking_table']}\")\n",
    "# sf.run_sql_str(f\"DROP TABLE MACHINELEARNINGOUTPUTS.dev.{models_dict['hold_out_table']}\")\n",
    "# sf.run_sql_str(f\"DROP TABLE MACHINELEARNINGOUTPUTS.dev.{table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
